{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Comparison for Invoice Work Order Number Extraction\n",
    "\n",
    "## Overview\n",
    "This notebook compares the effectiveness of different prompt strategies for extracting work order numbers from invoice images using the Pixtral-12B vision-language model. We will test three distinct prompt approaches:\n",
    "\n",
    "1. **Basic Prompt**: A simple, direct instruction\n",
    "   ```\n",
    "   Extract the work order number from this invoice image.\n",
    "   ```\n",
    "\n",
    "2. **Detailed Prompt**: Provides additional context and possible field labels\n",
    "   ```\n",
    "   This is an invoice image. Please find and extract the work order number. It may be labeled as 'Work Order Number', 'Numero de Orden', 'Order #', or similar.\n",
    "   ```\n",
    "\n",
    "3. **Positioned Prompt**: Includes positional information about where to look\n",
    "   ```\n",
    "   Extract the work order number from this invoice image. The work order number is typically located in the upper portion of the invoice.\n",
    "   ```\n",
    "\n",
    "### Objectives\n",
    "- Evaluate how different prompt formulations affect extraction accuracy\n",
    "- Identify which prompt strategy yields the highest exact match rate\n",
    "- Analyze error patterns for each prompt type\n",
    "- Establish best practices for invoice data extraction\n",
    "\n",
    "We'll process the same dataset with each prompt type while keeping all other variables constant, allowing for direct comparison of their effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Environment Verification\n",
    "### Environment Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import torch\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure basic logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('invoice_extraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to check if we're in a RunPod environment\n",
    "def is_runpod():\n",
    "    \"\"\"Check if we're running in a RunPod environment\"\"\"\n",
    "    return (\n",
    "        os.environ.get(\"RUNPOD_POD_ID\") is not None or \n",
    "        \"A100\" in os.environ.get(\"GPU_NAME\", \"\") or\n",
    "        \"H100\" in os.environ.get(\"GPU_NAME\", \"\")\n",
    "    )\n",
    "\n",
    "# Display basic system information\n",
    "print(f\"üîç Environment Setup and Verification\")\n",
    "print(f\"üêç Python version: {platform.python_version()}\")\n",
    "print(f\"üìä PyTorch version: {torch.__version__}\")\n",
    "print(f\"ü§ñ Transformers version: {transformers.__version__}\")\n",
    "print(f\"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"üñ•Ô∏è GPU: {device_name} ({memory:.2f} GB)\")\n",
    "    print(f\"üîç CUDA Version: {torch.version.cuda}\")\n",
    "    \n",
    "    # Display GPU memory usage\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    print(f\"üíæ GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
    "    \n",
    "    # Basic GPU compatibility check\n",
    "    if memory < 24:\n",
    "        print(\"‚ö†Ô∏è Warning: GPU memory may be insufficient for Pixtral-12B without quantization\")\n",
    "    else:\n",
    "        print(\"‚úÖ GPU memory sufficient for model loading\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected - running in CPU mode\")\n",
    "    print(\"   Note: Processing will be extremely slow without GPU acceleration\")\n",
    "\n",
    "# Identify environment type\n",
    "if is_runpod():\n",
    "    print(\"‚òÅÔ∏è RunPod environment detected\")\n",
    "else:\n",
    "    print(\"üíª Local environment detected\")\n",
    "\n",
    "# Display platform details for debugging\n",
    "print(f\"üíª System: {platform.system()} {platform.release()}\")\n",
    "print(f\"üìÇ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Path Configuration\n",
    "# Set up project paths - This cell finds the project root and initializes path configuration\n",
    "\n",
    "# First, try to find the project root directory\n",
    "notebook_path = Path().resolve()\n",
    "project_root = None\n",
    "\n",
    "# Look for project root markers (traversing upward from the notebook)\n",
    "for parent in [notebook_path] + list(notebook_path.parents):\n",
    "    if any((parent / marker).exists() for marker in ['.git', 'setup.py', 'requirements.txt']):\n",
    "        project_root = parent\n",
    "        break\n",
    "\n",
    "if project_root is None:\n",
    "    # If markers not found, assume we're in a subdirectory of the project\n",
    "    # and the parent directory is the project root\n",
    "    project_root = notebook_path.parent\n",
    "    print(\"‚ö†Ô∏è Could not definitively locate project root, using parent directory\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found project root: {project_root}\")\n",
    "\n",
    "# Add project root to Python path if not already there\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    print(f\"üìå Added {project_root} to Python path\")\n",
    "\n",
    "# Set environment variable for project root\n",
    "os.environ['PROJECT_ROOT'] = str(project_root)\n",
    "print(f\"üîÑ Set PROJECT_ROOT environment variable to {project_root}\")\n",
    "\n",
    "# Verify critical directories exist, create if needed\n",
    "critical_dirs = [\n",
    "    \"configs/environments\",\n",
    "    \"configs/prompts\",\n",
    "    \"src/config\",\n",
    "    \"src/models\",\n",
    "    \"src/prompts\",\n",
    "    \"src/execution\",\n",
    "    \"src/results\",\n",
    "    \"src/analysis\",\n",
    "    \"data/images\",\n",
    "    \"models/cache\",\n",
    "    \"results/raw\",\n",
    "    \"results/processed\",\n",
    "    \"results/visualizations\",\n",
    "    \"logs\"\n",
    "]\n",
    "\n",
    "for dir_path in critical_dirs:\n",
    "    full_path = project_root / dir_path\n",
    "    if not full_path.exists():\n",
    "        print(f\"üìÅ Creating directory: {dir_path}\")\n",
    "        full_path.mkdir(parents=True, exist_ok=True)\n",
    "    else:\n",
    "        print(f\"‚úÖ Directory exists: {dir_path}\")\n",
    "\n",
    "# Check for ground truth data file\n",
    "ground_truth_path = project_root / \"data\" / \"ground_truth.csv\"\n",
    "if os.path.exists(ground_truth_path):\n",
    "    print(f\"‚úÖ Ground truth data file found\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Ground truth data file not found at: {ground_truth_path}\")\n",
    "    print(\"   This will be needed for comparing extraction results.\")\n",
    "\n",
    "# Check for image files\n",
    "image_dir = project_root / \"data\" / \"images\"\n",
    "image_files = list(image_dir.glob(\"*.jpg\")) + list(image_dir.glob(\"*.png\"))\n",
    "if image_files:\n",
    "    print(f\"‚úÖ Found {len(image_files)} image files in {image_dir}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No image files found in {image_dir}\")\n",
    "    print(\"   This will be needed for extraction testing.\")\n",
    "\n",
    "# Set up results directory for this run with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_name = f\"prompt_comparison_{timestamp}\"\n",
    "results_dir = project_root / \"results\" / experiment_name\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"üìä Created results directory for this run: {results_dir}\")\n",
    "\n",
    "# Create subdirectories in results directory\n",
    "for subdir in [\"raw\", \"processed\", \"visualizations\"]:\n",
    "    (results_dir / subdir).mkdir(exist_ok=True)\n",
    "\n",
    "# Define path utility functions\n",
    "def get_results_path(filename):\n",
    "    \"\"\"Get full path for a results file\"\"\"\n",
    "    return results_dir / filename\n",
    "\n",
    "def get_raw_path(filename):\n",
    "    \"\"\"Get full path for a raw results file\"\"\"\n",
    "    return results_dir / \"raw\" / filename\n",
    "\n",
    "def get_processed_path(filename):\n",
    "    \"\"\"Get full path for a processed results file\"\"\"\n",
    "    return results_dir / \"processed\" / filename\n",
    "\n",
    "def get_visualization_path(filename):\n",
    "    \"\"\"Get full path for a visualization file\"\"\"\n",
    "    return results_dir / \"visualizations\" / filename\n",
    "\n",
    "def get_image_paths():\n",
    "    \"\"\"Get all image paths from the images directory\"\"\"\n",
    "    return list(image_dir.glob(\"*.jpg\")) + list(image_dir.glob(\"*.png\"))\n",
    "\n",
    "# Define paths dictionary for easy access\n",
    "paths = {\n",
    "    \"project_root\": project_root,\n",
    "    \"data_dir\": project_root / \"data\",\n",
    "    \"images_dir\": image_dir,\n",
    "    \"ground_truth_path\": ground_truth_path,\n",
    "    \"models_cache_dir\": project_root / \"models\" / \"cache\",\n",
    "    \"results_dir\": results_dir,\n",
    "    \"raw_results_dir\": results_dir / \"raw\",\n",
    "    \"processed_results_dir\": results_dir / \"processed\",\n",
    "    \"visualizations_dir\": results_dir / \"visualizations\",\n",
    "    \"get_results_path\": get_results_path,\n",
    "    \"get_raw_path\": get_raw_path,\n",
    "    \"get_processed_path\": get_processed_path,\n",
    "    \"get_visualization_path\": get_visualization_path,\n",
    "    \"get_image_paths\": get_image_paths\n",
    "}\n",
    "\n",
    "print(f\"\\nüìã Path configuration complete!\")\n",
    "print(f\"   Project root: {paths['project_root']}\")\n",
    "print(f\"   Data directory: {paths['data_dir']}\")\n",
    "print(f\"   Results for this run: {paths['results_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Core Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Any, Optional, Union\n",
    "\n",
    "# Data processing and visualization libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from Levenshtein import distance\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    LlavaForConditionalGeneration,\n",
    "    AutoModelForVision2Seq,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "import os\n",
    "\n",
    "# Create logs directory if it doesn't exist\n",
    "log_dir = \"logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "log_file = f\"{log_dir}/extraction_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),  # Log to console\n",
    "        logging.FileHandler(log_file)  # Log to file\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger('invoice_extraction')\n",
    "logger.info(\"Starting invoice extraction experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Project Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project-specific modules\n",
    "try:\n",
    "    # Attempt to import required modules from src directory\n",
    "    # These imports assume the project root has been added to sys.path in Cell 1\n",
    "    \n",
    "    # Configuration modules\n",
    "    from src.config.environment import get_environment_config\n",
    "    from src.config.paths import get_path_config, PathConfig\n",
    "    from src.config.experiment import get_experiment_config, ExperimentConfig\n",
    "    \n",
    "    # Model handling modules\n",
    "    from src.models.loader import load_model_and_processor, get_gpu_memory_info\n",
    "    from src.models.registry import get_model_config\n",
    "    from src.models.optimization import optimize_model_memory\n",
    "    \n",
    "    # Prompt management modules\n",
    "    from src.prompts.registry import get_prompt, list_available_prompts, format_prompt\n",
    "    from src.prompts.prompt_utils import analyze_prompt\n",
    "    \n",
    "    # Execution modules\n",
    "    from src.execution.inference import process_image\n",
    "    from src.execution.pipeline import ExtractionPipeline\n",
    "    from src.execution.batch import prepare_batch_items\n",
    "    \n",
    "    # Results and analysis modules\n",
    "    from src.results.collector import ResultsCollector\n",
    "    from src.results.metrics import calculate_metrics\n",
    "    from src.analysis.visualization import create_visualizations\n",
    "    \n",
    "    logger.info(\"Successfully imported project modules\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    logger.error(f\"Error importing project modules: {e}\")\n",
    "    logger.warning(\"Some functionality may be limited. Check your project structure and path configuration.\")\n",
    "    print(f\"‚ö†Ô∏è Warning: Failed to import some project modules: {e}\")\n",
    "    print(\"   This may happen if you haven't implemented these modules yet.\")\n",
    "    print(\"   The notebook will attempt to continue, but some cells may need modification.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of imported libraries\n",
    "print(\"üìö Core Libraries Imported:\")\n",
    "print(f\"üî¢ NumPy: {np.__version__}\")\n",
    "print(f\"üìä Pandas: {pd.__version__}\")\n",
    "print(f\"üß† PyTorch: {torch.__version__}\")\n",
    "print(f\"ü§ñ Transformers: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU acceleration\n",
    "if torch.cuda.is_available():\n",
    "    print(\"‚úÖ CUDA is available for GPU acceleration\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CUDA is not available, using CPU (this will be slow)\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"üîß Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Configuration\n",
    "### Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Configuration\n",
    "\n",
    "# Import configuration modules if not already imported\n",
    "try:\n",
    "    from src.config.environment import get_environment_config, EnvironmentConfig\n",
    "except ImportError:\n",
    "    # Fallback implementation if modules aren't available yet\n",
    "    logger.warning(\"Using fallback environment configuration\")\n",
    "    \n",
    "    class EnvironmentConfig:\n",
    "        \"\"\"Fallback Environment Configuration class\"\"\"\n",
    "        def __init__(self, env_type=\"auto\"):\n",
    "            self.environment = \"runpod\" if is_runpod() else \"local\"\n",
    "            self.gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n",
    "            self.max_batch_size = 1 if self.environment == \"local\" else 4\n",
    "            self.optimize_memory = True\n",
    "            self.use_4bit_quantization = self.environment == \"runpod\" and self.gpu_memory_gb < 40\n",
    "            self.use_half_precision = True\n",
    "            \n",
    "        def print_summary(self):\n",
    "            \"\"\"Print a summary of the environment configuration\"\"\"\n",
    "            print(f\"Environment: {self.environment}\")\n",
    "            print(f\"GPU Memory: {self.gpu_memory_gb:.2f} GB\")\n",
    "            print(f\"Max Batch Size: {self.max_batch_size}\")\n",
    "            print(f\"Memory Optimization: {'Enabled' if self.optimize_memory else 'Disabled'}\")\n",
    "            print(f\"4-bit Quantization: {'Enabled' if self.use_4bit_quantization else 'Disabled'}\")\n",
    "            print(f\"Half Precision: {'Enabled' if self.use_half_precision else 'Disabled'}\")\n",
    "    \n",
    "    def get_environment_config(env_type=\"auto\"):\n",
    "        \"\"\"Fallback function to get environment configuration\"\"\"\n",
    "        return EnvironmentConfig(env_type)\n",
    "\n",
    "# Detect whether we're in a RunPod or local environment\n",
    "env_type = \"runpod\" if is_runpod() else \"local\"\n",
    "print(f\"üîç Detected environment type: {env_type}\")\n",
    "\n",
    "# Load appropriate configuration\n",
    "try:\n",
    "    # Get environment configuration\n",
    "    env_config = get_environment_config(env_type)\n",
    "    print(f\"üìå Loaded {env_config.environment} environment configuration\")\n",
    "    \n",
    "    # Load environment-specific settings from YAML if available\n",
    "    yaml_config_path = project_root / \"configs\" / \"environments\" / f\"{env_config.environment}.yaml\"\n",
    "    if os.path.exists(yaml_config_path):\n",
    "        import yaml\n",
    "        with open(yaml_config_path, 'r') as f:\n",
    "            yaml_config = yaml.safe_load(f)\n",
    "        print(f\"üìÑ Loaded environment settings from {yaml_config_path}\")\n",
    "        \n",
    "        # Override config settings with YAML values if present\n",
    "        for key, value in yaml_config.items():\n",
    "            if hasattr(env_config, key):\n",
    "                setattr(env_config, key, value)\n",
    "                print(f\"  ‚Ü≥ Set {key} = {value}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è No environment config file found at {yaml_config_path}, using defaults\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading environment configuration: {e}\")\n",
    "    print(\"‚ö†Ô∏è Using default environment settings\")\n",
    "    env_config = EnvironmentConfig(env_type)\n",
    "\n",
    "# Set up GPU memory optimization based on environment config\n",
    "if torch.cuda.is_available() and hasattr(env_config, 'optimize_memory') and env_config.optimize_memory:\n",
    "    # Set up memory optimization for better GPU usage\n",
    "    if hasattr(torch.cuda, 'empty_cache'):\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"üßπ Cleared CUDA cache\")\n",
    "    \n",
    "    # Set memory configuration based on environment\n",
    "    if env_config.environment == \"runpod\":\n",
    "        # More aggressive optimization for RunPod\n",
    "        torch.cuda.set_per_process_memory_fraction(0.95)  # Use up to 95% of GPU memory\n",
    "        print(\"‚öôÔ∏è Set GPU memory fraction to 95% for RunPod environment\")\n",
    "    else:\n",
    "        # More conservative for local environment\n",
    "        torch.cuda.set_per_process_memory_fraction(0.8)  # Use up to 80% of GPU memory\n",
    "        print(\"‚öôÔ∏è Set GPU memory fraction to 80% for local environment\")\n",
    "\n",
    "# Initialize experiment configuration\n",
    "experiment_name = f\"pixtral_extraction_{timestamp}\"\n",
    "print(f\"üß™ Initializing experiment: {experiment_name}\")\n",
    "\n",
    "# Create experiment configuration dictionary\n",
    "experiment_config = {\n",
    "    \"name\": experiment_name,\n",
    "    \"timestamp\": timestamp,\n",
    "    \"environment\": env_config.environment,\n",
    "    \"model_name\": \"pixtral-12b\",  # Default model\n",
    "    \"model_repo_id\": \"mistral-community/pixtral-12b\",  # Default repo\n",
    "    \"field_to_extract\": \"work_order\",  # What we're extracting from invoices\n",
    "    \"use_4bit_quantization\": env_config.use_4bit_quantization if hasattr(env_config, 'use_4bit_quantization') else False,\n",
    "    \"use_half_precision\": env_config.use_half_precision if hasattr(env_config, 'use_half_precision') else True,\n",
    "    \"max_batch_size\": env_config.max_batch_size if hasattr(env_config, 'max_batch_size') else 1,\n",
    "    \"description\": \"Comparing prompt strategies for work order number extraction using Pixtral-12B\"\n",
    "}\n",
    "\n",
    "# Display environment and experiment configuration summary\n",
    "print(\"\\nüìã Environment Configuration Summary:\")\n",
    "env_config.print_summary()\n",
    "\n",
    "print(\"\\nüß™ Experiment Configuration Summary:\")\n",
    "for key, value in experiment_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Save configurations for reference\n",
    "config_path = get_results_path(\"experiment_config.json\")\n",
    "with open(config_path, \"w\") as f:\n",
    "    # Create serializable config by converting Path objects to strings\n",
    "    serializable_config = {\n",
    "        \"experiment\": experiment_config,\n",
    "        \"environment\": {k: str(v) if isinstance(v, Path) else v \n",
    "                        for k, v in vars(env_config).items() \n",
    "                        if not k.startswith('_') and not callable(v)},\n",
    "        \"paths\": {k: str(v) if isinstance(v, Path) else v \n",
    "                 for k, v in paths.items() \n",
    "                 if not callable(v)}\n",
    "    }\n",
    "    json.dump(serializable_config, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Configuration saved to: {config_path}\")\n",
    "print(\"‚úÖ Environment and experiment configuration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Prompt System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize prompt system\n",
    "try:\n",
    "    from src.prompts import initialize_prompt_system\n",
    "    \n",
    "    # Initialize the prompt system\n",
    "    prompt_init_result = initialize_prompt_system()\n",
    "    if prompt_init_result[\"status\"] == \"success\":\n",
    "        print(f\"‚úÖ Prompt system initialized with {prompt_init_result['prompt_count']} prompts\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Prompt system initialization issue: {prompt_init_result.get('error', 'Unknown error')}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Prompt system initialization module not found: {e}\")\n",
    "    print(\"üìù Will use fallback prompt implementation\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error initializing prompt system: {e}\")\n",
    "    print(\"üìù Will use fallback prompt implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Selection for Comparison\n",
    "# This cell allows selecting multiple prompts from all available YAML files\n",
    "\n",
    "try:\n",
    "    # Import prompt utilities with the correct function names\n",
    "    from src.prompts.registry import get_prompt, list_all_prompts, list_prompt_categories\n",
    "    from src.prompts.prompt_utils import analyze_prompt\n",
    "    import os\n",
    "    import yaml\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Get prompts directory\n",
    "    prompts_dir = project_root / \"configs\" / \"prompts\"\n",
    "    \n",
    "    # List available YAML files\n",
    "    yaml_files = list(prompts_dir.glob(\"*.yaml\"))\n",
    "    print(f\"üìã Found {len(yaml_files)} prompt configuration files:\")\n",
    "    for i, yaml_file in enumerate(yaml_files):\n",
    "        print(f\"   {i+1}. {yaml_file.name}\")\n",
    "    \n",
    "    # List available prompts\n",
    "    available_prompts = list_all_prompts()\n",
    "    print(f\"\\nüìã Found {len(available_prompts)} available prompts\")\n",
    "    \n",
    "    # List categories for reference\n",
    "    categories = list_prompt_categories()\n",
    "    print(f\"   Available categories: {', '.join(categories)}\")\n",
    "    \n",
    "    # Function to list prompts by category\n",
    "    def list_prompts_by_category():\n",
    "        \"\"\"List all prompts organized by category\"\"\"\n",
    "        prompt_by_category = {}\n",
    "        for prompt_name in available_prompts:\n",
    "            prompt = get_prompt(prompt_name)\n",
    "            if prompt:\n",
    "                category = prompt.category\n",
    "                if category not in prompt_by_category:\n",
    "                    prompt_by_category[category] = []\n",
    "                prompt_by_category[category].append(prompt.name)\n",
    "        \n",
    "        # Print organized list\n",
    "        for category, prompts in prompt_by_category.items():\n",
    "            print(f\"\\nüìå Category: {category}\")\n",
    "            for i, prompt_name in enumerate(prompts):\n",
    "                prompt = get_prompt(prompt_name)\n",
    "                print(f\"   {i+1}. {prompt_name}: \\\"{prompt.text}\\\"\")\n",
    "    \n",
    "    # Show prompts by category\n",
    "    list_prompts_by_category()\n",
    "    \n",
    "    # Allow user to select prompts\n",
    "    print(\"\\nüîç Select prompts for comparison:\")\n",
    "    print(\"   1. Use default selection (one from each category)\")\n",
    "    print(\"   2. Select specific prompts\")\n",
    "    print(\"   3. Use all available prompts\")\n",
    "    \n",
    "    selection_mode = input(\"Enter selection mode (1-3): \").strip()\n",
    "    \n",
    "    if selection_mode == \"1\":\n",
    "        # Default selection - one from each category\n",
    "        prompt_names = []\n",
    "        for category in categories:\n",
    "            # Get first prompt of each category\n",
    "            for prompt_name in available_prompts:\n",
    "                prompt = get_prompt(prompt_name)\n",
    "                if prompt and prompt.category == category:\n",
    "                    prompt_names.append(prompt_name)\n",
    "                    break\n",
    "        \n",
    "        print(f\"\\n‚úÖ Selected {len(prompt_names)} prompts (one from each category)\")\n",
    "    \n",
    "    elif selection_mode == \"2\":\n",
    "        # User selects specific prompts\n",
    "        prompt_names = []\n",
    "        print(\"\\nEnter prompt names (comma separated):\")\n",
    "        print(\"For example: basic_work_order, detailed_work_order, positioned_work_order\")\n",
    "        prompt_input = input(\"Enter prompt names: \").strip()\n",
    "        prompt_names = [name.strip() for name in prompt_input.split(\",\")]\n",
    "        print(f\"\\n‚úÖ Selected {len(prompt_names)} prompts\")\n",
    "    \n",
    "    elif selection_mode == \"3\":\n",
    "        # Use all available prompts\n",
    "        prompt_names = available_prompts\n",
    "        print(f\"\\n‚úÖ Selected all {len(prompt_names)} available prompts\")\n",
    "    \n",
    "    else:\n",
    "        # Invalid selection, use default\n",
    "        prompt_names = [\"basic_work_order\", \"detailed_work_order\", \"positioned_work_order\"]\n",
    "        print(f\"\\n‚ö†Ô∏è Invalid selection. Using default prompts: {', '.join(prompt_names)}\")\n",
    "    \n",
    "    # Container for prompt information\n",
    "    prompt_collection = []\n",
    "    \n",
    "    # Load each prompt\n",
    "    print(\"\\nüîç Loading selected prompts:\")\n",
    "    for prompt_name in prompt_names:\n",
    "        selected_prompt = get_prompt(prompt_name)\n",
    "        \n",
    "        if selected_prompt:\n",
    "            print(f\"\\n‚úÖ Loaded prompt: \\\"{selected_prompt.name}\\\"\")\n",
    "            print(f\"   Category: {selected_prompt.category}\")\n",
    "            print(f\"   Field: {selected_prompt.field_to_extract}\")\n",
    "            print(f\"   Prompt text: \\\"{selected_prompt.text}\\\"\")\n",
    "            \n",
    "            # Format the prompt for the Pixtral model\n",
    "            model_name = \"pixtral-12b\"\n",
    "            formatted_prompt = selected_prompt.format_for_model(model_name)\n",
    "            \n",
    "            # Analyze the prompt\n",
    "            try:\n",
    "                prompt_analysis = analyze_prompt(selected_prompt)\n",
    "            except (ImportError, NameError):\n",
    "                # Simple fallback analysis if function doesn't exist\n",
    "                word_count = len(selected_prompt.text.split())\n",
    "                char_count = len(selected_prompt.text)\n",
    "                prompt_analysis = {\n",
    "                    \"word_count\": word_count,\n",
    "                    \"character_count\": char_count,\n",
    "                    \"complexity_score\": word_count / 10  # Simple metric\n",
    "                }\n",
    "            \n",
    "            # Store prompt info\n",
    "            prompt_info = {\n",
    "                \"name\": selected_prompt.name,\n",
    "                \"text\": selected_prompt.text,\n",
    "                \"formatted_text\": formatted_prompt,\n",
    "                \"category\": selected_prompt.category,\n",
    "                \"field_to_extract\": selected_prompt.field_to_extract,\n",
    "                \"analysis\": prompt_analysis\n",
    "            }\n",
    "            \n",
    "            # Add to collection\n",
    "            prompt_collection.append(prompt_info)\n",
    "        else:\n",
    "            print(f\"‚ùå Error: Prompt '{prompt_name}' not found in registry\")\n",
    "    \n",
    "    # Print a summary of loaded prompts\n",
    "    print(f\"\\nüìä Loaded {len(prompt_collection)} prompts for comparison\")\n",
    "    \n",
    "    # Create a table comparing prompt characteristics\n",
    "    if prompt_collection:\n",
    "        import pandas as pd\n",
    "        prompt_df = pd.DataFrame([{\n",
    "            \"Name\": p[\"name\"],\n",
    "            \"Category\": p[\"category\"],\n",
    "            \"Word Count\": p[\"analysis\"][\"word_count\"],\n",
    "            \"Character Count\": p[\"analysis\"][\"character_count\"],\n",
    "            \"Complexity\": p[\"analysis\"].get(\"complexity_score\", \"N/A\")\n",
    "        } for p in prompt_collection])\n",
    "        \n",
    "        print(\"\\nüìã Prompt Comparison:\")\n",
    "        display(prompt_df)\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing prompt modules: {e}\")\n",
    "    print(\"   Using direct implementation instead\")\n",
    "    \n",
    "    # Define fallback prompts if imports fail\n",
    "    prompt_collection = [\n",
    "        {\n",
    "            \"name\": \"basic_work_order\",\n",
    "            \"text\": \"Extract the work order number from this invoice image.\",\n",
    "            \"formatted_text\": \"<s>[INST]Extract the work order number from this invoice image.\\n[IMG][/INST]\",\n",
    "            \"category\": \"basic\",\n",
    "            \"field_to_extract\": \"work_order\",\n",
    "            \"analysis\": {\n",
    "                \"word_count\": 8,\n",
    "                \"character_count\": 50,\n",
    "                \"complexity_score\": 0.8\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"detailed_work_order\",\n",
    "            \"text\": \"This is an invoice image. Please find and extract the work order number. It may be labeled as 'Work Order Number', 'Numero de Orden', 'Order #', or similar.\",\n",
    "            \"formatted_text\": \"<s>[INST]This is an invoice image. Please find and extract the work order number. It may be labeled as 'Work Order Number', 'Numero de Orden', 'Order #', or similar.\\n[IMG][/INST]\",\n",
    "            \"category\": \"detailed\",\n",
    "            \"field_to_extract\": \"work_order\",\n",
    "            \"analysis\": {\n",
    "                \"word_count\": 26,\n",
    "                \"character_count\": 146,\n",
    "                \"complexity_score\": 2.6\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"positioned_work_order\",\n",
    "            \"text\": \"Extract the work order number from this invoice image. The work order number is typically located in the upper portion of the invoice.\",\n",
    "            \"formatted_text\": \"<s>[INST]Extract the work order number from this invoice image. The work order number is typically located in the upper portion of the invoice.\\n[IMG][/INST]\",\n",
    "            \"category\": \"positioned\",\n",
    "            \"field_to_extract\": \"work_order\",\n",
    "            \"analysis\": {\n",
    "                \"word_count\": 21,\n",
    "                \"character_count\": 118,\n",
    "                \"complexity_score\": 2.1\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"‚úÖ Using fallback prompt collection:\")\n",
    "    for p in prompt_collection:\n",
    "        print(f\"   - {p['name']}: \\\"{p['text']}\\\"\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during prompt selection: {e}\")\n",
    "    print(\"   Using fallback basic prompt collection instead\")\n",
    "    # Use same fallback as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Parameters for Prompt Comparison\n",
    "\n",
    "# Import experiment configuration utilities if available\n",
    "try:\n",
    "    from src.config.experiment import get_experiment_config, ExperimentConfig\n",
    "except ImportError:\n",
    "    logger.warning(\"Using fallback experiment configuration\")\n",
    "    # Continue with manual configuration\n",
    "\n",
    "# Define experiment details\n",
    "print(\"üß™ Setting up experiment parameters for prompt comparison\")\n",
    "\n",
    "# Define experiment name and basic metadata\n",
    "experiment_details = {\n",
    "    \"name\": f\"prompt_comparison_{timestamp}\",\n",
    "    \"timestamp\": timestamp,\n",
    "    \"description\": \"Comparing prompt strategies for work order number extraction using Pixtral-12B\",\n",
    "    \"version\": \"1.0.0\",\n",
    "    \"owner\": os.environ.get(\"USER\", \"unknown\"),\n",
    "    \"prompts_compared\": len(prompt_collection)\n",
    "}\n",
    "\n",
    "# Model configuration settings\n",
    "model_config = {\n",
    "    \"name\": \"pixtral-12b\",\n",
    "    \"repo_id\": \"mistral-community/pixtral-12b\",\n",
    "    \"revision\": None,  # Use latest version\n",
    "    \"dtype\": \"bfloat16\" if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else \"float16\",\n",
    "    \"device_map\": \"auto\",  # Let the library decide the optimal device mapping\n",
    "    \"use_quantization\": env_config.use_4bit_quantization if hasattr(env_config, \"use_4bit_quantization\") else False,\n",
    "    \"quantization_bits\": 4 if hasattr(env_config, \"use_4bit_quantization\") and env_config.use_4bit_quantization else None,\n",
    "    \"cache_dir\": str(paths[\"models_cache_dir\"]),\n",
    "    \"trust_remote_code\": True,  # Needed for some models\n",
    "    \"use_flash_attention\": torch.cuda.is_available() and torch.__version__ >= \"2.0.0\",\n",
    "}\n",
    "\n",
    "# Extraction task parameters - using the field from the first prompt (should be the same for all)\n",
    "if prompt_collection:\n",
    "    field_to_extract = prompt_collection[0][\"field_to_extract\"]\n",
    "else:\n",
    "    field_to_extract = \"work_order\"  # Default if no prompts loaded\n",
    "\n",
    "extraction_config = {\n",
    "    \"field_to_extract\": field_to_extract,\n",
    "    \"field_column_name\": \"Work Order Number/Numero de Orden\",  # Column name in ground truth CSV\n",
    "    \"prompts\": [{\n",
    "        \"name\": p[\"name\"],\n",
    "        \"category\": p[\"category\"],\n",
    "        \"text\": p[\"text\"],\n",
    "        \"formatted_text\": p[\"formatted_text\"]\n",
    "    } for p in prompt_collection]\n",
    "}\n",
    "\n",
    "# Evaluation metrics configuration\n",
    "metrics_config = {\n",
    "    \"primary_metric\": \"exact_match\",\n",
    "    \"secondary_metrics\": [\"character_error_rate\", \"levenshtein_distance\", \"processing_time\"],\n",
    "    \"exact_match_case_sensitive\": False,  # Ignore case when comparing\n",
    "    \"exact_match_strip_whitespace\": True,  # Ignore whitespace when comparing\n",
    "    \"cer_normalize\": True,  # Normalize character error rate by length\n",
    "}\n",
    "\n",
    "# Pipeline execution settings\n",
    "execution_config = {\n",
    "    \"max_batch_size\": env_config.max_batch_size if hasattr(env_config, \"max_batch_size\") else 1,\n",
    "    \"save_checkpoints\": True,\n",
    "    \"checkpoint_frequency\": 5,  # Save after every 5 images\n",
    "    \"resume_from_checkpoint\": True,\n",
    "    \"timeout_per_image\": 60,  # Maximum seconds per image\n",
    "    \"max_retries\": 3,  # Retry failed images\n",
    "    \"log_gpu_memory\": True,\n",
    "}\n",
    "\n",
    "# Simplified comparison configuration\n",
    "comparison_config = {\n",
    "    \"prompts_compared\": len(prompt_collection),\n",
    "    \"image_count\": 20,  # Number of images to process\n",
    "    \"metrics_to_compare\": [\"exact_match_rate\", \"character_error_rate\", \"processing_time\"]\n",
    "}\n",
    "\n",
    "# Combine all configurations into master experiment config\n",
    "full_experiment_config = {\n",
    "    \"experiment\": experiment_details,\n",
    "    \"model\": model_config,\n",
    "    \"extraction\": extraction_config,\n",
    "    \"metrics\": metrics_config,\n",
    "    \"execution\": execution_config,\n",
    "    \"comparison\": comparison_config,\n",
    "    \"environment\": env_config.environment,\n",
    "    \"prompts\": prompt_collection  # Add all prompts\n",
    "}\n",
    "\n",
    "# Display summary of experiment configuration\n",
    "print(\"\\nüìã Experiment Details:\")\n",
    "print(f\"   Name: {experiment_details['name']}\")\n",
    "print(f\"   Description: {experiment_details['description']}\")\n",
    "print(f\"   Timestamp: {experiment_details['timestamp']}\")\n",
    "print(f\"   Comparing {len(prompt_collection)} different prompts\")\n",
    "\n",
    "print(\"\\nü§ñ Model Configuration:\")\n",
    "print(f\"   Model: {model_config['name']}\")\n",
    "print(f\"   Repository: {model_config['repo_id']}\")\n",
    "print(f\"   Data Type: {model_config['dtype']}\")\n",
    "print(f\"   Quantization: {'Enabled (' + str(model_config['quantization_bits']) + '-bit)' if model_config['use_quantization'] else 'Disabled'}\")\n",
    "print(f\"   Flash Attention: {'Enabled' if model_config['use_flash_attention'] else 'Disabled'}\")\n",
    "\n",
    "print(\"\\nüîç Extraction Task:\")\n",
    "print(f\"   Field to Extract: {extraction_config['field_to_extract']}\")\n",
    "print(f\"   Ground Truth Column: {extraction_config['field_column_name']}\")\n",
    "print(f\"   Number of Prompts: {len(extraction_config['prompts'])}\")\n",
    "\n",
    "print(\"\\nüìù Prompts to Compare:\")\n",
    "for i, prompt in enumerate(prompt_collection, 1):\n",
    "    print(f\"   {i}. {prompt['name']} ({prompt['category']}): \\\"{prompt['text']}\\\"\")\n",
    "\n",
    "print(\"\\nüìä Evaluation Metrics:\")\n",
    "print(f\"   Primary Metric: {metrics_config['primary_metric']}\")\n",
    "print(f\"   Secondary Metrics: {', '.join(metrics_config['secondary_metrics'])}\")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Execution Settings:\")\n",
    "print(f\"   Max Batch Size: {execution_config['max_batch_size']}\")\n",
    "print(f\"   Checkpointing: {'Enabled' if execution_config['save_checkpoints'] else 'Disabled'}\")\n",
    "if execution_config['save_checkpoints']:\n",
    "    print(f\"   Checkpoint Frequency: Every {execution_config['checkpoint_frequency']} images\")\n",
    "\n",
    "print(\"\\nüîç Comparison Settings:\")\n",
    "print(f\"   Prompts to Compare: {comparison_config['prompts_compared']}\")\n",
    "print(f\"   Images per Prompt: {comparison_config['image_count']}\")\n",
    "print(f\"   Metrics to Compare: {', '.join(comparison_config['metrics_to_compare'])}\")\n",
    "\n",
    "# Save full experiment configuration to file\n",
    "exp_config_path = get_results_path(\"full_experiment_config.json\")\n",
    "with open(exp_config_path, \"w\") as f:\n",
    "    json.dump(full_experiment_config, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Full experiment configuration saved to: {exp_config_path}\")\n",
    "print(\"‚úÖ Experiment parameters configured successfully!\")\n",
    "\n",
    "# Make config available for other cells\n",
    "experiment_config = full_experiment_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground Truth Data Loading\n",
    "\n",
    "# Import data handling utilities\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Try to import project-specific data utilities\n",
    "try:\n",
    "    from src.data.loader import load_ground_truth, map_images_to_ground_truth\n",
    "    from src.execution.batch import prepare_batch_items\n",
    "    print(\"‚úÖ Imported data handling utilities\")\n",
    "    use_project_utils = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Project data utilities not available, using direct implementation\")\n",
    "    use_project_utils = False\n",
    "\n",
    "# Load ground truth CSV data\n",
    "print(\"\\nüìä Loading ground truth data...\")\n",
    "\n",
    "ground_truth_path = paths[\"ground_truth_path\"]\n",
    "if not os.path.exists(ground_truth_path):\n",
    "    print(f\"‚ùå Ground truth file not found at: {ground_truth_path}\")\n",
    "    raise FileNotFoundError(f\"Ground truth file not found: {ground_truth_path}\")\n",
    "\n",
    "try:\n",
    "    # Load the CSV file\n",
    "    ground_truth_df = pd.read_csv(ground_truth_path)\n",
    "    print(f\"‚úÖ Loaded ground truth data: {len(ground_truth_df)} records\")\n",
    "    \n",
    "    # Display basic information\n",
    "    print(f\"\\nFirst few rows of ground truth data:\")\n",
    "    display(ground_truth_df.head())\n",
    "    \n",
    "    # Display column information\n",
    "    print(f\"\\nColumns in ground truth data:\")\n",
    "    for i, col in enumerate(ground_truth_df.columns):\n",
    "        print(f\"  {i+1}. {col}\")\n",
    "    \n",
    "    # Check for any missing values in key fields\n",
    "    field_column = extraction_config[\"field_column_name\"]\n",
    "    if field_column not in ground_truth_df.columns:\n",
    "        print(f\"‚ùå Field column '{field_column}' not found in ground truth data\")\n",
    "        print(f\"   Available columns: {', '.join(ground_truth_df.columns)}\")\n",
    "        raise ValueError(f\"Field column '{field_column}' not found in ground truth data\")\n",
    "    \n",
    "    missing_values = ground_truth_df[field_column].isna().sum()\n",
    "    if missing_values > 0:\n",
    "        print(f\"‚ö†Ô∏è Found {missing_values} missing values in {field_column} field\")\n",
    "    \n",
    "    # Map images to ground truth values\n",
    "    print(\"\\nüîÑ Mapping images to ground truth values...\")\n",
    "    \n",
    "    # Get image files\n",
    "    image_dir = paths[\"images_dir\"]\n",
    "    image_files = list(image_dir.glob(\"*.jpg\")) + list(image_dir.glob(\"*.png\"))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"‚ùå No image files found in {image_dir}\")\n",
    "        raise FileNotFoundError(f\"No image files found in {image_dir}\")\n",
    "    \n",
    "    print(f\"üì∏ Found {len(image_files)} image files\")\n",
    "    \n",
    "    # Determine the image ID column (typically 'Invoice' or similar)\n",
    "    # This could be configurable in your extraction_config\n",
    "    image_id_column = \"Invoice\"  # Default, adjust if your CSV uses a different column\n",
    "    \n",
    "    if image_id_column not in ground_truth_df.columns:\n",
    "        print(f\"‚ö†Ô∏è Image ID column '{image_id_column}' not found, trying alternatives...\")\n",
    "        # Try to find a suitable column\n",
    "        for candidate in [\"invoice\", \"image\", \"filename\", \"file\", \"id\"]:\n",
    "            matches = [col for col in ground_truth_df.columns if candidate in col.lower()]\n",
    "            if matches:\n",
    "                image_id_column = matches[0]\n",
    "                print(f\"‚úÖ Using '{image_id_column}' as image ID column\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"‚ùå Could not find a suitable image ID column\")\n",
    "            print(f\"   Please specify the column that contains image identifiers\")\n",
    "            raise ValueError(\"Image ID column not found\")\n",
    "    \n",
    "    # Create mapping between images and ground truth\n",
    "    ground_truth_mapping = {}\n",
    "    unmatched_images = []\n",
    "    \n",
    "    for image_path in image_files:\n",
    "        image_id = image_path.stem  # Get filename without extension\n",
    "        \n",
    "        # Find matching row in ground truth\n",
    "        # Converting to string to ensure matching works with numeric IDs\n",
    "        matching_row = ground_truth_df[ground_truth_df[image_id_column].astype(str) == image_id]\n",
    "        \n",
    "        if len(matching_row) > 0:\n",
    "            row = matching_row.iloc[0]\n",
    "            ground_truth_mapping[image_id] = {\n",
    "                'image_path': str(image_path),\n",
    "                'work_order': str(row[field_column]).strip(),\n",
    "                'image_id': image_id\n",
    "            }\n",
    "        else:\n",
    "            unmatched_images.append(image_id)\n",
    "            print(f\"‚ö†Ô∏è No matching ground truth found for image {image_id}\")\n",
    "    \n",
    "    print(f\"‚úÖ Successfully mapped {len(ground_truth_mapping)} images to ground truth data\")\n",
    "    \n",
    "    if unmatched_images:\n",
    "        print(f\"‚ö†Ô∏è Found {len(unmatched_images)} images without ground truth data\")\n",
    "        if len(unmatched_images) < 10:\n",
    "            print(f\"   Unmatched images: {', '.join(unmatched_images)}\")\n",
    "        else:\n",
    "            print(f\"   First 10 unmatched images: {', '.join(unmatched_images[:10])}...\")\n",
    "    \n",
    "    # Create structured batch items for processing\n",
    "    print(\"\\nüîÑ Creating structured batch items for processing...\")\n",
    "    \n",
    "    batch_items = []\n",
    "    for image_id, data in ground_truth_mapping.items():\n",
    "        batch_items.append({\n",
    "            \"image_id\": image_id,\n",
    "            \"image_path\": data[\"image_path\"],\n",
    "            \"ground_truth\": data[\"work_order\"],\n",
    "            \"field_type\": extraction_config[\"field_to_extract\"]\n",
    "        })\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(batch_items)} batch items\")\n",
    "    \n",
    "    # Display statistics and sample mapping\n",
    "    print(\"\\nüìä Ground Truth Statistics:\")\n",
    "    \n",
    "    # Count unique work order values\n",
    "    unique_work_orders = len(set(data[\"work_order\"] for data in ground_truth_mapping.values()))\n",
    "    print(f\"   Total Images: {len(ground_truth_mapping)}\")\n",
    "    print(f\"   Unique Work Orders: {unique_work_orders}\")\n",
    "    \n",
    "    # Calculate average work order length\n",
    "    work_order_lengths = [len(data[\"work_order\"]) for data in ground_truth_mapping.values()]\n",
    "    avg_length = sum(work_order_lengths) / len(work_order_lengths) if work_order_lengths else 0\n",
    "    print(f\"   Average Work Order Length: {avg_length:.2f} characters\")\n",
    "    print(f\"   Min Length: {min(work_order_lengths) if work_order_lengths else 0} characters\")\n",
    "    print(f\"   Max Length: {max(work_order_lengths) if work_order_lengths else 0} characters\")\n",
    "    \n",
    "    # Display sample of the mapping\n",
    "    print(\"\\nüìã Sample of Ground Truth Mapping:\")\n",
    "    random_samples = random.sample(list(ground_truth_mapping.items()), min(5, len(ground_truth_mapping)))\n",
    "    for i, (img_id, data) in enumerate(random_samples, 1):\n",
    "        print(f\"   {i}. Image: {img_id}\")\n",
    "        print(f\"      ‚Ü≥ Path: {data['image_path']}\")\n",
    "        print(f\"      ‚Ü≥ Work Order: '{data['work_order']}'\")\n",
    "    \n",
    "    # Save the ground truth mapping for reference\n",
    "    mapping_path = get_results_path(\"ground_truth_mapping.json\")\n",
    "    with open(mapping_path, \"w\") as f:\n",
    "        # Convert to serializable format (paths need to be strings)\n",
    "        serializable_mapping = {\n",
    "            img_id: {k: v for k, v in data.items()}\n",
    "            for img_id, data in ground_truth_mapping.items()\n",
    "        }\n",
    "        json.dump(serializable_mapping, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Ground truth mapping saved to: {mapping_path}\")\n",
    "    print(\"‚úÖ Ground truth data loading complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading ground truth data: {e}\")\n",
    "    ground_truth_df = None\n",
    "    ground_truth_mapping = {}\n",
    "    batch_items = []\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "### GPU Verification & Memory Check / Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Verification\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Try to import project-specific utilities\n",
    "try:\n",
    "    from src.models.loader import get_gpu_memory_info, verify_gpu_compatibility\n",
    "    print(\"‚úÖ Imported GPU verification utilities\")\n",
    "    use_project_utils = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Project GPU utilities not available, using direct implementation\")\n",
    "    use_project_utils = False\n",
    "    \n",
    "    # Implement fallback functions\n",
    "    def get_gpu_memory_info():\n",
    "        \"\"\"Get GPU memory information\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return {\n",
    "                \"total_memory_gb\": 0,\n",
    "                \"allocated_memory_gb\": 0,\n",
    "                \"free_memory_gb\": 0,\n",
    "                \"utilization\": 0\n",
    "            }\n",
    "        \n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "        allocated_memory = torch.cuda.memory_allocated(0)\n",
    "        reserved_memory = torch.cuda.memory_reserved(0)\n",
    "        free_memory = total_memory - allocated_memory\n",
    "        \n",
    "        # Convert to GB\n",
    "        total_memory_gb = total_memory / 1e9\n",
    "        allocated_memory_gb = allocated_memory / 1e9\n",
    "        free_memory_gb = free_memory / 1e9\n",
    "        utilization = (allocated_memory / total_memory) * 100\n",
    "        \n",
    "        return {\n",
    "            \"total_memory_gb\": total_memory_gb,\n",
    "            \"allocated_memory_gb\": allocated_memory_gb,\n",
    "            \"free_memory_gb\": free_memory_gb,\n",
    "            \"utilization\": utilization\n",
    "        }\n",
    "    \n",
    "    def verify_gpu_compatibility(model_name):\n",
    "        \"\"\"Check if GPU is compatible with model requirements\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return {\n",
    "                \"compatible\": False,\n",
    "                \"reason\": \"No GPU available\",\n",
    "                \"model_requirements\": {\"minimum_memory_gb\": 24},\n",
    "                \"current_gpu\": {\"name\": \"None\", \"memory_gb\": 0}\n",
    "            }\n",
    "        \n",
    "        # Get GPU info\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        total_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        \n",
    "        # Define model requirements\n",
    "        # Pixtral-12B typically needs around 24GB without quantization\n",
    "        model_requirements = {\n",
    "            \"pixtral-12b\": {\"minimum_memory_gb\": 24},\n",
    "            \"pixtral-7b\": {\"minimum_memory_gb\": 16},\n",
    "            \"default\": {\"minimum_memory_gb\": 12}\n",
    "        }\n",
    "        \n",
    "        # Get requirements for this model\n",
    "        req = model_requirements.get(model_name.lower(), model_requirements[\"default\"])\n",
    "        \n",
    "        # Check compatibility\n",
    "        compatible = total_memory_gb >= req[\"minimum_memory_gb\"]\n",
    "        \n",
    "        return {\n",
    "            \"compatible\": compatible,\n",
    "            \"reason\": None if compatible else f\"Insufficient GPU memory: {total_memory_gb:.1f}GB available, {req['minimum_memory_gb']}GB required\",\n",
    "            \"model_requirements\": req,\n",
    "            \"current_gpu\": {\n",
    "                \"name\": gpu_name,\n",
    "                \"memory_gb\": total_memory_gb\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Check for GPU availability\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"‚ùå No GPU detected!\")\n",
    "    print(\"   This model requires a GPU for reasonable processing times.\")\n",
    "    print(\"   If you're on RunPod, verify that the GPU is properly allocated.\")\n",
    "    raise RuntimeError(\"No GPU available for model loading\")\n",
    "\n",
    "# Get model name from experiment configuration\n",
    "model_name = model_config[\"name\"]\n",
    "print(f\"üîç Checking GPU compatibility for {model_name}...\")\n",
    "\n",
    "# Check GPU compatibility\n",
    "compatibility = verify_gpu_compatibility(model_name)\n",
    "\n",
    "# Debug the compatibility object structure\n",
    "print(f\"DEBUG - Compatibility object structure: {compatibility}\")\n",
    "\n",
    "# Use defensively coded approach to handle different structures\n",
    "if compatibility.get(\"compatible\", False):\n",
    "    print(f\"‚úÖ GPU is compatible with {model_name}\")\n",
    "    \n",
    "    # Get GPU info, using direct access if keys don't exist\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    # Try to get from compatibility if available\n",
    "    if isinstance(compatibility.get(\"current_gpu\"), dict):\n",
    "        gpu_name = compatibility[\"current_gpu\"].get(\"name\", gpu_name)\n",
    "        gpu_memory = compatibility[\"current_gpu\"].get(\"memory_gb\", gpu_memory)\n",
    "    \n",
    "    print(f\"   GPU: {gpu_name}\")\n",
    "    print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # Get requirements\n",
    "    req_memory = 24  # Default\n",
    "    if isinstance(compatibility.get(\"model_requirements\"), dict):\n",
    "        req_memory = compatibility[\"model_requirements\"].get(\"minimum_memory_gb\", req_memory)\n",
    "    \n",
    "    print(f\"   Required: {req_memory} GB\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Warning: {compatibility.get('reason', 'GPU may not be compatible')}\")\n",
    "    print(f\"   Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"   Proceeding with quantization enabled to reduce memory requirements\")\n",
    "    \n",
    "    # Enable quantization if not already enabled\n",
    "    if not model_config[\"use_quantization\"]:\n",
    "        print(\"üîÑ Automatically enabling 4-bit quantization to fit model in memory\")\n",
    "        model_config[\"use_quantization\"] = True\n",
    "        model_config[\"quantization_bits\"] = 4\n",
    "\n",
    "# Display pre-loading memory state\n",
    "print(\"\\nüß† Pre-loading GPU memory state:\")\n",
    "pre_memory = get_gpu_memory_info()\n",
    "print(f\"   Total memory: {pre_memory['total_memory_gb']:.2f} GB\")\n",
    "print(f\"   Allocated memory: {pre_memory['allocated_memory_gb']:.2f} GB\")\n",
    "print(f\"   Free memory: {pre_memory['free_memory_gb']:.2f} GB\")\n",
    "print(f\"   Utilization: {pre_memory.get('utilization', pre_memory.get('utilization_percent', 0)):.1f}%\")\n",
    "\n",
    "# Clear memory before loading\n",
    "print(\"\\nüßπ Cleaning up GPU memory before model loading...\")\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# Verify memory after cleanup\n",
    "post_cleanup_memory = get_gpu_memory_info()\n",
    "print(f\"   Memory after cleanup: {post_cleanup_memory['allocated_memory_gb']:.2f} GB allocated\")\n",
    "print(f\"   Free memory: {post_cleanup_memory['free_memory_gb']:.2f} GB\")\n",
    "\n",
    "# Configure model loading parameters based on compatibility check\n",
    "print(\"\\n‚öôÔ∏è Configuring model loading parameters...\")\n",
    "\n",
    "# Set up quantization configuration if enabled\n",
    "quantization_config = None\n",
    "if model_config[\"use_quantization\"]:\n",
    "    try:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        \n",
    "        bits = model_config[\"quantization_bits\"]\n",
    "        print(f\"   Using {bits}-bit quantization\")\n",
    "        \n",
    "        if bits == 4:\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "            print(\"   Configured 4-bit NormalFloat quantization\")\n",
    "        elif bits == 8:\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_8bit=True\n",
    "            )\n",
    "            print(\"   Configured 8-bit quantization\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è BitsAndBytesConfig not available, quantization will be limited\")\n",
    "        quantization_config = {\"load_in_4bit\": True}\n",
    "\n",
    "# Determine precision (dtype)\n",
    "if torch.cuda.is_available():\n",
    "    if torch.cuda.get_device_capability()[0] >= 8:  # Ampere or newer\n",
    "        dtype = torch.bfloat16\n",
    "        print(\"   Using bfloat16 precision (optimal for Ampere+ GPUs)\")\n",
    "    else:\n",
    "        dtype = torch.float16\n",
    "        print(\"   Using float16 precision (for older GPUs)\")\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "    print(\"   Using float32 precision (CPU mode)\")\n",
    "\n",
    "# Set up model loading configuration\n",
    "model_loading_config = {\n",
    "    \"model_name\": model_config[\"repo_id\"],\n",
    "    \"revision\": model_config[\"revision\"],\n",
    "    \"device_map\": model_config[\"device_map\"],\n",
    "    \"torch_dtype\": dtype,\n",
    "    \"quantization_config\": quantization_config,\n",
    "    \"cache_dir\": Path(model_config[\"cache_dir\"]),\n",
    "    \"trust_remote_code\": model_config[\"trust_remote_code\"]\n",
    "}\n",
    "\n",
    "# Save pre-loading state for comparison\n",
    "pre_loading_info = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model_name\": model_name,\n",
    "    \"gpu_info\": {\n",
    "        \"name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\",\n",
    "        \"total_memory_gb\": pre_memory[\"total_memory_gb\"],\n",
    "        \"allocated_memory_gb\": pre_memory[\"allocated_memory_gb\"],\n",
    "        \"free_memory_gb\": pre_memory[\"free_memory_gb\"]\n",
    "    },\n",
    "    \"quantization_enabled\": model_config[\"use_quantization\"],\n",
    "    \"quantization_bits\": model_config[\"quantization_bits\"],\n",
    "    \"precision\": str(dtype)\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "pre_loading_path = get_results_path(\"pre_loading_gpu_state.json\")\n",
    "with open(pre_loading_path, \"w\") as f:\n",
    "    # Convert non-serializable types\n",
    "    serializable_info = {**pre_loading_info}\n",
    "    serializable_info[\"precision\"] = str(serializable_info[\"precision\"])\n",
    "    json.dump(serializable_info, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Pre-loading GPU state saved to: {pre_loading_path}\")\n",
    "print(\"‚úÖ GPU verification complete and model loading configured!\")\n",
    "\n",
    "# Make model loading configuration available to next cell\n",
    "model_load_params = model_loading_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Loading\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, AutoModelForVision2Seq\n",
    "from datetime import datetime\n",
    "\n",
    "# Try to import project-specific model loading utilities\n",
    "try:\n",
    "    from src.models.loader import load_model_and_processor, get_gpu_memory_info\n",
    "    print(\"‚úÖ Imported model loading utilities\")\n",
    "    use_project_utils = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Project model utilities not available, using direct implementation\")\n",
    "    use_project_utils = False\n",
    "\n",
    "# Record start time for loading\n",
    "start_time = time.time()\n",
    "\n",
    "# Display loading message\n",
    "model_name = model_config[\"name\"]\n",
    "repo_id = model_config[\"repo_id\"]\n",
    "print(f\"‚è≥ Loading {model_name} from {repo_id}...\")\n",
    "print(f\"   This may take several minutes depending on your internet connection and GPU speed.\")\n",
    "\n",
    "try:\n",
    "    # Method 1: Use project utility if available\n",
    "    if use_project_utils:\n",
    "        print(\"Using project model loading utility...\")\n",
    "        # Remove model_name from model_load_params to avoid duplicate\n",
    "        model_params = model_load_params.copy()\n",
    "        if 'model_name' in model_params:\n",
    "            del model_params['model_name']  # Remove to avoid duplication\n",
    "        \n",
    "        model, processor = load_model_and_processor(\n",
    "            model_name=model_name,  # Pass this directly\n",
    "            **model_params  # Pass remaining parameters\n",
    "        )\n",
    "    \n",
    "    # Method 2: Direct loading if project utility not available\n",
    "    else:\n",
    "        print(\"Using direct model loading...\")\n",
    "        \n",
    "        # Clear memory before loading\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Load the model with appropriate configuration\n",
    "        print(f\"Loading model: {repo_id}\")\n",
    "        model = LlavaForConditionalGeneration.from_pretrained(\n",
    "            repo_id,\n",
    "            torch_dtype=model_load_params[\"torch_dtype\"],\n",
    "            device_map=model_load_params[\"device_map\"],\n",
    "            quantization_config=model_load_params[\"quantization_config\"],\n",
    "            cache_dir=model_load_params[\"cache_dir\"],\n",
    "            trust_remote_code=model_load_params[\"trust_remote_code\"]\n",
    "        )\n",
    "        \n",
    "        # Load the processor\n",
    "        print(f\"Loading processor for: {repo_id}\")\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            repo_id,\n",
    "            cache_dir=model_load_params[\"cache_dir\"],\n",
    "            trust_remote_code=model_load_params[\"trust_remote_code\"]\n",
    "        )\n",
    "    \n",
    "    # Calculate loading time\n",
    "    loading_time = time.time() - start_time\n",
    "    print(f\"‚úÖ Model loaded successfully in {loading_time:.2f} seconds!\")\n",
    "    \n",
    "    # Get model device information\n",
    "    if hasattr(model, \"device\"):\n",
    "        model_device = model.device\n",
    "    elif hasattr(model, \"hf_device_map\"):\n",
    "        model_device = str(model.hf_device_map)\n",
    "    else:\n",
    "        model_device = \"Unknown\"\n",
    "    \n",
    "    print(f\"üìç Model loaded on device: {model_device}\")\n",
    "    \n",
    "    # Get model dtype information\n",
    "    if hasattr(model, \"dtype\"):\n",
    "        model_dtype = str(model.dtype)\n",
    "    else:\n",
    "        # Try to infer from parameters\n",
    "        for param in model.parameters():\n",
    "            model_dtype = str(param.dtype)\n",
    "            break\n",
    "    \n",
    "    print(f\"üî¢ Model data type: {model_dtype}\")\n",
    "    \n",
    "    # Display post-loading memory state\n",
    "    print(\"\\nüß† Post-loading GPU memory state:\")\n",
    "    post_memory = get_gpu_memory_info()\n",
    "    print(f\"   Total memory: {post_memory['total_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Allocated memory: {post_memory['allocated_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Free memory: {post_memory['free_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Utilization: {post_memory.get('utilization', post_memory.get('utilization_percent', 0)):.1f}%\")\n",
    "    \n",
    "    # Calculate memory usage for the model\n",
    "    memory_used = post_memory['allocated_memory_gb'] - pre_memory['allocated_memory_gb']\n",
    "    print(f\"üîç Model memory usage: {memory_used:.2f} GB\")\n",
    "    \n",
    "    # Store model metadata\n",
    "    model_metadata = {\n",
    "        \"name\": model_name,\n",
    "        \"repo_id\": repo_id,\n",
    "        \"loading_time_seconds\": loading_time,\n",
    "        \"device\": str(model_device),\n",
    "        \"dtype\": model_dtype,\n",
    "        \"loaded_at\": datetime.now().isoformat(),\n",
    "        \"memory_usage_gb\": memory_used,\n",
    "        \"quantization\": {\n",
    "            \"enabled\": model_config[\"use_quantization\"],\n",
    "            \"bits\": model_config[\"quantization_bits\"] if model_config[\"use_quantization\"] else None\n",
    "        },\n",
    "        \"gpu_info\": {\n",
    "            \"name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\",\n",
    "            \"pre_loading_memory_gb\": pre_memory[\"allocated_memory_gb\"],\n",
    "            \"post_loading_memory_gb\": post_memory[\"allocated_memory_gb\"],\n",
    "            \"total_memory_gb\": post_memory[\"total_memory_gb\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save model metadata\n",
    "    model_metadata_path = get_results_path(\"model_metadata.json\")\n",
    "    with open(model_metadata_path, \"w\") as f:\n",
    "        json.dump(model_metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Model metadata saved to: {model_metadata_path}\")\n",
    "    \n",
    "    # Verify processor functionality\n",
    "    print(\"\\nüîç Verifying processor functionality...\")\n",
    "    \n",
    "    # Simple test to verify processor\n",
    "    sample_text = \"Extract the work order number from this invoice image.\"\n",
    "    try:\n",
    "        # Just verify it can process a simple example without running inference\n",
    "        sample_input = processor(text=sample_text, return_tensors=\"pt\")\n",
    "        print(\"‚úÖ Processor verified: Can process text inputs\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Processor test failed for text processing: {e}\")\n",
    "    \n",
    "    print(\"‚úÖ Model and processor loaded successfully and ready for inference!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    # Attempt to free memory in case of loading failure\n",
    "    if 'model' in locals():\n",
    "        del model\n",
    "    if 'processor' in locals():\n",
    "        del processor\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    # Raise the exception to halt execution\n",
    "    raise RuntimeError(f\"Failed to load model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Pipeline\n",
    "### Pipeline Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Initialization for Prompt Comparison\n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "# Try to import project-specific pipeline utilities\n",
    "try:\n",
    "    from src.execution.pipeline import ExtractionPipeline\n",
    "    print(\"‚úÖ Imported extraction pipeline\")\n",
    "    use_project_pipeline = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Project pipeline not available, using direct implementation\")\n",
    "    use_project_pipeline = False\n",
    "    \n",
    "    # Define a simple pipeline class if the project one isn't available\n",
    "    class ExtractionPipeline:\n",
    "        \"\"\"Basic extraction pipeline implementation with support for prompt comparison\"\"\"\n",
    "        def __init__(self, experiment_name):\n",
    "            self.experiment_name = experiment_name\n",
    "            self.config = {}\n",
    "            self.model = None\n",
    "            self.processor = None\n",
    "            self.ground_truth_mapping = {}\n",
    "            self.results = {}  # Results organized by prompt\n",
    "            self.batch_size = 1\n",
    "            self.checkpoints = {}\n",
    "            self.current_batch = 0\n",
    "            self.current_prompt_index = 0\n",
    "            self.prompts = []\n",
    "            \n",
    "        def configure(self, config):\n",
    "            \"\"\"Configure the pipeline with settings\"\"\"\n",
    "            self.config.update(config)\n",
    "            return self\n",
    "            \n",
    "        def setup_model(self, model=None, processor=None):\n",
    "            \"\"\"Set up model and processor\"\"\"\n",
    "            if model is not None:\n",
    "                self.model = model\n",
    "            if processor is not None:\n",
    "                self.processor = processor\n",
    "            return self\n",
    "            \n",
    "        def load_ground_truth(self, ground_truth_mapping=None, field_column=None):\n",
    "            \"\"\"Load ground truth data\"\"\"\n",
    "            if ground_truth_mapping is not None:\n",
    "                self.ground_truth_mapping = ground_truth_mapping\n",
    "            return self\n",
    "            \n",
    "        def set_prompts(self, prompts):\n",
    "            \"\"\"Set the prompts to be used for comparison\"\"\"\n",
    "            self.prompts = prompts\n",
    "            # Initialize results dictionary with keys for each prompt\n",
    "            for prompt in prompts:\n",
    "                prompt_key = prompt[\"name\"]\n",
    "                if prompt_key not in self.results:\n",
    "                    self.results[prompt_key] = []\n",
    "            return self\n",
    "            \n",
    "        def determine_optimal_batch_size(self, start_size=1, max_size=8):\n",
    "            \"\"\"Determine the optimal batch size for the GPU\"\"\"\n",
    "            # In a real implementation, this would test different batch sizes\n",
    "            # For this fallback, we'll just use the config value or default to 1\n",
    "            self.batch_size = self.config.get(\"batch_processing\", {}).get(\"batch_size\", 1)\n",
    "            return self.batch_size\n",
    "        \n",
    "        def get_current_prompt(self):\n",
    "            \"\"\"Get the current prompt being processed\"\"\"\n",
    "            if 0 <= self.current_prompt_index < len(self.prompts):\n",
    "                return self.prompts[self.current_prompt_index]\n",
    "            return None\n",
    "        \n",
    "        def next_prompt(self):\n",
    "            \"\"\"Move to the next prompt for processing\"\"\"\n",
    "            if self.current_prompt_index < len(self.prompts) - 1:\n",
    "                self.current_prompt_index += 1\n",
    "                return self.get_current_prompt()\n",
    "            return None\n",
    "        \n",
    "        def reset_prompt_index(self):\n",
    "            \"\"\"Reset to the first prompt\"\"\"\n",
    "            self.current_prompt_index = 0\n",
    "            return self.get_current_prompt()\n",
    "\n",
    "# Initialize pipeline\n",
    "print(\"üöÄ Initializing prompt comparison pipeline...\")\n",
    "experiment_name = experiment_config[\"experiment\"][\"name\"]\n",
    "\n",
    "# Create pipeline instance\n",
    "pipeline = ExtractionPipeline(experiment_name=experiment_name)\n",
    "\n",
    "# Set up batch processing configuration\n",
    "print(\"\\n‚öôÔ∏è Setting up pipeline configuration...\")\n",
    "batch_config = {\n",
    "    \"auto_batch_size\": True,  # Determine optimal batch size automatically\n",
    "    \"max_batch_size\": 8,      # Maximum batch size to consider\n",
    "    \"default_batch_size\": 1,  # Default if auto-estimation fails\n",
    "    \"optimize_memory\": True   # Clean up memory between batches\n",
    "}\n",
    "\n",
    "# Set up checkpointing configuration\n",
    "checkpoint_config = {\n",
    "    \"enable_checkpoints\": True,\n",
    "    \"checkpoint_frequency\": 5,  # Save checkpoint after every 5 batches\n",
    "    \"checkpoint_path\": str(get_results_path(\"extraction_checkpoint.json\")),\n",
    "    \"resume_from_checkpoint\": True  # Resume from checkpoint if available\n",
    "}\n",
    "\n",
    "# Set up metrics configuration\n",
    "metrics_config = {\n",
    "    \"metrics\": [\"exact_match\", \"character_error_rate\", \"levenshtein_distance\", \"processing_time\"],\n",
    "    \"case_sensitive\": False,\n",
    "    \"strip_whitespace\": True\n",
    "}\n",
    "\n",
    "# Set up prompt comparison configuration\n",
    "comparison_config = {\n",
    "    \"prompt_count\": len(prompt_collection),\n",
    "    \"prompt_names\": [p[\"name\"] for p in prompt_collection],\n",
    "    \"prompt_categories\": [p[\"category\"] for p in prompt_collection],\n",
    "    \"image_count\": comparison_config[\"image_count\"],  # From experiment config\n",
    "    \"metrics_to_compare\": comparison_config[\"metrics_to_compare\"]  # From experiment config\n",
    "}\n",
    "\n",
    "# Create complete pipeline configuration dict\n",
    "pipeline_config = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"model_name\": model_config[\"name\"],\n",
    "    \"field_to_extract\": extraction_config[\"field_to_extract\"],\n",
    "    \"description\": experiment_config[\"experiment\"][\"description\"],\n",
    "    \"batch_processing\": batch_config,\n",
    "    \"checkpointing\": checkpoint_config,\n",
    "    \"metrics\": metrics_config,\n",
    "    \"comparison\": comparison_config,\n",
    "    \"environment\": env_config.environment,\n",
    "    \"timestamp\": timestamp\n",
    "}\n",
    "\n",
    "# Update pipeline's config dictionary directly\n",
    "for key, value in pipeline_config.items():\n",
    "    pipeline.config[key] = value\n",
    "\n",
    "print(f\"‚úÖ Updated pipeline configuration with {len(pipeline_config)} parameters\")\n",
    "\n",
    "# Set up the model and processor in the pipeline\n",
    "print(\"\\nüîÑ Setting up model and processor in pipeline...\")\n",
    "pipeline.setup_model(model=model, processor=processor)\n",
    "\n",
    "# Load ground truth data\n",
    "print(\"\\nüìä Loading ground truth data into pipeline...\")\n",
    "pipeline.load_ground_truth(ground_truth_mapping=ground_truth_mapping, field_column=extraction_config[\"field_to_extract\"])\n",
    "\n",
    "# Set the prompts for comparison\n",
    "print(f\"\\nüìù Loading {len(prompt_collection)} prompts for comparison...\")\n",
    "pipeline.set_prompts(prompt_collection)\n",
    "\n",
    "# Determine optimal batch size if auto-batch is enabled\n",
    "if batch_config[\"auto_batch_size\"]:\n",
    "    print(\"\\nüßÆ Determining optimal batch size...\")\n",
    "    try:\n",
    "        # Prepare a sample task to estimate batch size\n",
    "        if hasattr(pipeline, \"prepare_extraction_task\"):\n",
    "            sample_items = pipeline.prepare_extraction_task(limit=1)\n",
    "        \n",
    "        if hasattr(pipeline, \"determine_optimal_batch_size\"):\n",
    "            optimal_batch_size = pipeline.determine_optimal_batch_size(\n",
    "                start_size=1,\n",
    "                max_size=batch_config[\"max_batch_size\"]\n",
    "            )\n",
    "            print(f\"‚úÖ Determined optimal batch size: {optimal_batch_size}\")\n",
    "        else:\n",
    "            # Fallback to default\n",
    "            optimal_batch_size = batch_config[\"default_batch_size\"]\n",
    "            print(f\"‚ÑπÔ∏è Using default batch size: {optimal_batch_size}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error determining batch size: {e}\")\n",
    "        print(f\"‚ÑπÔ∏è Falling back to default batch size: {batch_config['default_batch_size']}\")\n",
    "        optimal_batch_size = batch_config[\"default_batch_size\"]\n",
    "    \n",
    "    # Update config with determined batch size\n",
    "    pipeline.config[\"batch_processing\"][\"batch_size\"] = optimal_batch_size\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è Using fixed batch size: {batch_config.get('batch_size', batch_config['default_batch_size'])}\")\n",
    "\n",
    "# Check for existing checkpoint\n",
    "checkpoint_path = checkpoint_config[\"checkpoint_path\"]\n",
    "if os.path.exists(checkpoint_path) and checkpoint_config[\"resume_from_checkpoint\"]:\n",
    "    print(f\"\\nüîÑ Found existing checkpoint at: {checkpoint_path}\")\n",
    "    try:\n",
    "        with open(checkpoint_path, 'r') as f:\n",
    "            checkpoint = json.load(f)\n",
    "        if isinstance(checkpoint, dict) and \"prompt_index\" in checkpoint:\n",
    "            print(f\"‚ÑπÔ∏è Checkpoint from: {datetime.fromtimestamp(checkpoint.get('timestamp', 0)).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            \n",
    "            # Get completed prompts\n",
    "            completed_prompts = checkpoint.get(\"completed_prompts\", [])\n",
    "            if completed_prompts:\n",
    "                print(f\"‚ÑπÔ∏è Completed prompts: {', '.join(completed_prompts)}\")\n",
    "            \n",
    "            # Get current prompt progress\n",
    "            current_prompt = checkpoint.get(\"current_prompt\")\n",
    "            current_prompt_progress = len(checkpoint.get(\"results\", {}).get(current_prompt, []))\n",
    "            print(f\"‚ÑπÔ∏è Current prompt: {current_prompt} ({current_prompt_progress} images processed)\")\n",
    "            \n",
    "            # Set pipeline state from checkpoint\n",
    "            pipeline.current_prompt_index = checkpoint.get(\"prompt_index\", 0)\n",
    "            pipeline.current_batch = checkpoint.get(\"batch_idx\", 0)\n",
    "            pipeline.results = checkpoint.get(\"results\", {})\n",
    "            \n",
    "            print(f\"‚ÑπÔ∏è Will resume from prompt {pipeline.current_prompt_index + 1}/{len(prompt_collection)}, batch {pipeline.current_batch}\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è Checkpoint format is different, will start from beginning\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading checkpoint: {e}\")\n",
    "        print(\"‚ÑπÔ∏è Will start processing from the beginning\")\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è No existing checkpoint found or resume disabled\")\n",
    "    print(\"‚ÑπÔ∏è Will start processing from the beginning\")\n",
    "\n",
    "# Prepare for extraction\n",
    "print(\"\\nüìã Pipeline initialization summary:\")\n",
    "print(f\"   Experiment: {pipeline.config['experiment_name']}\")\n",
    "print(f\"   Model: {pipeline.config['model_name']}\")\n",
    "print(f\"   Field to extract: {pipeline.config['field_to_extract']}\")\n",
    "print(f\"   Prompts to compare: {len(pipeline.prompts)}\")\n",
    "for i, prompt in enumerate(pipeline.prompts, 1):\n",
    "    print(f\"      {i}. {prompt['name']} ({prompt['category']})\")\n",
    "print(f\"   Batch size: {pipeline.config['batch_processing'].get('batch_size', batch_config['default_batch_size'])}\")\n",
    "print(f\"   Checkpointing: {'Enabled' if checkpoint_config['enable_checkpoints'] else 'Disabled'}\")\n",
    "print(f\"   Images to process: {len(pipeline.ground_truth_mapping) if hasattr(pipeline, 'ground_truth_mapping') and pipeline.ground_truth_mapping else len(batch_items)}\")\n",
    "print(f\"   Total comparisons: {len(pipeline.prompts) * comparison_config['image_count']}\")\n",
    "\n",
    "# Save pipeline configuration\n",
    "pipeline_config_path = get_results_path(\"pipeline_config.json\")\n",
    "with open(pipeline_config_path, \"w\") as f:\n",
    "    # Convert any non-serializable objects to strings\n",
    "    serializable_config = json.loads(json.dumps(pipeline.config, default=str))\n",
    "    json.dump(serializable_config, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Pipeline configuration saved to: {pipeline_config_path}\")\n",
    "print(\"‚úÖ Prompt comparison pipeline initialized and ready for extraction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Image Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Image Test for All Prompts\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import time\n",
    "from PIL import Image\n",
    "from Levenshtein import distance\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Try to import project-specific inference utilities\n",
    "try:\n",
    "    from src.execution.inference import process_image_with_metrics\n",
    "    print(\"‚úÖ Imported inference utilities\")\n",
    "    use_project_utils = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Project inference utilities not available, using direct implementation\")\n",
    "    use_project_utils = False\n",
    "    \n",
    "    # Define a direct implementation for single image processing\n",
    "    def process_image_with_metrics(image_path, ground_truth, prompt, model_name, field_type, model, processor, metrics=None):\n",
    "        \"\"\"Process a single image and calculate metrics against ground truth\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Load the image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "            # Format the prompt with instruction tags\n",
    "            formatted_prompt = prompt[\"formatted_text\"]\n",
    "            \n",
    "            # Process using processor\n",
    "            inputs = processor(\n",
    "                text=formatted_prompt,\n",
    "                images=[image],  # Notice we're passing a list here\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Convert inputs to appropriate dtypes\n",
    "            for key in inputs:\n",
    "                if key == \"pixel_values\":\n",
    "                    inputs[key] = inputs[key].to(dtype=next(model.parameters()).dtype, device=model.device)\n",
    "                else:\n",
    "                    inputs[key] = inputs[key].to(device=model.device)\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_new_tokens=50, do_sample=False)\n",
    "            \n",
    "            # Decode the output\n",
    "            raw_extraction = processor.batch_decode(\n",
    "                outputs, \n",
    "                skip_special_tokens=True, \n",
    "                clean_up_tokenization_spaces=False\n",
    "            )[0]\n",
    "            \n",
    "            # Process the output - clean up whitespace and extract just numbers if needed\n",
    "            processed_extraction = raw_extraction.strip()\n",
    "            \n",
    "            # Calculate processing time\n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            # Calculate metrics\n",
    "            result = {\n",
    "                \"raw_extraction\": raw_extraction,\n",
    "                \"processed_extraction\": processed_extraction,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"processing_time\": processing_time,\n",
    "                \"prompt_name\": prompt[\"name\"],\n",
    "                \"prompt_category\": prompt[\"category\"],\n",
    "                \"prompt_text\": prompt[\"text\"]\n",
    "            }\n",
    "            \n",
    "            # Calculate exact match\n",
    "            exact_match = processed_extraction.lower() == ground_truth.lower()\n",
    "            result[\"exact_match\"] = exact_match\n",
    "            \n",
    "            # Calculate character error rate (Levenshtein distance normalized by length)\n",
    "            if len(ground_truth) > 0:\n",
    "                cer = distance(processed_extraction, ground_truth) / len(ground_truth)\n",
    "            else:\n",
    "                cer = 1.0 if len(processed_extraction) > 0 else 0.0\n",
    "            result[\"character_error_rate\"] = cer\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            processing_time = time.time() - start_time\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"processing_time\": processing_time,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"raw_extraction\": \"ERROR\",\n",
    "                \"processed_extraction\": \"ERROR\",\n",
    "                \"exact_match\": False,\n",
    "                \"character_error_rate\": 1.0,\n",
    "                \"prompt_name\": prompt[\"name\"],\n",
    "                \"prompt_category\": prompt[\"category\"],\n",
    "                \"prompt_text\": prompt[\"text\"]\n",
    "            }\n",
    "\n",
    "# Verify we have the pipeline, model, and ground truth data\n",
    "if 'pipeline' not in locals():\n",
    "    print(\"‚ùå Pipeline not initialized. Run the pipeline initialization cell first.\")\n",
    "    raise RuntimeError(\"Pipeline not initialized\")\n",
    "    \n",
    "if 'model' not in locals() or model is None:\n",
    "    print(\"‚ùå Model not loaded. Run the model loading cell first.\")\n",
    "    raise RuntimeError(\"Model not loaded\")\n",
    "    \n",
    "if len(ground_truth_mapping) == 0:\n",
    "    print(\"‚ùå No ground truth data available. Run the ground truth data loading cell first.\")\n",
    "    raise RuntimeError(\"No ground truth data available\")\n",
    "\n",
    "if 'prompt_collection' not in locals() or not prompt_collection:\n",
    "    print(\"‚ùå No prompts available. Run the prompt selection cell first.\")\n",
    "    raise RuntimeError(\"No prompts available\")\n",
    "\n",
    "print(f\"üß™ Running a test on a single image with {len(prompt_collection)} different prompts...\")\n",
    "\n",
    "# Select a sample image\n",
    "print(\"\\nüîç Selecting a test image...\")\n",
    "try:\n",
    "    # Get all image paths\n",
    "    image_paths = list(Path(paths[\"images_dir\"]).glob(\"*.jpg\")) + list(Path(paths[\"images_dir\"]).glob(\"*.png\"))\n",
    "    \n",
    "    if not image_paths:\n",
    "        raise FileNotFoundError(\"No image files found\")\n",
    "    \n",
    "    # Choose a random image that has ground truth data\n",
    "    valid_images = [img for img in image_paths if img.stem in ground_truth_mapping]\n",
    "    \n",
    "    if not valid_images:\n",
    "        print(\"‚ö†Ô∏è No images with ground truth found, using a random image\")\n",
    "        test_image = random.choice(image_paths)\n",
    "    else:\n",
    "        test_image = random.choice(valid_images)\n",
    "    \n",
    "    # Get ground truth for the test image\n",
    "    test_image_id = test_image.stem\n",
    "    ground_truth = ground_truth_mapping.get(test_image_id, {}).get('work_order', 'Unknown')\n",
    "    \n",
    "    print(f\"‚úÖ Selected test image: {test_image.name}\")\n",
    "    print(f\"üìã Ground truth value: '{ground_truth}'\")\n",
    "    \n",
    "    # Display the test image\n",
    "    print(\"\\nüì∏ Test Image:\")\n",
    "    img = Image.open(test_image)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Test Invoice: {test_image_id}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Initialize collection for test results\n",
    "    test_results = []\n",
    "    overall_start_time = time.time()\n",
    "    \n",
    "    # Test each prompt against the image\n",
    "    print(f\"\\n‚è≥ Processing test image with {len(prompt_collection)} prompts...\")\n",
    "    \n",
    "    for i, prompt in enumerate(prompt_collection, 1):\n",
    "        print(f\"\\nüîÑ Testing Prompt {i}/{len(prompt_collection)}: {prompt['name']} ({prompt['category']})\")\n",
    "        print(f\"   Prompt text: \\\"{prompt['text']}\\\"\")\n",
    "        \n",
    "        prompt_start_time = time.time()\n",
    "        \n",
    "        # Run the test using the pipeline or direct implementation\n",
    "        if hasattr(pipeline, \"process_single_image\") and callable(getattr(pipeline, \"process_single_image\")):\n",
    "            # Use pipeline's method if available\n",
    "            result = pipeline.process_single_image(\n",
    "                image_path=str(test_image),\n",
    "                ground_truth=ground_truth,\n",
    "                prompt=prompt,\n",
    "                metrics=[\"exact_match\", \"character_error_rate\"]\n",
    "            )\n",
    "        else:\n",
    "            # Use direct implementation\n",
    "            result = process_image_with_metrics(\n",
    "                image_path=str(test_image),\n",
    "                ground_truth=ground_truth,\n",
    "                prompt=prompt,\n",
    "                model_name=model_config[\"name\"],\n",
    "                field_type=extraction_config[\"field_to_extract\"],\n",
    "                model=model,\n",
    "                processor=processor,\n",
    "                metrics=[\"exact_match\", \"character_error_rate\"]\n",
    "            )\n",
    "        \n",
    "        # Calculate total time including setup\n",
    "        prompt_total_time = time.time() - prompt_start_time\n",
    "        result[\"total_test_time\"] = prompt_total_time\n",
    "        \n",
    "        # Display prompt results\n",
    "        print(f\"   Raw Extraction: '{result.get('raw_extraction', 'ERROR')}'\")\n",
    "        print(f\"   Processed Extraction: '{result.get('processed_extraction', 'ERROR')}'\")\n",
    "        print(f\"   Exact Match: {result.get('exact_match', False)}\")\n",
    "        print(f\"   Character Error Rate: {result.get('character_error_rate', 1.0):.4f}\")\n",
    "        print(f\"   Processing Time: {result.get('processing_time', 0):.2f} seconds\")\n",
    "        print(f\"   Total Time: {prompt_total_time:.2f} seconds\")\n",
    "        \n",
    "        # Add to results collection\n",
    "        result[\"prompt_name\"] = prompt[\"name\"]\n",
    "        result[\"prompt_category\"] = prompt[\"category\"]\n",
    "        result[\"prompt_text\"] = prompt[\"text\"]\n",
    "        test_results.append(result)\n",
    "    \n",
    "    # Calculate overall time\n",
    "    overall_time = time.time() - overall_start_time\n",
    "    \n",
    "    # Create a summary table of results\n",
    "    results_df = pd.DataFrame([\n",
    "        {\n",
    "            \"Prompt Name\": r[\"prompt_name\"],\n",
    "            \"Category\": r[\"prompt_category\"],\n",
    "            \"Extracted Text\": r.get(\"processed_extraction\", \"ERROR\"),\n",
    "            \"Exact Match\": r.get(\"exact_match\", False),\n",
    "            \"CER\": r.get(\"character_error_rate\", 1.0),\n",
    "            \"Time (s)\": r.get(\"processing_time\", 0)\n",
    "        }\n",
    "        for r in test_results\n",
    "    ])\n",
    "    \n",
    "    # Display summary table\n",
    "    print(\"\\nüìä Prompt Comparison Results:\")\n",
    "    display(results_df)\n",
    "    \n",
    "    # Count successful extractions\n",
    "    successful = sum(1 for r in test_results if r.get(\"exact_match\", False))\n",
    "    print(f\"\\n‚úÖ Successful extractions: {successful}/{len(prompt_collection)} ({successful/len(prompt_collection)*100:.1f}%)\")\n",
    "    \n",
    "    # Show the most successful prompts\n",
    "    if successful > 0:\n",
    "        best_prompts = [r[\"prompt_name\"] for r in test_results if r.get(\"exact_match\", False)]\n",
    "        print(f\"   Best performing prompts: {', '.join(best_prompts)}\")\n",
    "    \n",
    "    # If none are exact matches, show the ones with lowest CER\n",
    "    if successful == 0:\n",
    "        # Sort by character error rate (ascending)\n",
    "        sorted_results = sorted(test_results, key=lambda r: r.get(\"character_error_rate\", 1.0))\n",
    "        best_prompt = sorted_results[0][\"prompt_name\"]\n",
    "        best_cer = sorted_results[0].get(\"character_error_rate\", 1.0)\n",
    "        print(f\"   No exact matches. Best prompt: {best_prompt} (CER: {best_cer:.4f})\")\n",
    "    \n",
    "    # Save test results\n",
    "    test_results_path = get_results_path(\"single_image_prompt_comparison.json\")\n",
    "    with open(test_results_path, \"w\") as f:\n",
    "        # Add image and test metadata\n",
    "        test_summary = {\n",
    "            \"image_id\": test_image_id,\n",
    "            \"image_path\": str(test_image),\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"test_timestamp\": datetime.now().isoformat(),\n",
    "            \"total_test_time\": overall_time,\n",
    "            \"prompt_count\": len(prompt_collection),\n",
    "            \"successful_extractions\": successful,\n",
    "            \"results\": test_results\n",
    "        }\n",
    "        \n",
    "        # Convert to serializable format\n",
    "        import json\n",
    "        json.dump(test_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Test results saved to: {test_results_path}\")\n",
    "    print(f\"‚è±Ô∏è Total testing time: {overall_time:.2f} seconds\")\n",
    "    \n",
    "    # Store the results for reference\n",
    "    test_summary_results = test_summary\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during single image test: {e}\")\n",
    "    test_summary_results = {\"error\": str(e)}\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Extraction Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Extraction for All Prompts\n",
    "\n",
    "# Import required libraries\n",
    "import time\n",
    "import torch\n",
    "import gc\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Verify pipeline, model, and other requirements are available\n",
    "if 'pipeline' not in locals():\n",
    "    print(\"‚ùå Pipeline not initialized. Run the pipeline initialization cell first.\")\n",
    "    raise RuntimeError(\"Pipeline not initialized\")\n",
    "    \n",
    "if 'model' not in locals() or model is None:\n",
    "    print(\"‚ùå Model not loaded. Run the model loading cell first.\")\n",
    "    raise RuntimeError(\"Model not loaded\")\n",
    "    \n",
    "if 'batch_items' not in locals() or len(batch_items) == 0:\n",
    "    print(\"‚ùå No batch items available. Run the ground truth data loading cell first.\")\n",
    "    raise RuntimeError(\"No batch items available\")\n",
    "\n",
    "if 'prompt_collection' not in locals() or not prompt_collection:\n",
    "    print(\"‚ùå No prompts available. Run the prompt selection cell first.\")\n",
    "    raise RuntimeError(\"No prompts available\")\n",
    "\n",
    "# Verify test was successful before proceeding\n",
    "if 'test_summary_results' not in locals() or 'error' in test_summary_results:\n",
    "    print(\"‚ö†Ô∏è Warning: Single image test did not complete successfully.\")\n",
    "    proceed = input(\"Do you want to proceed with full extraction anyway? (y/n): \").strip().lower()\n",
    "    if proceed != 'y':\n",
    "        raise RuntimeError(\"Full extraction aborted by user. Run the single image test first.\")\n",
    "\n",
    "# Define utility functions for the extraction run\n",
    "def format_eta(seconds):\n",
    "    \"\"\"Format seconds into a human-readable ETA string\"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.0f} seconds\"\n",
    "    elif seconds < 3600:\n",
    "        minutes = seconds / 60\n",
    "        return f\"{minutes:.1f} minutes\"\n",
    "    else:\n",
    "        hours = seconds / 3600\n",
    "        return f\"{hours:.2f} hours\"\n",
    "\n",
    "def estimate_completion_time(current, total, elapsed_time):\n",
    "    \"\"\"Estimate completion time based on progress\"\"\"\n",
    "    if current == 0 or elapsed_time <= 0:\n",
    "        return \"Calculating...\"\n",
    "    \n",
    "    items_per_second = current / elapsed_time\n",
    "    remaining_items = total - current\n",
    "    \n",
    "    if items_per_second > 0:\n",
    "        remaining_seconds = remaining_items / items_per_second\n",
    "        eta = format_eta(remaining_seconds)\n",
    "        completion_time = datetime.now() + timedelta(seconds=remaining_seconds)\n",
    "        formatted_time = completion_time.strftime(\"%H:%M:%S\")\n",
    "        return f\"{eta} (completion around {formatted_time})\"\n",
    "    else:\n",
    "        return \"Unable to estimate\"\n",
    "\n",
    "def log_memory_usage(step, info=None):\n",
    "    \"\"\"Log GPU memory usage for monitoring\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    \n",
    "    memory_info = {\n",
    "        \"step\": step,\n",
    "        \"timestamp\": time.time(),\n",
    "        \"total_memory_gb\": torch.cuda.get_device_properties(0).total_memory / 1e9,\n",
    "        \"allocated_memory_gb\": torch.cuda.memory_allocated(0) / 1e9,\n",
    "        \"reserved_memory_gb\": torch.cuda.memory_reserved(0) / 1e9,\n",
    "        \"info\": info\n",
    "    }\n",
    "    \n",
    "    # Calculate free memory (total - allocated)\n",
    "    memory_info[\"free_memory_gb\"] = memory_info[\"total_memory_gb\"] - memory_info[\"allocated_memory_gb\"]\n",
    "    \n",
    "    return memory_info\n",
    "\n",
    "# Set up checkpoint path for resuming\n",
    "checkpoint_path = get_results_path(\"prompt_comparison_checkpoint.json\")\n",
    "checkpoint_frequency = pipeline_config[\"checkpointing\"][\"checkpoint_frequency\"]\n",
    "enable_checkpoints = pipeline_config[\"checkpointing\"][\"enable_checkpoints\"]\n",
    "\n",
    "# Limit the number of images to process (if specified in comparison config)\n",
    "image_limit = comparison_config.get(\"image_count\", len(batch_items))\n",
    "if image_limit < len(batch_items):\n",
    "    print(f\"‚öôÔ∏è Limiting extraction to first {image_limit} images (out of {len(batch_items)})\")\n",
    "    batch_items = batch_items[:image_limit]\n",
    "else:\n",
    "    print(f\"‚öôÔ∏è Processing all {len(batch_items)} images with each prompt\")\n",
    "\n",
    "# Initialize result storage dictionary - organized by prompt\n",
    "results_by_prompt = {}\n",
    "for prompt in prompt_collection:\n",
    "    prompt_name = prompt[\"name\"]\n",
    "    results_by_prompt[prompt_name] = []\n",
    "\n",
    "# Initialize tracking\n",
    "current_prompt_idx = 0\n",
    "prompt_states = {p[\"name\"]: \"pending\" for p in prompt_collection}  # pending, in_progress, completed\n",
    "completed_prompts = []\n",
    "\n",
    "# Check for existing checkpoint if resuming is enabled\n",
    "if os.path.exists(checkpoint_path) and enable_checkpoints:\n",
    "    try:\n",
    "        with open(checkpoint_path, \"r\") as f:\n",
    "            checkpoint = json.load(f)\n",
    "        \n",
    "        if \"results_by_prompt\" in checkpoint:\n",
    "            # Load results from checkpoint\n",
    "            results_by_prompt = checkpoint[\"results_by_prompt\"]\n",
    "            completed_prompts = checkpoint.get(\"completed_prompts\", [])\n",
    "            current_prompt_idx = checkpoint.get(\"current_prompt_index\", 0)\n",
    "            prompt_states = checkpoint.get(\"prompt_states\", prompt_states)\n",
    "            \n",
    "            # Count results\n",
    "            total_results = sum(len(results) for results in results_by_prompt.values())\n",
    "            print(f\"üìÇ Resuming from checkpoint: {total_results} total results already processed\")\n",
    "            print(f\"   Completed prompts: {', '.join(completed_prompts)}\")\n",
    "            if current_prompt_idx < len(prompt_collection):\n",
    "                current_prompt = prompt_collection[current_prompt_idx][\"name\"]\n",
    "                print(f\"   Current prompt: {current_prompt} ({len(results_by_prompt.get(current_prompt, []))} images processed)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading checkpoint: {e}\")\n",
    "        print(\"   Starting from the beginning\")\n",
    "\n",
    "# Determine batch size to use\n",
    "batch_size = pipeline_config[\"batch_processing\"].get(\"batch_size\", 1)\n",
    "print(f\"üî¢ Using batch size: {batch_size}\")\n",
    "\n",
    "# Calculate total operations\n",
    "total_prompts = len(prompt_collection)\n",
    "total_images = len(batch_items)\n",
    "total_operations = total_prompts * total_images\n",
    "completed_operations = sum(len(results) for results in results_by_prompt.values())\n",
    "remaining_operations = total_operations - completed_operations\n",
    "\n",
    "print(f\"\\nüìä Experiment Scale:\")\n",
    "print(f\"   Prompts to compare: {total_prompts}\")\n",
    "print(f\"   Images per prompt: {total_images}\")\n",
    "print(f\"   Total operations: {total_operations}\")\n",
    "print(f\"   Already completed: {completed_operations}\")\n",
    "print(f\"   Remaining: {remaining_operations}\")\n",
    "\n",
    "# Display confirmation and execution plan\n",
    "print(\"\\nüöÄ Starting prompt comparison pipeline...\")\n",
    "print(f\"üíæ Checkpointing: {'Enabled' if enable_checkpoints else 'Disabled'}\")\n",
    "if enable_checkpoints:\n",
    "    print(f\"   Saving checkpoint every {checkpoint_frequency} batches to: {checkpoint_path}\")\n",
    "\n",
    "# Ask for confirmation before proceeding\n",
    "proceed = input(\"\\nüí¨ Proceed with full prompt comparison? This may take a while. (y/n): \").strip().lower()\n",
    "if proceed != 'y':\n",
    "    print(\"‚ùå Extraction canceled by user.\")\n",
    "    raise RuntimeError(\"Extraction canceled by user.\")\n",
    "\n",
    "# Record start time for the full extraction\n",
    "full_start_time = time.time()\n",
    "memory_log = []\n",
    "memory_log.append(log_memory_usage(\"start\", \"Before extraction starts\"))\n",
    "\n",
    "# Create master results tracking\n",
    "all_results = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": model_config[\"name\"],\n",
    "    \"total_prompts\": total_prompts,\n",
    "    \"total_images\": total_images,\n",
    "    \"results_by_prompt\": results_by_prompt,\n",
    "    \"prompt_summaries\": {},\n",
    "    \"comparison_metrics\": {}\n",
    "}\n",
    "\n",
    "# Main execution loop - process each prompt\n",
    "try:\n",
    "    # Start from the current prompt (for resuming)\n",
    "    for prompt_idx in range(current_prompt_idx, len(prompt_collection)):\n",
    "        prompt = prompt_collection[prompt_idx]\n",
    "        prompt_name = prompt[\"name\"]\n",
    "        prompt_category = prompt[\"category\"]\n",
    "        \n",
    "        # Skip if this prompt is already completed\n",
    "        if prompt_name in completed_prompts:\n",
    "            print(f\"\\n‚úÖ Prompt {prompt_idx+1}/{len(prompt_collection)}: '{prompt_name}' already completed, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Set up for this prompt\n",
    "        print(f\"\\nüîÑ Processing Prompt {prompt_idx+1}/{len(prompt_collection)}: '{prompt_name}' ({prompt_category})\")\n",
    "        print(f\"   Prompt text: \\\"{prompt['text']}\\\"\")\n",
    "        print(f\"   Formatted: \\\"{prompt['formatted_text']}\\\"\")\n",
    "        \n",
    "        # Update prompt state\n",
    "        prompt_states[prompt_name] = \"in_progress\"\n",
    "        \n",
    "        # Get already processed results for this prompt\n",
    "        processed_items = results_by_prompt.get(prompt_name, [])\n",
    "        processed_ids = [item[\"image_id\"] for item in processed_items]\n",
    "        \n",
    "        # Filter out already processed images for this prompt\n",
    "        current_batch_items = [item for item in batch_items if item[\"image_id\"] not in processed_ids]\n",
    "        \n",
    "        # If all images for this prompt are already processed, skip to next prompt\n",
    "        if not current_batch_items:\n",
    "            print(f\"‚úÖ All images already processed for prompt '{prompt_name}', marking as completed\")\n",
    "            completed_prompts.append(prompt_name)\n",
    "            prompt_states[prompt_name] = \"completed\"\n",
    "            current_prompt_idx = prompt_idx + 1\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if enable_checkpoints:\n",
    "                checkpoint = {\n",
    "                    \"timestamp\": time.time(),\n",
    "                    \"current_prompt_index\": current_prompt_idx,\n",
    "                    \"completed_prompts\": completed_prompts,\n",
    "                    \"prompt_states\": prompt_states,\n",
    "                    \"results_by_prompt\": results_by_prompt\n",
    "                }\n",
    "                with open(checkpoint_path, \"w\") as f:\n",
    "                    json.dump(checkpoint, f)\n",
    "                print(f\"üíæ Checkpoint saved after completing prompt '{prompt_name}'\")\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        # Process images for this prompt\n",
    "        prompt_start_time = time.time()\n",
    "        num_batches = math.ceil(len(current_batch_items) / batch_size)\n",
    "        \n",
    "        print(f\"\\n‚è≥ Processing {len(current_batch_items)} images with prompt '{prompt_name}'...\")\n",
    "        print(f\"   Divided into {num_batches} batches of size {batch_size}\")\n",
    "        \n",
    "        # Process in batches\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            # Get current batch\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(current_batch_items))\n",
    "            current_batch = current_batch_items[start_idx:end_idx]\n",
    "            \n",
    "            # Skip if batch is empty\n",
    "            if not current_batch:\n",
    "                print(f\"‚ö†Ô∏è Batch {batch_idx+1}/{num_batches} is empty, skipping\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nüîÑ Processing Batch {batch_idx+1}/{num_batches} ({len(current_batch)} images)\")\n",
    "            \n",
    "            # Log memory before batch\n",
    "            memory_log.append(log_memory_usage(f\"prompt_{prompt_idx}_batch_{batch_idx}_start\", \n",
    "                                               f\"Before prompt '{prompt_name}' batch {batch_idx+1}\"))\n",
    "            \n",
    "            # Process each item in the batch\n",
    "            batch_results = []\n",
    "            for i, item in enumerate(current_batch):\n",
    "                item_start_time = time.time()\n",
    "                \n",
    "                # Display progress\n",
    "                current_prompt_progress = len(processed_items) + len(batch_results) + 1\n",
    "                total_prompt_items = len(current_batch_items) + len(processed_items)\n",
    "                \n",
    "                # Calculate overall progress across all prompts\n",
    "                overall_completed = completed_operations + current_prompt_progress\n",
    "                overall_percentage = (overall_completed / total_operations) * 100\n",
    "                \n",
    "                # Calculate ETA\n",
    "                elapsed_time = time.time() - full_start_time\n",
    "                eta = estimate_completion_time(overall_completed, total_operations, elapsed_time)\n",
    "                \n",
    "                print(f\"\\n‚è≥ Prompt {prompt_idx+1}/{total_prompts}: '{prompt_name}' | \"\n",
    "                      f\"Image {current_prompt_progress}/{total_prompt_items} | \"\n",
    "                      f\"Overall: {overall_percentage:.1f}% complete | ETA: {eta}\")\n",
    "                \n",
    "                # Process the item\n",
    "                try:\n",
    "                    if hasattr(pipeline, \"process_single_image\") and callable(getattr(pipeline, \"process_single_image\")):\n",
    "                        # Use pipeline's method if available\n",
    "                        result = pipeline.process_single_image(\n",
    "                            image_path=item[\"image_path\"],\n",
    "                            ground_truth=item[\"ground_truth\"],\n",
    "                            prompt=prompt,\n",
    "                            metrics=[\"exact_match\", \"character_error_rate\"]\n",
    "                        )\n",
    "                    else:\n",
    "                        # Use direct implementation\n",
    "                        result = process_image_with_metrics(\n",
    "                            image_path=item[\"image_path\"],\n",
    "                            ground_truth=item[\"ground_truth\"],\n",
    "                            prompt=prompt,\n",
    "                            model_name=model_config[\"name\"],\n",
    "                            field_type=extraction_config[\"field_to_extract\"],\n",
    "                            model=model,\n",
    "                            processor=processor,\n",
    "                            metrics=[\"exact_match\", \"character_error_rate\"]\n",
    "                        )\n",
    "                    \n",
    "                    # Add metadata to result\n",
    "                    result[\"image_id\"] = item[\"image_id\"]\n",
    "                    result[\"prompt_name\"] = prompt_name\n",
    "                    result[\"prompt_category\"] = prompt_category\n",
    "                    result[\"prompt_text\"] = prompt[\"text\"]\n",
    "                    result[\"processing_timestamp\"] = datetime.now().isoformat()\n",
    "                    result[\"batch_idx\"] = batch_idx\n",
    "                    result[\"item_idx\"] = i\n",
    "                    \n",
    "                    # Display result\n",
    "                    exact_match = result.get(\"exact_match\", False)\n",
    "                    match_symbol = \"‚úì\" if exact_match else \"‚úó\"\n",
    "                    print(f\"   Result: {match_symbol} | Extracted: '{result.get('processed_extraction', 'ERROR')}' | \"\n",
    "                          f\"GT: '{result.get('ground_truth', 'Unknown')}' | \"\n",
    "                          f\"CER: {result.get('character_error_rate', 1.0):.4f} | \"\n",
    "                          f\"Time: {result.get('processing_time', 0):.2f}s\")\n",
    "                    \n",
    "                    # Add to batch results\n",
    "                    batch_results.append(result)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error processing item {item['image_id']} with prompt '{prompt_name}': {e}\")\n",
    "                    # Add error result\n",
    "                    error_result = {\n",
    "                        \"image_id\": item[\"image_id\"],\n",
    "                        \"prompt_name\": prompt_name,\n",
    "                        \"prompt_category\": prompt_category,\n",
    "                        \"prompt_text\": prompt[\"text\"],\n",
    "                        \"error\": str(e),\n",
    "                        \"processing_timestamp\": datetime.now().isoformat(),\n",
    "                        \"batch_idx\": batch_idx,\n",
    "                        \"item_idx\": i,\n",
    "                        \"ground_truth\": item[\"ground_truth\"],\n",
    "                        \"exact_match\": False,\n",
    "                        \"character_error_rate\": 1.0\n",
    "                    }\n",
    "                    batch_results.append(error_result)\n",
    "            \n",
    "            # Add batch results to results for this prompt\n",
    "            results_by_prompt[prompt_name].extend(batch_results)\n",
    "            \n",
    "            # Update completed operations count\n",
    "            completed_operations += len(batch_results)\n",
    "            \n",
    "            # Log memory after batch\n",
    "            memory_log.append(log_memory_usage(f\"prompt_{prompt_idx}_batch_{batch_idx}_end\", \n",
    "                                              f\"After prompt '{prompt_name}' batch {batch_idx+1}\"))\n",
    "            \n",
    "            # Calculate batch statistics\n",
    "            batch_time = time.time() - batch_start_time\n",
    "            avg_item_time = batch_time / len(current_batch) if current_batch else 0\n",
    "            exact_matches = sum(1 for r in batch_results if r.get(\"exact_match\", False))\n",
    "            accuracy = (exact_matches / len(current_batch) * 100) if current_batch else 0\n",
    "            \n",
    "            print(f\"\\nüìä Batch {batch_idx+1} Summary:\")\n",
    "            print(f\"   Processed {len(current_batch)} items in {batch_time:.2f}s ({avg_item_time:.2f}s per item)\")\n",
    "            print(f\"   Exact Match: {exact_matches}/{len(current_batch)} ({accuracy:.1f}%)\")\n",
    "            \n",
    "            # Clean up memory between batches if configured\n",
    "            if pipeline_config[\"batch_processing\"].get(\"optimize_memory\", True):\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    print(\"   üßπ Cleaned up memory\")\n",
    "            \n",
    "            # Save checkpoint if enabled\n",
    "            if enable_checkpoints and (batch_idx + 1) % checkpoint_frequency == 0:\n",
    "                checkpoint = {\n",
    "                    \"timestamp\": time.time(),\n",
    "                    \"current_prompt_index\": prompt_idx,\n",
    "                    \"completed_prompts\": completed_prompts,\n",
    "                    \"prompt_states\": prompt_states,\n",
    "                    \"results_by_prompt\": results_by_prompt\n",
    "                }\n",
    "                with open(checkpoint_path, \"w\") as f:\n",
    "                    json.dump(checkpoint, f)\n",
    "                \n",
    "                print(f\"   üíæ Saved checkpoint after batch {batch_idx+1}\")\n",
    "        \n",
    "        # Calculate prompt summary statistics\n",
    "        prompt_time = time.time() - prompt_start_time\n",
    "        prompt_results = results_by_prompt[prompt_name]\n",
    "        prompt_exact_matches = sum(1 for r in prompt_results if r.get(\"exact_match\", False))\n",
    "        prompt_accuracy = (prompt_exact_matches / len(prompt_results) * 100) if prompt_results else 0\n",
    "        avg_cer = sum(r.get(\"character_error_rate\", 1.0) for r in prompt_results) / len(prompt_results) if prompt_results else 0\n",
    "        avg_time = sum(r.get(\"processing_time\", 0) for r in prompt_results) / len(prompt_results) if prompt_results else 0\n",
    "        \n",
    "        # Store prompt summary\n",
    "        prompt_summary = {\n",
    "            \"prompt_name\": prompt_name,\n",
    "            \"prompt_category\": prompt_category,\n",
    "            \"total_images\": len(prompt_results),\n",
    "            \"exact_matches\": prompt_exact_matches,\n",
    "            \"accuracy\": prompt_accuracy,\n",
    "            \"avg_character_error_rate\": avg_cer,\n",
    "            \"avg_processing_time\": avg_time,\n",
    "            \"total_processing_time\": prompt_time\n",
    "        }\n",
    "        \n",
    "        # Add to master results\n",
    "        all_results[\"prompt_summaries\"][prompt_name] = prompt_summary\n",
    "        \n",
    "        # Display prompt summary\n",
    "        print(f\"\\nüìä Prompt '{prompt_name}' Summary:\")\n",
    "        print(f\"   Processed {len(prompt_results)} images in {prompt_time:.2f}s\")\n",
    "        print(f\"   Exact Match Accuracy: {prompt_exact_matches}/{len(prompt_results)} ({prompt_accuracy:.1f}%)\")\n",
    "        print(f\"   Average Character Error Rate: {avg_cer:.4f}\")\n",
    "        print(f\"   Average Processing Time: {avg_time:.2f}s per image\")\n",
    "        \n",
    "        # Mark prompt as completed\n",
    "        completed_prompts.append(prompt_name)\n",
    "        prompt_states[prompt_name] = \"completed\"\n",
    "        current_prompt_idx = prompt_idx + 1\n",
    "        \n",
    "        # Save checkpoint after completing a prompt\n",
    "        if enable_checkpoints:\n",
    "            checkpoint = {\n",
    "                \"timestamp\": time.time(),\n",
    "                \"current_prompt_index\": current_prompt_idx,\n",
    "                \"completed_prompts\": completed_prompts,\n",
    "                \"prompt_states\": prompt_states,\n",
    "                \"results_by_prompt\": results_by_prompt\n",
    "            }\n",
    "            with open(checkpoint_path, \"w\") as f:\n",
    "                json.dump(checkpoint, f)\n",
    "            print(f\"üíæ Checkpoint saved after completing prompt '{prompt_name}'\")\n",
    "    \n",
    "    # All prompts completed!\n",
    "    \n",
    "    # Log final memory usage\n",
    "    memory_log.append(log_memory_usage(\"end\", \"After all prompts complete\"))\n",
    "    \n",
    "    # Calculate overall completion time\n",
    "    total_time = time.time() - full_start_time\n",
    "    \n",
    "    # Add comparison metrics\n",
    "    prompt_summaries = list(all_results[\"prompt_summaries\"].values())\n",
    "    \n",
    "    # Create sorted list of prompts by accuracy\n",
    "    sorted_by_accuracy = sorted(prompt_summaries, key=lambda x: x[\"accuracy\"], reverse=True)\n",
    "    best_prompt = sorted_by_accuracy[0][\"prompt_name\"] if sorted_by_accuracy else \"None\"\n",
    "    \n",
    "    # Create sorted list of prompts by CER\n",
    "    sorted_by_cer = sorted(prompt_summaries, key=lambda x: x[\"avg_character_error_rate\"])\n",
    "    best_cer_prompt = sorted_by_cer[0][\"prompt_name\"] if sorted_by_cer else \"None\"\n",
    "    \n",
    "    # Create sorted list of prompts by speed\n",
    "    sorted_by_speed = sorted(prompt_summaries, key=lambda x: x[\"avg_processing_time\"])\n",
    "    fastest_prompt = sorted_by_speed[0][\"prompt_name\"] if sorted_by_speed else \"None\"\n",
    "    \n",
    "    # Store comparison metrics\n",
    "    all_results[\"comparison_metrics\"] = {\n",
    "        \"best_accuracy_prompt\": best_prompt,\n",
    "        \"best_cer_prompt\": best_cer_prompt,\n",
    "        \"fastest_prompt\": fastest_prompt,\n",
    "        \"total_processing_time\": total_time,\n",
    "        \"total_operations\": total_operations\n",
    "    }\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_df = pd.DataFrame([\n",
    "        {\n",
    "            \"Prompt\": p[\"prompt_name\"],\n",
    "            \"Category\": p[\"prompt_category\"],\n",
    "            \"Accuracy (%)\": p[\"accuracy\"],\n",
    "            \"CER\": p[\"avg_character_error_rate\"],\n",
    "            \"Avg Time (s)\": p[\"avg_processing_time\"]\n",
    "        }\n",
    "        for p in prompt_summaries\n",
    "    ]).sort_values(\"Accuracy (%)\", ascending=False)\n",
    "    \n",
    "    # Display final comparison table\n",
    "    print(\"\\nüìä Prompt Comparison Results:\")\n",
    "    display(comparison_df)\n",
    "    \n",
    "    # Save all results\n",
    "    results_path = get_results_path(\"prompt_comparison_results.json\")\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    \n",
    "    # Save individual prompt results\n",
    "    for prompt_name, results in results_by_prompt.items():\n",
    "        prompt_results_path = get_results_path(f\"prompt_{prompt_name}_results.json\")\n",
    "        with open(prompt_results_path, \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Save memory log\n",
    "    memory_log_path = get_results_path(\"memory_usage_log.json\")\n",
    "    with open(memory_log_path, \"w\") as f:\n",
    "        json.dump(memory_log, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Results saved to: {results_path}\")\n",
    "    print(f\"üíæ Memory usage log saved to: {memory_log_path}\")\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\nüéâ Prompt Comparison Complete!\")\n",
    "    print(f\"   Compared {len(prompt_collection)} prompts across {total_images} images\")\n",
    "    print(f\"   Total processing time: {total_time:.2f}s ({format_eta(total_time)})\")\n",
    "    print(f\"   Best accuracy: {best_prompt} ({sorted_by_accuracy[0]['accuracy']:.1f}%)\")\n",
    "    print(f\"   Best character error rate: {best_cer_prompt} ({sorted_by_cer[0]['avg_character_error_rate']:.4f})\")\n",
    "    print(f\"   Fastest processing: {fastest_prompt} ({sorted_by_speed[0]['avg_processing_time']:.2f}s per image)\")\n",
    "    \n",
    "    # Store results for analysis\n",
    "    prompt_comparison_results = all_results\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during prompt comparison: {e}\")\n",
    "    \n",
    "    # Try to save partial results\n",
    "    try:\n",
    "        partial_results_path = get_results_path(\"partial_prompt_comparison_results.json\")\n",
    "        with open(partial_results_path, \"w\") as f:\n",
    "            json.dump({\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"error\": str(e),\n",
    "                \"results_by_prompt\": results_by_prompt,\n",
    "                \"completed_prompts\": completed_prompts,\n",
    "                \"prompt_states\": prompt_states\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        print(f\"üíæ Partial results saved to: {partial_results_path}\")\n",
    "    except Exception as save_error:\n",
    "        print(f\"‚ùå Error saving partial results: {save_error}\")\n",
    "    \n",
    "    # Try to save memory log\n",
    "    if memory_log:\n",
    "        try:\n",
    "            memory_log.append(log_memory_usage(\"error\", f\"Error: {str(e)}\"))\n",
    "            memory_log_path = get_results_path(\"error_memory_usage_log.json\")\n",
    "            with open(memory_log_path, \"w\") as f:\n",
    "                json.dump(memory_log, f, indent=2)\n",
    "            \n",
    "            print(f\"üíæ Memory usage log saved to: {memory_log_path}\")\n",
    "        except Exception as log_error:\n",
    "            print(f\"‚ùå Error saving memory log: {log_error}\")\n",
    "    \n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    # Clean up memory regardless of success or failure\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results / Analysis\n",
    "### Basic Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Comparison Metrics Calculation\n",
    "\n",
    "# Import required libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Verify we have prompt comparison results\n",
    "if 'prompt_comparison_results' not in locals() or not prompt_comparison_results:\n",
    "    print(\"‚ùå No prompt comparison results found. Run the prompt comparison extraction cell first.\")\n",
    "    raise RuntimeError(\"No prompt comparison results available for analysis\")\n",
    "\n",
    "print(\"üìä Calculating metrics for prompt comparison results...\")\n",
    "\n",
    "# Get results by prompt\n",
    "results_by_prompt = prompt_comparison_results.get(\"results_by_prompt\", {})\n",
    "prompt_summaries = prompt_comparison_results.get(\"prompt_summaries\", {})\n",
    "\n",
    "# Count prompts and total results\n",
    "prompt_count = len(results_by_prompt)\n",
    "total_results = sum(len(results) for results in results_by_prompt.values())\n",
    "\n",
    "print(f\"‚úÖ Found results for {prompt_count} prompts with {total_results} total extractions\")\n",
    "\n",
    "# Combine all results into a single dataframe for overall analysis\n",
    "all_results = []\n",
    "for prompt_name, results in results_by_prompt.items():\n",
    "    for result in results:\n",
    "        # Ensure result has prompt information\n",
    "        if \"prompt_name\" not in result:\n",
    "            result[\"prompt_name\"] = prompt_name\n",
    "        all_results.append(result)\n",
    "\n",
    "# Convert to DataFrame\n",
    "all_results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Display overall dataframe info\n",
    "print(\"\\nüìã Overall Results Information:\")\n",
    "print(f\"   Total extractions: {len(all_results_df)}\")\n",
    "print(f\"   Columns: {', '.join(all_results_df.columns)}\")\n",
    "\n",
    "# Create per-prompt metrics\n",
    "prompt_metrics = {}\n",
    "\n",
    "# Verify columns needed for analysis\n",
    "required_columns = [\"exact_match\", \"character_error_rate\", \"processing_time\", \"ground_truth\", \"processed_extraction\"]\n",
    "missing_columns = [col for col in required_columns if col not in all_results_df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"‚ö†Ô∏è Warning: Missing required columns: {', '.join(missing_columns)}\")\n",
    "    print(\"   Some metrics may not be calculated correctly\")\n",
    "\n",
    "# Process each prompt\n",
    "for prompt_name, results in results_by_prompt.items():\n",
    "    print(f\"\\nüìä Analyzing results for prompt: {prompt_name}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    prompt_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Basic cleaning\n",
    "    if 'exact_match' in prompt_df.columns:\n",
    "        prompt_df['exact_match'] = prompt_df['exact_match'].astype(bool)\n",
    "    \n",
    "    if 'processing_time' in prompt_df.columns:\n",
    "        prompt_df['processing_time'] = pd.to_numeric(prompt_df['processing_time'], errors='coerce')\n",
    "    \n",
    "    # Get prompt category\n",
    "    prompt_category = prompt_df['prompt_category'].iloc[0] if 'prompt_category' in prompt_df.columns else \"Unknown\"\n",
    "    \n",
    "    # Check for error cases\n",
    "    error_cases = prompt_df[prompt_df['error'].notna()] if 'error' in prompt_df.columns else pd.DataFrame()\n",
    "    valid_results = prompt_df[~prompt_df.index.isin(error_cases.index)] if not error_cases.empty else prompt_df\n",
    "    \n",
    "    # Calculate accuracy metrics\n",
    "    if 'exact_match' in valid_results.columns:\n",
    "        exact_matches = valid_results['exact_match'].sum()\n",
    "        total_valid = len(valid_results)\n",
    "        exact_match_rate = exact_matches / total_valid if total_valid > 0 else 0\n",
    "        \n",
    "        print(f\"   Exact Match Accuracy: {exact_matches}/{total_valid} ({exact_match_rate:.2%})\")\n",
    "    else:\n",
    "        exact_matches = 0\n",
    "        total_valid = len(valid_results)\n",
    "        exact_match_rate = 0\n",
    "    \n",
    "    # Calculate character error rate\n",
    "    if 'character_error_rate' in valid_results.columns:\n",
    "        cer_values = valid_results['character_error_rate'].dropna()\n",
    "        \n",
    "        if len(cer_values) > 0:\n",
    "            min_cer = cer_values.min()\n",
    "            max_cer = cer_values.max()\n",
    "            mean_cer = cer_values.mean()\n",
    "            median_cer = cer_values.median()\n",
    "            \n",
    "            print(f\"   CER: min={min_cer:.4f}, max={max_cer:.4f}, mean={mean_cer:.4f}, median={median_cer:.4f}\")\n",
    "        else:\n",
    "            min_cer = max_cer = mean_cer = median_cer = 0\n",
    "    else:\n",
    "        min_cer = max_cer = mean_cer = median_cer = 0\n",
    "    \n",
    "    # Calculate timing metrics\n",
    "    if 'processing_time' in valid_results.columns:\n",
    "        time_values = valid_results['processing_time'].dropna()\n",
    "        \n",
    "        if len(time_values) > 0:\n",
    "            min_time = time_values.min()\n",
    "            max_time = time_values.max()\n",
    "            mean_time = time_values.mean()\n",
    "            median_time = time_values.median()\n",
    "            total_time = time_values.sum()\n",
    "            \n",
    "            print(f\"   Time: min={min_time:.2f}s, max={max_time:.2f}s, mean={mean_time:.2f}s, total={total_time:.2f}s\")\n",
    "        else:\n",
    "            min_time = max_time = mean_time = median_time = total_time = 0\n",
    "    else:\n",
    "        min_time = max_time = mean_time = median_time = total_time = 0\n",
    "    \n",
    "    # Categorize results\n",
    "    if 'exact_match' in valid_results.columns:\n",
    "        successes = len(valid_results[valid_results['exact_match'] == True])\n",
    "        failures = len(valid_results[valid_results['exact_match'] == False])\n",
    "        errors = len(error_cases)\n",
    "        \n",
    "        print(f\"   Categories: {successes} successes, {failures} failures, {errors} errors\")\n",
    "    else:\n",
    "        successes = failures = 0\n",
    "        errors = len(error_cases)\n",
    "    \n",
    "    # Store metrics for this prompt\n",
    "    prompt_metrics[prompt_name] = {\n",
    "        \"category\": prompt_category,\n",
    "        \"total_extractions\": len(prompt_df),\n",
    "        \"valid_extractions\": total_valid,\n",
    "        \"error_count\": len(error_cases),\n",
    "        \"accuracy\": {\n",
    "            \"exact_match_count\": int(exact_matches),\n",
    "            \"exact_match_rate\": float(exact_match_rate)\n",
    "        },\n",
    "        \"character_error_rate\": {\n",
    "            \"min\": float(min_cer),\n",
    "            \"max\": float(max_cer),\n",
    "            \"mean\": float(mean_cer),\n",
    "            \"median\": float(median_cer)\n",
    "        },\n",
    "        \"timing\": {\n",
    "            \"min_seconds\": float(min_time),\n",
    "            \"max_seconds\": float(max_time),\n",
    "            \"mean_seconds\": float(mean_time),\n",
    "            \"median_seconds\": float(median_time),\n",
    "            \"total_seconds\": float(total_time)\n",
    "        },\n",
    "        \"categories\": {\n",
    "            \"successful_count\": int(successes),\n",
    "            \"failed_count\": int(failures),\n",
    "            \"error_count\": int(errors)\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Create comparative metrics\n",
    "print(\"\\nüîç Generating comparative metrics...\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for prompt_name, metrics in prompt_metrics.items():\n",
    "    comparison_data.append({\n",
    "        \"Prompt\": prompt_name,\n",
    "        \"Category\": metrics[\"category\"],\n",
    "        \"Accuracy (%)\": metrics[\"accuracy\"][\"exact_match_rate\"] * 100,\n",
    "        \"CER\": metrics[\"character_error_rate\"][\"mean\"],\n",
    "        \"Avg Time (s)\": metrics[\"timing\"][\"mean_seconds\"],\n",
    "        \"Success #\": metrics[\"categories\"][\"successful_count\"],\n",
    "        \"Fail #\": metrics[\"categories\"][\"failed_count\"],\n",
    "        \"Error #\": metrics[\"categories\"][\"error_count\"],\n",
    "        \"Total\": metrics[\"total_extractions\"]\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data).sort_values(\"Accuracy (%)\", ascending=False)\n",
    "\n",
    "# Display comparison table\n",
    "print(\"\\nüìä Prompt Comparison:\")\n",
    "display(comparison_df)\n",
    "\n",
    "# Identify best performing prompt in each category\n",
    "best_accuracy_prompt = comparison_df.iloc[0][\"Prompt\"] if not comparison_df.empty else None\n",
    "best_accuracy = comparison_df.iloc[0][\"Accuracy (%)\"] if not comparison_df.empty else 0\n",
    "\n",
    "best_cer_idx = comparison_df[\"CER\"].idxmin() if \"CER\" in comparison_df else None\n",
    "best_cer_prompt = comparison_df.loc[best_cer_idx, \"Prompt\"] if best_cer_idx is not None else None\n",
    "best_cer = comparison_df.loc[best_cer_idx, \"CER\"] if best_cer_idx is not None else 1.0\n",
    "\n",
    "fastest_idx = comparison_df[\"Avg Time (s)\"].idxmin() if \"Avg Time (s)\" in comparison_df else None\n",
    "fastest_prompt = comparison_df.loc[fastest_idx, \"Prompt\"] if fastest_idx is not None else None\n",
    "fastest_time = comparison_df.loc[fastest_idx, \"Avg Time (s)\"] if fastest_idx is not None else 0\n",
    "\n",
    "print(\"\\nüèÜ Best Performing Prompts:\")\n",
    "print(f\"   Best Accuracy: {best_accuracy_prompt} ({best_accuracy:.1f}%)\")\n",
    "print(f\"   Lowest CER: {best_cer_prompt} ({best_cer:.4f})\")\n",
    "print(f\"   Fastest: {fastest_prompt} ({fastest_time:.2f}s)\")\n",
    "\n",
    "# Combine all metrics\n",
    "combined_metrics = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"total_prompts\": prompt_count,\n",
    "    \"total_extractions\": total_results,\n",
    "    \"prompt_metrics\": prompt_metrics,\n",
    "    \"comparison\": {\n",
    "        \"best_accuracy_prompt\": best_accuracy_prompt,\n",
    "        \"best_accuracy_value\": float(best_accuracy),\n",
    "        \"best_cer_prompt\": best_cer_prompt,\n",
    "        \"best_cer_value\": float(best_cer),\n",
    "        \"fastest_prompt\": fastest_prompt,\n",
    "        \"fastest_prompt_time\": float(fastest_time)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metrics to file\n",
    "metrics_path = get_results_path(\"prompt_comparison_metrics.json\")\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    json.dump(combined_metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Comparison metrics saved to: {metrics_path}\")\n",
    "\n",
    "# Generate visualizations\n",
    "print(\"\\nüìà Generating visualizations...\")\n",
    "\n",
    "# Set up the visualization style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"colorblind\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# 1. Accuracy comparison bar chart\n",
    "plt.subplot(2, 2, 1)\n",
    "comparison_df.sort_values(\"Accuracy (%)\", ascending=True).plot(\n",
    "    kind='barh', \n",
    "    x='Prompt', \n",
    "    y='Accuracy (%)', \n",
    "    legend=False, \n",
    "    title='Accuracy by Prompt'\n",
    ")\n",
    "plt.tight_layout()\n",
    "\n",
    "# 2. CER comparison bar chart\n",
    "plt.subplot(2, 2, 2)\n",
    "comparison_df.sort_values(\"CER\", ascending=False).plot(\n",
    "    kind='barh', \n",
    "    x='Prompt', \n",
    "    y='CER', \n",
    "    legend=False, \n",
    "    title='Character Error Rate by Prompt (lower is better)'\n",
    ")\n",
    "plt.tight_layout()\n",
    "\n",
    "# 3. Speed comparison bar chart\n",
    "plt.subplot(2, 2, 3)\n",
    "comparison_df.sort_values(\"Avg Time (s)\", ascending=False).plot(\n",
    "    kind='barh', \n",
    "    x='Prompt', \n",
    "    y='Avg Time (s)', \n",
    "    legend=False, \n",
    "    title='Average Processing Time by Prompt'\n",
    ")\n",
    "plt.tight_layout()\n",
    "\n",
    "# 4. Success/Failure/Error stacked bar chart\n",
    "plt.subplot(2, 2, 4)\n",
    "comparison_df.set_index('Prompt')[['Success #', 'Fail #', 'Error #']].plot(\n",
    "    kind='bar', \n",
    "    stacked=True,\n",
    "    title='Result Distribution by Prompt'\n",
    ")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "comparison_chart_path = get_visualization_path(\"prompt_comparison_summary.png\")\n",
    "plt.savefig(comparison_chart_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"   ‚úÖ Summary chart saved to: {comparison_chart_path}\")\n",
    "\n",
    "# Create categorical analysis - group by category\n",
    "if \"Category\" in comparison_df.columns:\n",
    "    category_comparison = comparison_df.groupby(\"Category\").agg({\n",
    "        \"Accuracy (%)\": \"mean\",\n",
    "        \"CER\": \"mean\",\n",
    "        \"Avg Time (s)\": \"mean\",\n",
    "        \"Success #\": \"sum\",\n",
    "        \"Fail #\": \"sum\",\n",
    "        \"Error #\": \"sum\",\n",
    "        \"Total\": \"sum\"\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Display category comparison\n",
    "    print(\"\\nüìä Category Comparison:\")\n",
    "    display(category_comparison)\n",
    "    \n",
    "    # Create category comparison chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # 1. Accuracy by category\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.barplot(x=\"Category\", y=\"Accuracy (%)\", data=category_comparison)\n",
    "    plt.title(\"Accuracy by Prompt Category\")\n",
    "    \n",
    "    # 2. CER by category\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.barplot(x=\"Category\", y=\"CER\", data=category_comparison)\n",
    "    plt.title(\"CER by Prompt Category\")\n",
    "    \n",
    "    # 3. Time by category\n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.barplot(x=\"Category\", y=\"Avg Time (s)\", data=category_comparison)\n",
    "    plt.title(\"Processing Time by Category\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    category_chart_path = get_visualization_path(\"category_comparison.png\")\n",
    "    plt.savefig(category_chart_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"   ‚úÖ Category comparison chart saved to: {category_chart_path}\")\n",
    "\n",
    "# Store metrics and DataFrames for further analysis\n",
    "comparison_metrics = combined_metrics\n",
    "comparison_results = {\n",
    "    \"all_results_df\": all_results_df,\n",
    "    \"comparison_df\": comparison_df,\n",
    "    \"category_comparison\": category_comparison if \"Category\" in comparison_df.columns else pd.DataFrame(),\n",
    "    \"results_by_prompt\": {prompt: pd.DataFrame(results) for prompt, results in results_by_prompt.items()}\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Prompt comparison metrics calculation and visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Analysis for Prompt Comparison\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from Levenshtein import distance\n",
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Verify we have comparison results\n",
    "if 'comparison_results' not in locals() or not comparison_results:\n",
    "    print(\"‚ùå No comparison results found. Run the metrics calculation cell first.\")\n",
    "    raise RuntimeError(\"No comparison results available for analysis\")\n",
    "\n",
    "print(\"üîç Analyzing extraction errors and failure patterns across prompts...\")\n",
    "\n",
    "# Access results by prompt\n",
    "results_by_prompt = comparison_results.get(\"results_by_prompt\", {})\n",
    "if not results_by_prompt:\n",
    "    print(\"‚ùå No results by prompt found in comparison results.\")\n",
    "    raise RuntimeError(\"Missing prompt-specific results\")\n",
    "\n",
    "# Create a detailed analysis function for extraction failures\n",
    "def analyze_extraction_difference(ground_truth, extraction):\n",
    "    \"\"\"Analyze the difference between ground truth and extraction\"\"\"\n",
    "    if not isinstance(ground_truth, str) or not isinstance(extraction, str):\n",
    "        return \"Invalid input types\"\n",
    "    \n",
    "    # Clean strings\n",
    "    gt = ground_truth.strip()\n",
    "    ext = extraction.strip()\n",
    "    \n",
    "    # Handle empty cases\n",
    "    if not gt and not ext:\n",
    "        return \"Both empty\"\n",
    "    if not gt:\n",
    "        return \"Ground truth empty\"\n",
    "    if not ext:\n",
    "        return \"Extraction empty\"\n",
    "    \n",
    "    # Calculate length difference\n",
    "    len_diff = len(ext) - len(gt)\n",
    "    \n",
    "    # Check for case differences\n",
    "    if gt.lower() == ext.lower():\n",
    "        return \"Case difference only\"\n",
    "    \n",
    "    # Check for whitespace-only differences\n",
    "    if gt.replace(\" \", \"\") == ext.replace(\" \", \"\"):\n",
    "        return \"Whitespace difference only\"\n",
    "        \n",
    "    # Check for character type mismatches (e.g., letter vs. number)\n",
    "    gt_alphas = sum(c.isalpha() for c in gt)\n",
    "    ext_alphas = sum(c.isalpha() for c in ext)\n",
    "    gt_digits = sum(c.isdigit() for c in gt)\n",
    "    ext_digits = sum(c.isdigit() for c in ext)\n",
    "    \n",
    "    if abs(gt_alphas - ext_alphas) > 0.5 * max(gt_alphas, ext_alphas) or \\\n",
    "       abs(gt_digits - ext_digits) > 0.5 * max(gt_digits, ext_digits):\n",
    "        return \"Character type mismatch\"\n",
    "        \n",
    "    # Check for substring relationship\n",
    "    if gt in ext:\n",
    "        return \"Ground truth is substring of extraction\"\n",
    "    if ext in gt:\n",
    "        return \"Extraction is substring of ground truth\"\n",
    "        \n",
    "    # Check for digits-only case\n",
    "    if gt.isdigit() and ext.isdigit():\n",
    "        if len(gt) == len(ext):\n",
    "            return \"Digit transposition or substitution\"\n",
    "        elif len(gt) > len(ext):\n",
    "            return \"Missing digits\"\n",
    "        else:\n",
    "            return \"Extra digits\"\n",
    "            \n",
    "    # For more complex cases, calculate edit distance and characterize\n",
    "    edit_dist = distance(gt, ext)\n",
    "    if edit_dist <= 2:\n",
    "        return \"Minor edit distance (1-2 chars)\"\n",
    "    elif edit_dist <= len(gt) * 0.3:\n",
    "        return \"Moderate edit distance (‚â§30%)\"\n",
    "    else:\n",
    "        return \"Major difference (>30% edit distance)\"\n",
    "\n",
    "# Initialize error analysis storage for each prompt\n",
    "prompt_error_analysis = {}\n",
    "\n",
    "# Track global error patterns\n",
    "all_error_patterns = Counter()\n",
    "total_failures = 0\n",
    "\n",
    "# Process each prompt's results\n",
    "for prompt_name, prompt_df in results_by_prompt.items():\n",
    "    print(f\"\\nüîç Analyzing errors for prompt: {prompt_name}\")\n",
    "    \n",
    "    # Filter to get failures only\n",
    "    if 'exact_match' not in prompt_df.columns:\n",
    "        print(f\"‚ö†Ô∏è No 'exact_match' column found for prompt {prompt_name}, skipping\")\n",
    "        continue\n",
    "        \n",
    "    failures_df = prompt_df[prompt_df['exact_match'] == False].copy()\n",
    "    \n",
    "    if failures_df.empty:\n",
    "        print(f\"‚úÖ No failures found for prompt '{prompt_name}' - perfect accuracy!\")\n",
    "        prompt_error_analysis[prompt_name] = {\n",
    "            \"total_failures\": 0,\n",
    "            \"error_patterns\": [],\n",
    "            \"examples\": [],\n",
    "            \"length_stats\": {}\n",
    "        }\n",
    "        continue\n",
    "    \n",
    "    print(f\"   Found {len(failures_df)} failures to analyze\")\n",
    "    total_failures += len(failures_df)\n",
    "    \n",
    "    # Process each failure to categorize error patterns\n",
    "    pattern_column = []\n",
    "    details_column = []\n",
    "    \n",
    "    for _, row in failures_df.iterrows():\n",
    "        gt = row.get('ground_truth', '')\n",
    "        extracted = row.get('processed_extraction', '')\n",
    "        \n",
    "        if not isinstance(gt, str):\n",
    "            gt = str(gt) if gt is not None else ''\n",
    "        if not isinstance(extracted, str):\n",
    "            extracted = str(extracted) if extracted is not None else ''\n",
    "        \n",
    "        # Get error pattern\n",
    "        pattern = analyze_extraction_difference(gt, extracted)\n",
    "        pattern_column.append(pattern)\n",
    "        all_error_patterns[pattern] += 1\n",
    "        \n",
    "        # Create detailed comparison\n",
    "        if len(gt) > 0 and len(extracted) > 0:\n",
    "            # Highlight differences character by character\n",
    "            max_len = max(len(gt), len(extracted))\n",
    "            detail_text = f\"GT: '{gt}'\\nEX: '{extracted}'\\nDF: '\"\n",
    "            \n",
    "            for i in range(max_len):\n",
    "                if i < len(gt) and i < len(extracted):\n",
    "                    if gt[i] == extracted[i]:\n",
    "                        detail_text += \" \"\n",
    "                    else:\n",
    "                        detail_text += \"^\"\n",
    "                elif i < len(gt):\n",
    "                    detail_text += \"-\"  # Missing in extraction\n",
    "                else:\n",
    "                    detail_text += \"+\"  # Extra in extraction\n",
    "            \n",
    "            detail_text += \"'\"\n",
    "        else:\n",
    "            if len(gt) == 0:\n",
    "                detail_text = \"Ground truth is empty\"\n",
    "            elif len(extracted) == 0:\n",
    "                detail_text = \"Extraction is empty\"\n",
    "            else:\n",
    "                detail_text = \"Unknown comparison\"\n",
    "        \n",
    "        details_column.append(detail_text)\n",
    "    \n",
    "    # Add columns to dataframe\n",
    "    failures_df.loc[:, 'error_pattern'] = pattern_column\n",
    "    failures_df.loc[:, 'difference_details'] = details_column\n",
    "    \n",
    "    # Count error patterns for this prompt\n",
    "    pattern_counts = Counter(pattern_column)\n",
    "    \n",
    "    # Create dataframe of error patterns\n",
    "    error_patterns = pd.DataFrame({\n",
    "        'pattern': list(pattern_counts.keys()),\n",
    "        'count': list(pattern_counts.values())\n",
    "    })\n",
    "    \n",
    "    # Add percentage column\n",
    "    error_patterns['percentage'] = error_patterns['count'] / len(failures_df) * 100\n",
    "    \n",
    "    # Sort by count descending\n",
    "    error_patterns = error_patterns.sort_values('count', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Display top error patterns for this prompt\n",
    "    print(f\"\\n   Top Error Patterns for '{prompt_name}':\")\n",
    "    for i, row in error_patterns.head(3).iterrows():\n",
    "        print(f\"      {row['pattern']}: {row['count']} occurrences ({row['percentage']:.1f}%)\")\n",
    "    \n",
    "    # Select representative examples\n",
    "    examples = []\n",
    "    for pattern in error_patterns['pattern']:\n",
    "        pattern_rows = failures_df[failures_df['error_pattern'] == pattern]\n",
    "        \n",
    "        if not pattern_rows.empty:\n",
    "            # Select a representative example (one with the median CER)\n",
    "            if 'character_error_rate' in pattern_rows.columns:\n",
    "                pattern_rows = pattern_rows.sort_values('character_error_rate')\n",
    "                example_row = pattern_rows.iloc[len(pattern_rows) // 2]  # Median\n",
    "            else:\n",
    "                example_row = pattern_rows.iloc[0]\n",
    "            \n",
    "            examples.append({\n",
    "                'pattern': pattern,\n",
    "                'image_id': example_row.get('image_id', 'Unknown'),\n",
    "                'ground_truth': example_row.get('ground_truth', ''),\n",
    "                'extracted_text': example_row.get('processed_extraction', ''),\n",
    "                'character_error_rate': example_row.get('character_error_rate', None),\n",
    "                'difference_details': example_row.get('difference_details', '')\n",
    "            })\n",
    "    \n",
    "    # Analyze length characteristics\n",
    "    if 'ground_truth' in failures_df.columns and 'processed_extraction' in failures_df.columns:\n",
    "        failures_df.loc[:, 'gt_length'] = failures_df['ground_truth'].astype(str).apply(len)\n",
    "        failures_df.loc[:, 'extraction_length'] = failures_df['processed_extraction'].astype(str).apply(len)\n",
    "        failures_df.loc[:, 'length_diff'] = failures_df['extraction_length'] - failures_df['gt_length']\n",
    "        \n",
    "        # Create length analysis\n",
    "        len_stats = {\n",
    "            'avg_gt_length': float(failures_df['gt_length'].mean()),\n",
    "            'avg_extraction_length': float(failures_df['extraction_length'].mean()),\n",
    "            'avg_abs_length_diff': float(failures_df['length_diff'].abs().mean()),\n",
    "            'shorter_extractions': int((failures_df['length_diff'] < 0).sum()),\n",
    "            'longer_extractions': int((failures_df['length_diff'] > 0).sum()),\n",
    "            'same_length': int((failures_df['length_diff'] == 0).sum())\n",
    "        }\n",
    "    else:\n",
    "        len_stats = {}\n",
    "    \n",
    "    # Store analysis for this prompt\n",
    "    prompt_error_analysis[prompt_name] = {\n",
    "        \"total_failures\": len(failures_df),\n",
    "        \"error_patterns\": error_patterns.to_dict('records'),\n",
    "        \"examples\": examples,\n",
    "        \"length_stats\": len_stats\n",
    "    }\n",
    "\n",
    "# Create comparative error pattern analysis\n",
    "print(\"\\nüìä Comparative Error Pattern Analysis:\")\n",
    "\n",
    "# Create a dataframe with error patterns across all prompts\n",
    "all_patterns_df = pd.DataFrame({\n",
    "    'pattern': list(all_error_patterns.keys()),\n",
    "    'total_count': list(all_error_patterns.values())\n",
    "})\n",
    "all_patterns_df['percentage'] = all_patterns_df['total_count'] / total_failures * 100 if total_failures > 0 else 0\n",
    "all_patterns_df = all_patterns_df.sort_values('total_count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display the most common error patterns overall\n",
    "print(\"\\nüîç Most Common Error Patterns Across All Prompts:\")\n",
    "for i, row in all_patterns_df.head(5).iterrows():\n",
    "    print(f\"   {row['pattern']}: {row['total_count']} occurrences ({row['percentage']:.1f}%)\")\n",
    "\n",
    "# Create a matrix of error patterns by prompt\n",
    "pattern_matrix = pd.DataFrame(index=all_patterns_df['pattern'])\n",
    "\n",
    "for prompt_name, analysis in prompt_error_analysis.items():\n",
    "    if analysis[\"total_failures\"] == 0:\n",
    "        # Skip prompts with no failures\n",
    "        pattern_matrix[prompt_name] = 0\n",
    "        continue\n",
    "        \n",
    "    prompt_patterns = pd.DataFrame(analysis[\"error_patterns\"])\n",
    "    if prompt_patterns.empty:\n",
    "        pattern_matrix[prompt_name] = 0\n",
    "        continue\n",
    "        \n",
    "    # Create a Series with pattern counts\n",
    "    pattern_series = pd.Series(\n",
    "        prompt_patterns.set_index('pattern')['count']\n",
    "    ).reindex(all_patterns_df['pattern']).fillna(0)\n",
    "    \n",
    "    # Add to the matrix\n",
    "    pattern_matrix[prompt_name] = pattern_series\n",
    "\n",
    "# Calculate percentages\n",
    "for col in pattern_matrix.columns:\n",
    "    total = pattern_matrix[col].sum()\n",
    "    if total > 0:\n",
    "        pattern_matrix[f\"{col}_pct\"] = pattern_matrix[col] / total * 100\n",
    "\n",
    "# Display the matrix\n",
    "print(\"\\nüìä Error Pattern Distribution by Prompt:\")\n",
    "display(pattern_matrix.head(10))\n",
    "\n",
    "# Identify prompts that avoid specific error types\n",
    "print(\"\\nüîç Error Type Avoidance by Prompt:\")\n",
    "for pattern in all_patterns_df['pattern'].head(5):\n",
    "    pattern_counts = pattern_matrix.loc[pattern].drop([c for c in pattern_matrix.columns if c.endswith('_pct')])\n",
    "    min_count_prompt = pattern_counts.idxmin()\n",
    "    min_count = pattern_counts.min()\n",
    "    max_count_prompt = pattern_counts.idxmax()\n",
    "    max_count = pattern_counts.max()\n",
    "    \n",
    "    print(f\"   For '{pattern}' errors:\")\n",
    "    print(f\"      Best prompt: {min_count_prompt} ({min_count:.0f} occurrences)\")\n",
    "    print(f\"      Worst prompt: {max_count_prompt} ({max_count:.0f} occurrences)\")\n",
    "\n",
    "# Visualize error patterns\n",
    "if all_patterns_df.shape[0] > 0:\n",
    "    print(\"\\nüìä Creating error pattern visualizations...\")\n",
    "    \n",
    "    # 1. Top 5 error patterns by frequency\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='total_count', y='pattern', data=all_patterns_df.head(5))\n",
    "    plt.title('Top 5 Error Patterns Across All Prompts')\n",
    "    plt.tight_layout()\n",
    "    top_errors_chart_path = get_visualization_path(\"top_error_patterns.png\")\n",
    "    plt.savefig(top_errors_chart_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Heatmap of error patterns by prompt\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    pattern_viz_data = pattern_matrix.drop([c for c in pattern_matrix.columns if c.endswith('_pct')])\n",
    "    if not pattern_viz_data.empty and pattern_viz_data.shape[0] > 0 and pattern_viz_data.shape[1] > 0:\n",
    "        # Only include top 10 patterns for readability\n",
    "        pattern_viz_data = pattern_viz_data.loc[all_patterns_df['pattern'].head(10)]\n",
    "        sns.heatmap(pattern_viz_data, annot=True, fmt='.0f', cmap='YlOrRd')\n",
    "        plt.title('Error Pattern Distribution by Prompt')\n",
    "        plt.tight_layout()\n",
    "        heatmap_path = get_visualization_path(\"error_pattern_heatmap.png\")\n",
    "        plt.savefig(heatmap_path, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"   ‚úÖ Error pattern visualizations saved to:\")\n",
    "        print(f\"      - {top_errors_chart_path}\")\n",
    "        print(f\"      - {heatmap_path}\")\n",
    "\n",
    "# Function to convert NumPy types to native Python types for JSON serialization\n",
    "def convert_to_serializable(obj):\n",
    "    \"\"\"Convert NumPy types to native Python types for JSON serialization\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(item) for item in obj]\n",
    "    elif isinstance(obj, (np.integer, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return convert_to_serializable(obj.tolist())\n",
    "    elif isinstance(obj, pd.DataFrame):\n",
    "        return convert_to_serializable(obj.to_dict('records'))\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Prepare the error analysis data for the report\n",
    "prompt_comparison_error_analysis = {\n",
    "    'overall_patterns': all_patterns_df.to_dict('records'),\n",
    "    'total_failures': total_failures,\n",
    "    'pattern_matrix': pattern_matrix.reset_index().to_dict('records'),\n",
    "    'prompt_analysis': prompt_error_analysis\n",
    "}\n",
    "\n",
    "# Convert to serializable format\n",
    "prompt_comparison_error_analysis = convert_to_serializable(prompt_comparison_error_analysis)\n",
    "\n",
    "# Save error analysis to file\n",
    "error_analysis_path = get_results_path(\"prompt_comparison_error_analysis.json\")\n",
    "with open(error_analysis_path, \"w\") as f:\n",
    "    json.dump(prompt_comparison_error_analysis, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Prompt comparison error analysis saved to: {error_analysis_path}\")\n",
    "print(\"‚úÖ Error analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization for Prompt Comparison\n",
    "\n",
    "# Import required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Verify we have results to visualize\n",
    "if 'prompt_comparison_results' not in locals() or not prompt_comparison_results:\n",
    "    print(\"‚ùå No prompt comparison results to visualize. Run the metrics calculation cell first.\")\n",
    "    raise RuntimeError(\"No prompt comparison results available for visualization\")\n",
    "\n",
    "print(\"üìä Creating visualizations for prompt comparison results...\")\n",
    "\n",
    "# Create directory for visualizations if needed\n",
    "vis_dir = get_results_path(\"visualizations\")\n",
    "os.makedirs(vis_dir, exist_ok=True)\n",
    "print(f\"üìÅ Saving visualizations to: {vis_dir}\")\n",
    "\n",
    "# Function to save figures\n",
    "def save_figure(fig, filename):\n",
    "    \"\"\"Save a figure to the visualizations directory\"\"\"\n",
    "    filepath = os.path.join(vis_dir, filename)\n",
    "    fig.savefig(filepath, bbox_inches='tight', dpi=150)\n",
    "    return filepath\n",
    "\n",
    "# Get results by prompt and prompt summaries\n",
    "results_by_prompt = prompt_comparison_results.get(\"results_by_prompt\", {})\n",
    "prompt_summaries = prompt_comparison_results.get(\"prompt_summaries\", {})\n",
    "\n",
    "# Convert summaries to DataFrame for easier visualization\n",
    "summary_data = []\n",
    "for prompt_name, summary in prompt_summaries.items():\n",
    "    summary_data.append({\n",
    "        \"Prompt\": prompt_name,\n",
    "        \"Category\": summary.get(\"prompt_category\", \"Unknown\"),\n",
    "        \"Accuracy (%)\": summary.get(\"accuracy\", 0),\n",
    "        \"Exact Matches\": summary.get(\"exact_matches\", 0),\n",
    "        \"Total Images\": summary.get(\"total_images\", 0),\n",
    "        \"Avg CER\": summary.get(\"avg_character_error_rate\", 0),\n",
    "        \"Avg Time (s)\": summary.get(\"avg_processing_time\", 0),\n",
    "        \"Total Time (s)\": summary.get(\"total_processing_time\", 0)\n",
    "    })\n",
    "\n",
    "# Create DataFrame and sort by accuracy\n",
    "if summary_data:\n",
    "    summary_df = pd.DataFrame(summary_data).sort_values(\"Accuracy (%)\", ascending=False)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No prompt summary data available\")\n",
    "    summary_df = pd.DataFrame()\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 1. Create prompt accuracy comparison bar chart\n",
    "# -------------------------------------------------------------------------------\n",
    "print(\"üìä Creating prompt accuracy comparison chart...\")\n",
    "\n",
    "if not summary_df.empty:\n",
    "    # Create the figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Create bar chart\n",
    "    bars = ax.bar(summary_df[\"Prompt\"], summary_df[\"Accuracy (%)\"], \n",
    "                  color=sns.color_palette(\"viridis\", len(summary_df)))\n",
    "    \n",
    "    # Add title and labels\n",
    "    ax.set_title('Prompt Comparison: Extraction Accuracy', fontsize=16)\n",
    "    ax.set_ylabel('Accuracy (%)', fontsize=14)\n",
    "    ax.set_xlabel('Prompt', fontsize=14)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{height:.1f}%', \n",
    "                ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    # Rotate x-axis labels for readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    accuracy_chart_path = save_figure(fig, \"prompt_accuracy_comparison.png\")\n",
    "    plt.close(fig)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data available for accuracy comparison chart\")\n",
    "    accuracy_chart_path = None\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 2. Create character error rate comparison bar chart\n",
    "# -------------------------------------------------------------------------------\n",
    "print(\"üìä Creating CER comparison chart...\")\n",
    "\n",
    "if not summary_df.empty and \"Avg CER\" in summary_df.columns:\n",
    "    # Create the figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Sort by CER (ascending - lower is better)\n",
    "    cer_sorted_df = summary_df.sort_values(\"Avg CER\")\n",
    "    \n",
    "    # Create bar chart\n",
    "    bars = ax.bar(cer_sorted_df[\"Prompt\"], cer_sorted_df[\"Avg CER\"], \n",
    "                  color=sns.color_palette(\"crest\", len(cer_sorted_df)))\n",
    "    \n",
    "    # Add title and labels\n",
    "    ax.set_title('Prompt Comparison: Character Error Rate (Lower is Better)', fontsize=16)\n",
    "    ax.set_ylabel('Average CER', fontsize=14)\n",
    "    ax.set_xlabel('Prompt', fontsize=14)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.4f}', \n",
    "                ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    # Rotate x-axis labels for readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    cer_chart_path = save_figure(fig, \"prompt_cer_comparison.png\")\n",
    "    plt.close(fig)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data available for CER comparison chart\")\n",
    "    cer_chart_path = None\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 3. Create processing time comparison bar chart\n",
    "# -------------------------------------------------------------------------------\n",
    "print(\"üìä Creating processing time comparison chart...\")\n",
    "\n",
    "if not summary_df.empty and \"Avg Time (s)\" in summary_df.columns:\n",
    "    # Create the figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Sort by processing time (ascending - faster is better)\n",
    "    time_sorted_df = summary_df.sort_values(\"Avg Time (s)\")\n",
    "    \n",
    "    # Create bar chart\n",
    "    bars = ax.bar(time_sorted_df[\"Prompt\"], time_sorted_df[\"Avg Time (s)\"], \n",
    "                  color=sns.color_palette(\"magma\", len(time_sorted_df)))\n",
    "    \n",
    "    # Add title and labels\n",
    "    ax.set_title('Prompt Comparison: Average Processing Time (Lower is Better)', fontsize=16)\n",
    "    ax.set_ylabel('Average Time (seconds)', fontsize=14)\n",
    "    ax.set_xlabel('Prompt', fontsize=14)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                f'{height:.2f}s', \n",
    "                ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    # Rotate x-axis labels for readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    time_chart_path = save_figure(fig, \"prompt_time_comparison.png\")\n",
    "    plt.close(fig)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data available for processing time comparison chart\")\n",
    "    time_chart_path = None\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 4. Create multi-metric radar chart\n",
    "# -------------------------------------------------------------------------------\n",
    "print(\"üìä Creating radar chart for multi-metric comparison...\")\n",
    "\n",
    "if not summary_df.empty and len(summary_df) >= 2:\n",
    "    # Create a radar chart to compare prompts across metrics\n",
    "    \n",
    "    # Normalize metrics for comparison\n",
    "    metrics = [\"Accuracy (%)\", \"Avg CER\", \"Avg Time (s)\"]\n",
    "    radar_df = summary_df.copy()\n",
    "    \n",
    "    # Invert CER and Time so that higher is always better\n",
    "    radar_df[\"Avg CER\"] = 1 - radar_df[\"Avg CER\"]\n",
    "    \n",
    "    # Normalize each metric to 0-1 scale\n",
    "    for metric in metrics:\n",
    "        if radar_df[metric].max() != radar_df[metric].min():\n",
    "            radar_df[metric] = (radar_df[metric] - radar_df[metric].min()) / (radar_df[metric].max() - radar_df[metric].min())\n",
    "        else:\n",
    "            radar_df[metric] = 1\n",
    "    \n",
    "    # Create radar chart\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, polar=True)\n",
    "    \n",
    "    # Set categories and number of prompts\n",
    "    categories = [\"Accuracy\", \"1-CER\", \"Speed\"]\n",
    "    N = len(categories)\n",
    "    \n",
    "    # Create angles for each category\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Close the loop\n",
    "    \n",
    "    # Set radar chart properties\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_rlabel_position(0)\n",
    "    \n",
    "    # Add category labels\n",
    "    plt.xticks(angles[:-1], categories, size=14)\n",
    "    \n",
    "    # Plot each prompt\n",
    "    for i, prompt in enumerate(radar_df[\"Prompt\"]):\n",
    "        values = radar_df.loc[radar_df[\"Prompt\"] == prompt, metrics].values.flatten().tolist()\n",
    "        values += values[:1]  # Close the loop\n",
    "        \n",
    "        # Plot values\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=prompt)\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    \n",
    "    # Add title\n",
    "    plt.title('Prompt Performance Comparison (Higher is Better)', size=16, y=1.1)\n",
    "    \n",
    "    # Save the figure\n",
    "    radar_chart_path = save_figure(fig, \"prompt_radar_comparison.png\")\n",
    "    plt.close(fig)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not enough data for radar comparison chart\")\n",
    "    radar_chart_path = None\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 5. Create stacked bar chart for success/failure/error counts\n",
    "# -------------------------------------------------------------------------------\n",
    "print(\"üìä Creating stacked bar chart for result counts...\")\n",
    "\n",
    "if not summary_df.empty:\n",
    "    # Create the figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Extract data for stacked bars\n",
    "    prompts = summary_df[\"Prompt\"]\n",
    "    successes = summary_df[\"Exact Matches\"]\n",
    "    failures = summary_df[\"Total Images\"] - summary_df[\"Exact Matches\"]\n",
    "    \n",
    "    # Create stacked bar chart\n",
    "    ax.bar(prompts, successes, label='Exact Matches', color='#2ecc71')\n",
    "    ax.bar(prompts, failures, bottom=successes, label='Non-Matches', color='#e74c3c')\n",
    "    \n",
    "    # Add title and labels\n",
    "    ax.set_title('Prompt Comparison: Exact Matches vs Non-Matches', fontsize=16)\n",
    "    ax.set_ylabel('Number of Images', fontsize=14)\n",
    "    ax.set_xlabel('Prompt', fontsize=14)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend()\n",
    "    \n",
    "    # Rotate x-axis labels for readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    stacked_chart_path = save_figure(fig, \"prompt_results_stacked.png\")\n",
    "    plt.close(fig)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data available for stacked bar chart\")\n",
    "    stacked_chart_path = None\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 6. Generate HTML dashboard for prompt comparison\n",
    "# -------------------------------------------------------------------------------\n",
    "print(\"üìÑ Generating HTML dashboard for prompt comparison...\")\n",
    "\n",
    "# Get metadata for the dashboard\n",
    "experiment_name = prompt_comparison_results.get(\"experiment\", {}).get(\"name\", \"Prompt Comparison Experiment\")\n",
    "model_name = prompt_comparison_results.get(\"model\", {}).get(\"name\", \"Unknown Model\")\n",
    "prompt_count = len(summary_df)\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Get comparison metrics\n",
    "best_accuracy_prompt = prompt_comparison_results.get(\"comparison_metrics\", {}).get(\"best_accuracy_prompt\", \"Unknown\")\n",
    "best_accuracy = prompt_comparison_results.get(\"comparison_metrics\", {}).get(\"best_accuracy_value\", 0)\n",
    "best_cer_prompt = prompt_comparison_results.get(\"comparison_metrics\", {}).get(\"best_cer_prompt\", \"Unknown\")\n",
    "best_cer = prompt_comparison_results.get(\"comparison_metrics\", {}).get(\"best_cer_value\", 0)\n",
    "fastest_prompt = prompt_comparison_results.get(\"comparison_metrics\", {}).get(\"fastest_prompt\", \"Unknown\")\n",
    "fastest_time = prompt_comparison_results.get(\"comparison_metrics\", {}).get(\"fastest_prompt_time\", 0)\n",
    "\n",
    "# Create prompt info HTML\n",
    "prompt_info_html = \"\"\n",
    "for i, row in summary_df.iterrows():\n",
    "    prompt_name = row[\"Prompt\"]\n",
    "    category = row[\"Category\"]\n",
    "    accuracy = row[\"Accuracy (%)\"]\n",
    "    cer = row[\"Avg CER\"]\n",
    "    time = row[\"Avg Time (s)\"]\n",
    "    \n",
    "    prompt_info_html += f\"\"\"\n",
    "    <div class=\"prompt-box\">\n",
    "        <h3>{prompt_name} <span class=\"category\">({category})</span></h3>\n",
    "        <div class=\"prompt-metrics\">\n",
    "            <div class=\"metric\"><span class=\"label\">Accuracy:</span> <span class=\"value\">{accuracy:.1f}%</span></div>\n",
    "            <div class=\"metric\"><span class=\"label\">CER:</span> <span class=\"value\">{cer:.4f}</span></div>\n",
    "            <div class=\"metric\"><span class=\"label\">Avg Time:</span> <span class=\"value\">{time:.2f}s</span></div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "# Create HTML content\n",
    "html_content = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Prompt Comparison Dashboard</title>\n",
    "    <style>\n",
    "        body {{\n",
    "            font-family: Arial, sans-serif;\n",
    "            margin: 0;\n",
    "            padding: 20px;\n",
    "            background-color: #f5f5f5;\n",
    "        }}\n",
    "        .container {{\n",
    "            max-width: 1200px;\n",
    "            margin: 0 auto;\n",
    "            background-color: white;\n",
    "            padding: 20px;\n",
    "            border-radius: 5px;\n",
    "            box-shadow: 0 0 10px rgba(0,0,0,0.1);\n",
    "        }}\n",
    "        h1, h2, h3 {{\n",
    "            color: #333;\n",
    "        }}\n",
    "        .metrics {{\n",
    "            display: flex;\n",
    "            flex-wrap: wrap;\n",
    "            margin: 20px 0;\n",
    "        }}\n",
    "        .metric-card {{\n",
    "            background-color: white;\n",
    "            border-radius: 5px;\n",
    "            box-shadow: 0 0 5px rgba(0,0,0,0.1);\n",
    "            padding: 15px;\n",
    "            margin: 10px;\n",
    "            min-width: 200px;\n",
    "            flex: 1;\n",
    "        }}\n",
    "        .metric-title {{\n",
    "            font-size: 14px;\n",
    "            color: #666;\n",
    "            margin-bottom: 5px;\n",
    "        }}\n",
    "        .metric-value {{\n",
    "            font-size: 24px;\n",
    "            font-weight: bold;\n",
    "            color: #333;\n",
    "        }}\n",
    "        .chart-container {{\n",
    "            margin: 20px 0;\n",
    "            text-align: center;\n",
    "        }}\n",
    "        .chart {{\n",
    "            max-width: 100%;\n",
    "            height: auto;\n",
    "            margin: 10px 0;\n",
    "            border: 1px solid #eee;\n",
    "        }}\n",
    "        .experiment-info {{\n",
    "            background-color: #f9f9f9;\n",
    "            padding: 15px;\n",
    "            border-radius: 5px;\n",
    "            margin-bottom: 20px;\n",
    "        }}\n",
    "        .prompt-box {{\n",
    "            background-color: #f5f5f5;\n",
    "            padding: 15px;\n",
    "            border-radius: 5px;\n",
    "            border-left: 4px solid #3498db;\n",
    "            margin: 10px 0;\n",
    "        }}\n",
    "        .prompt-metrics {{\n",
    "            display: flex;\n",
    "            margin-top: 10px;\n",
    "        }}\n",
    "        .metric {{\n",
    "            margin-right: 20px;\n",
    "        }}\n",
    "        .label {{\n",
    "            font-weight: bold;\n",
    "            color: #555;\n",
    "        }}\n",
    "        .value {{\n",
    "            color: #333;\n",
    "        }}\n",
    "        .category {{\n",
    "            font-weight: normal;\n",
    "            color: #777;\n",
    "            font-size: 16px;\n",
    "        }}\n",
    "        .winners {{\n",
    "            display: flex;\n",
    "            flex-wrap: wrap;\n",
    "            margin: 20px 0;\n",
    "        }}\n",
    "        .winner-card {{\n",
    "            background-color: #f9f9f9;\n",
    "            border-radius: 5px;\n",
    "            box-shadow: 0 0 5px rgba(0,0,0,0.1);\n",
    "            padding: 15px;\n",
    "            margin: 10px;\n",
    "            flex: 1;\n",
    "            min-width: 250px;\n",
    "            border-left: 4px solid #2ecc71;\n",
    "        }}\n",
    "        .winner-title {{\n",
    "            font-size: 16px;\n",
    "            font-weight: bold;\n",
    "            color: #333;\n",
    "            margin-bottom: 10px;\n",
    "        }}\n",
    "        .winner-prompt {{\n",
    "            font-size: 18px;\n",
    "            color: #2980b9;\n",
    "            margin-bottom: 5px;\n",
    "        }}\n",
    "        .winner-value {{\n",
    "            font-size: 16px;\n",
    "            color: #333;\n",
    "        }}\n",
    "        table {{\n",
    "            width: 100%;\n",
    "            border-collapse: collapse;\n",
    "            margin: 20px 0;\n",
    "        }}\n",
    "        th, td {{\n",
    "            padding: 10px;\n",
    "            text-align: left;\n",
    "            border-bottom: 1px solid #ddd;\n",
    "        }}\n",
    "        th {{\n",
    "            background-color: #f2f2f2;\n",
    "        }}\n",
    "        tr:hover {{\n",
    "            background-color: #f9f9f9;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>Prompt Comparison Dashboard</h1>\n",
    "        \n",
    "        <div class=\"experiment-info\">\n",
    "            <h2>Experiment Information</h2>\n",
    "            <p><strong>Experiment:</strong> {experiment_name}</p>\n",
    "            <p><strong>Model:</strong> {model_name}</p>\n",
    "            <p><strong>Date:</strong> {timestamp}</p>\n",
    "            <p><strong>Prompts Compared:</strong> {prompt_count}</p>\n",
    "        </div>\n",
    "        \n",
    "        <h2>Top Performers</h2>\n",
    "        <div class=\"winners\">\n",
    "            <div class=\"winner-card\">\n",
    "                <div class=\"winner-title\">Best Accuracy</div>\n",
    "                <div class=\"winner-prompt\">{best_accuracy_prompt}</div>\n",
    "                <div class=\"winner-value\">{best_accuracy:.1f}%</div>\n",
    "            </div>\n",
    "            <div class=\"winner-card\">\n",
    "                <div class=\"winner-title\">Lowest Character Error Rate</div>\n",
    "                <div class=\"winner-prompt\">{best_cer_prompt}</div>\n",
    "                <div class=\"winner-value\">{best_cer:.4f}</div>\n",
    "            </div>\n",
    "            <div class=\"winner-card\">\n",
    "                <div class=\"winner-title\">Fastest Processing</div>\n",
    "                <div class=\"winner-prompt\">{fastest_prompt}</div>\n",
    "                <div class=\"winner-value\">{fastest_time:.2f}s</div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <h2>Comparison Charts</h2>\n",
    "        \n",
    "        <div class=\"chart-container\">\n",
    "            <h3>Accuracy Comparison</h3>\n",
    "            <img class=\"chart\" src=\"prompt_accuracy_comparison.png\" alt=\"Accuracy Comparison\">\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"chart-container\">\n",
    "            <h3>Character Error Rate Comparison</h3>\n",
    "            <img class=\"chart\" src=\"prompt_cer_comparison.png\" alt=\"CER Comparison\">\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"chart-container\">\n",
    "            <h3>Processing Time Comparison</h3>\n",
    "            <img class=\"chart\" src=\"prompt_time_comparison.png\" alt=\"Time Comparison\">\n",
    "        </div>\n",
    "        \n",
    "        {f'<div class=\"chart-container\"><h3>Multi-Metric Comparison</h3><img class=\"chart\" src=\"prompt_radar_comparison.png\" alt=\"Radar Comparison\"></div>' if 'radar_chart_path' in locals() and radar_chart_path else ''}\n",
    "        \n",
    "        {f'<div class=\"chart-container\"><h3>Success/Failure Breakdown</h3><img class=\"chart\" src=\"prompt_results_stacked.png\" alt=\"Results Breakdown\"></div>' if 'stacked_chart_path' in locals() and stacked_chart_path else ''}\n",
    "        \n",
    "        <h2>Detailed Results</h2>\n",
    "        \n",
    "        <h3>Prompt Comparison Table</h3>\n",
    "        <table>\n",
    "            <thead>\n",
    "                <tr>\n",
    "                    <th>Prompt</th>\n",
    "                    <th>Category</th>\n",
    "                    <th>Accuracy (%)</th>\n",
    "                    <th>CER</th>\n",
    "                    <th>Avg Time (s)</th>\n",
    "                    <th>Exact Matches</th>\n",
    "                    <th>Total Images</th>\n",
    "                </tr>\n",
    "            </thead>\n",
    "            <tbody>\n",
    "                {\"\".join([f'<tr><td>{row[\"Prompt\"]}</td><td>{row[\"Category\"]}</td><td>{row[\"Accuracy (%)\"]:.1f}</td><td>{row[\"Avg CER\"]:.4f}</td><td>{row[\"Avg Time (s)\"]:.2f}</td><td>{row[\"Exact Matches\"]}</td><td>{row[\"Total Images\"]}</td></tr>' for _, row in summary_df.iterrows()])}\n",
    "            </tbody>\n",
    "        </table>\n",
    "        \n",
    "        <h3>Prompt Details</h3>\n",
    "        {prompt_info_html}\n",
    "        \n",
    "        <h2>Conclusion</h2>\n",
    "        <p>This dashboard presents the results of comparing {prompt_count} different prompts for extracting work order numbers from invoice images using the {model_name} model.</p>\n",
    "        <p>The best performing prompt was <strong>{best_accuracy_prompt}</strong> with an accuracy of <strong>{best_accuracy:.1f}%</strong>.</p>\n",
    "        \n",
    "        <footer style=\"margin-top: 50px; text-align: center; color: #777; font-size: 12px;\">\n",
    "            Generated on {timestamp} ‚Ä¢ Invoice Processing Analysis System\n",
    "        </footer>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Save the HTML dashboard\n",
    "dashboard_path = get_results_path(\"prompt_comparison_dashboard.html\")\n",
    "with open(dashboard_path, \"w\") as f:\n",
    "    f.write(html_content)\n",
    "\n",
    "print(f\"\\nüìÑ Dashboard saved to: {dashboard_path}\")\n",
    "print(\"‚úÖ Prompt comparison visualizations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource Cleanup\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üßπ Cleaning up resources and recording final state...\")\n",
    "\n",
    "# Record initial memory state\n",
    "initial_memory = None\n",
    "if torch.cuda.is_available():\n",
    "    initial_memory = {\n",
    "        \"total_memory_gb\": torch.cuda.get_device_properties(0).total_memory / 1e9,\n",
    "        \"allocated_memory_gb\": torch.cuda.memory_allocated(0) / 1e9,\n",
    "        \"reserved_memory_gb\": torch.cuda.memory_reserved(0) / 1e9,\n",
    "        \"free_memory_gb\": (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1e9\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüß† Initial GPU Memory State:\")\n",
    "    print(f\"   Total Memory: {initial_memory['total_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Allocated Memory: {initial_memory['allocated_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Reserved Memory: {initial_memory['reserved_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Free Memory: {initial_memory['free_memory_gb']:.2f} GB\")\n",
    "\n",
    "# Release model from memory\n",
    "print(\"\\nüîÑ Releasing model from memory...\")\n",
    "if 'model' in locals():\n",
    "    try:\n",
    "        # Delete the model\n",
    "        del model\n",
    "        print(\"   ‚úì Model deleted\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error deleting model: {e}\")\n",
    "else:\n",
    "    print(\"   ‚ÑπÔ∏è No model in memory to delete\")\n",
    "\n",
    "# Release processor from memory\n",
    "if 'processor' in locals():\n",
    "    try:\n",
    "        # Delete the processor\n",
    "        del processor\n",
    "        print(\"   ‚úì Processor deleted\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error deleting processor: {e}\")\n",
    "else:\n",
    "    print(\"   ‚ÑπÔ∏è No processor in memory to delete\")\n",
    "\n",
    "# Release pipeline from memory\n",
    "if 'pipeline' in locals():\n",
    "    try:\n",
    "        # Delete the pipeline\n",
    "        del pipeline\n",
    "        print(\"   ‚úì Pipeline deleted\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error deleting pipeline: {e}\")\n",
    "else:\n",
    "    print(\"   ‚ÑπÔ∏è No pipeline in memory to delete\")\n",
    "\n",
    "# Release other large variables\n",
    "large_variables = ['extraction_results', 'categorized_results', 'all_results', 'metrics_results']\n",
    "for var_name in large_variables:\n",
    "    if var_name in locals():\n",
    "        try:\n",
    "            # Use exec to delete the variable\n",
    "            exec(f\"del {var_name}\")\n",
    "            print(f\"   ‚úì Deleted {var_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error deleting {var_name}: {e}\")\n",
    "\n",
    "# Run garbage collection\n",
    "print(\"\\nüßπ Running garbage collection...\")\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"   ‚úì CUDA cache emptied and synchronized\")\n",
    "else:\n",
    "    print(\"   ‚ÑπÔ∏è No CUDA available for cache emptying\")\n",
    "\n",
    "# Clean up temporary files\n",
    "print(\"\\nüóëÔ∏è Cleaning up temporary files...\")\n",
    "\n",
    "# Define temporary directories to clean\n",
    "temp_dirs = []\n",
    "\n",
    "# Clean up any temporary files in the results directory\n",
    "temp_files = []  # Add any specific temp files pattern here if needed\n",
    "\n",
    "# Remove temporary files if any were specified\n",
    "for file_pattern in temp_files:\n",
    "    matching_files = []  # Add glob or similar to find matching files\n",
    "    for file_path in matching_files:\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"   ‚úì Removed temporary file: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error removing file {file_path}: {e}\")\n",
    "\n",
    "# Record final memory state\n",
    "final_memory = None\n",
    "if torch.cuda.is_available():\n",
    "    final_memory = {\n",
    "        \"total_memory_gb\": torch.cuda.get_device_properties(0).total_memory / 1e9,\n",
    "        \"allocated_memory_gb\": torch.cuda.memory_allocated(0) / 1e9,\n",
    "        \"reserved_memory_gb\": torch.cuda.memory_reserved(0) / 1e9,\n",
    "        \"free_memory_gb\": (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1e9\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüß† Final GPU Memory State:\")\n",
    "    print(f\"   Total Memory: {final_memory['total_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Allocated Memory: {final_memory['allocated_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Reserved Memory: {final_memory['reserved_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Free Memory: {final_memory['free_memory_gb']:.2f} GB\")\n",
    "    \n",
    "    if initial_memory:\n",
    "        memory_freed = initial_memory['allocated_memory_gb'] - final_memory['allocated_memory_gb']\n",
    "        print(f\"   Memory Freed: {memory_freed:.2f} GB\")\n",
    "\n",
    "# Record final environment state\n",
    "final_state = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"torch_version\": torch.__version__,\n",
    "    \"cuda_available\": torch.cuda.is_available(),\n",
    "    \"initial_memory\": initial_memory,\n",
    "    \"final_memory\": final_memory,\n",
    "    \"cleanup_success\": True\n",
    "}\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    final_state[\"cuda_version\"] = torch.version.cuda\n",
    "    final_state[\"device_name\"] = torch.cuda.get_device_name(0)\n",
    "\n",
    "# Save final state\n",
    "try:\n",
    "    final_state_path = get_results_path(\"final_environment_state.json\")\n",
    "    with open(final_state_path, \"w\") as f:\n",
    "        json.dump(final_state, f, indent=2)\n",
    "    print(f\"\\nüíæ Final environment state saved to: {final_state_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Error saving final state: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Resource cleanup complete!\")\n",
    "print(\"\\nüìù Experiment execution finished. Results and visualizations are available in the results directory.\")\n",
    "print(f\"   Results directory: {get_results_path('')}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
