{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Model, Single Prompt Test\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates the extraction of work order numbers from invoice images using a single model (Pixtral-12B) and a single prompt approach. It serves as a replication of the successful RunPod experiment using our new structured framework.\n",
    "\n",
    "### Objectives\n",
    "- Load the Pixtral-12B model using our model management framework\n",
    "- Apply a specific prompt that was effective in previous tests\n",
    "- Process invoice images to extract work order numbers\n",
    "- Evaluate extraction accuracy against ground truth data\n",
    "- Establish a baseline for further prompt and model experimentation\n",
    "\n",
    "### Approach\n",
    "We'll use the basic prompt that worked well in our initial tests:\n",
    "```\n",
    "Extract the work order number from this invoice image.\n",
    "```\n",
    "\n",
    "This will be formatted appropriately for the Pixtral model using our prompt management system.\n",
    "\n",
    "### Expected Results\n",
    "- Extraction accuracy metrics (exact match rate, character error rate)\n",
    "- Processing time analysis\n",
    "- Comparison with previous RunPod experiment results\n",
    "- Foundation for systematic prompt and model comparisons\n",
    "\n",
    "This notebook represents Step 1 in our experimental workflow, focusing on reproducing known successful results before expanding to prompt comparison (Step 2) and model comparison (Step 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Environment Verification\n",
    "### Environment Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import torch\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure basic logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('invoice_extraction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to check if we're in a RunPod environment\n",
    "def is_runpod():\n",
    "    \"\"\"Check if we're running in a RunPod environment\"\"\"\n",
    "    return (\n",
    "        os.environ.get(\"RUNPOD_POD_ID\") is not None or \n",
    "        \"A100\" in os.environ.get(\"GPU_NAME\", \"\") or\n",
    "        \"H100\" in os.environ.get(\"GPU_NAME\", \"\")\n",
    "    )\n",
    "\n",
    "# Display basic system information\n",
    "print(f\"üîç Environment Setup and Verification\")\n",
    "print(f\"üêç Python version: {platform.python_version()}\")\n",
    "print(f\"üìä PyTorch version: {torch.__version__}\")\n",
    "print(f\"ü§ñ Transformers version: {transformers.__version__}\")\n",
    "print(f\"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"üñ•Ô∏è GPU: {device_name} ({memory:.2f} GB)\")\n",
    "    print(f\"üîç CUDA Version: {torch.version.cuda}\")\n",
    "    \n",
    "    # Display GPU memory usage\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    print(f\"üíæ GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
    "    \n",
    "    # Basic GPU compatibility check\n",
    "    if memory < 24:\n",
    "        print(\"‚ö†Ô∏è Warning: GPU memory may be insufficient for Pixtral-12B without quantization\")\n",
    "    else:\n",
    "        print(\"‚úÖ GPU memory sufficient for model loading\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected - running in CPU mode\")\n",
    "    print(\"   Note: Processing will be extremely slow without GPU acceleration\")\n",
    "\n",
    "# Identify environment type\n",
    "if is_runpod():\n",
    "    print(\"‚òÅÔ∏è RunPod environment detected\")\n",
    "else:\n",
    "    print(\"üíª Local environment detected\")\n",
    "\n",
    "# Display platform details for debugging\n",
    "print(f\"üíª System: {platform.system()} {platform.release()}\")\n",
    "print(f\"üìÇ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Core Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Any, Optional, Union\n",
    "\n",
    "# Data processing and visualization libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from Levenshtein import distance\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    LlavaForConditionalGeneration,\n",
    "    AutoModelForVision2Seq,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),  # Log to console\n",
    "        logging.FileHandler(f\"logs/extraction_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")  # Log to file\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger('invoice_extraction')\n",
    "logger.info(\"Starting invoice extraction experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project-specific modules\n",
    "try:\n",
    "    # Attempt to import required modules from src directory\n",
    "    # These imports assume the project root has been added to sys.path in Cell 1\n",
    "    \n",
    "    # Configuration modules\n",
    "    from src.config.environment import get_environment_config\n",
    "    from src.config.paths import get_path_config, PathConfig\n",
    "    from src.config.experiment import get_experiment_config, ExperimentConfig\n",
    "    \n",
    "    # Model handling modules\n",
    "    from src.models.loader import load_model_and_processor, get_gpu_memory_info\n",
    "    from src.models.registry import get_model_config\n",
    "    from src.models.optimization import optimize_model_memory\n",
    "    \n",
    "    # Prompt management modules\n",
    "    from src.prompts.registry import get_prompt, list_available_prompts, format_prompt\n",
    "    from src.prompts.prompt_utils import analyze_prompt\n",
    "    \n",
    "    # Execution modules\n",
    "    from src.execution.inference import process_image\n",
    "    from src.execution.pipeline import ExtractionPipeline\n",
    "    from src.execution.batch import prepare_batch_items\n",
    "    \n",
    "    # Results and analysis modules\n",
    "    from src.results.collector import ResultsCollector\n",
    "    from src.results.metrics import calculate_metrics\n",
    "    from src.analysis.visualization import create_visualizations\n",
    "    \n",
    "    logger.info(\"Successfully imported project modules\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    logger.error(f\"Error importing project modules: {e}\")\n",
    "    logger.warning(\"Some functionality may be limited. Check your project structure and path configuration.\")\n",
    "    print(f\"‚ö†Ô∏è Warning: Failed to import some project modules: {e}\")\n",
    "    print(\"   This may happen if you haven't implemented these modules yet.\")\n",
    "    print(\"   The notebook will attempt to continue, but some cells may need modification.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of imported libraries\n",
    "print(\"üìö Core Libraries Imported:\")\n",
    "print(f\"üî¢ NumPy: {np.__version__}\")\n",
    "print(f\"üìä Pandas: {pd.__version__}\")\n",
    "print(f\"üß† PyTorch: {torch.__version__}\")\n",
    "print(f\"ü§ñ Transformers: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU acceleration\n",
    "if torch.cuda.is_available():\n",
    "    print(\"‚úÖ CUDA is available for GPU acceleration\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CUDA is not available, using CPU (this will be slow)\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"üîß Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Path Configuration\n",
    "# Set up project paths - This cell finds the project root and initializes path configuration\n",
    "\n",
    "# First, try to find the project root directory\n",
    "notebook_path = Path().resolve()\n",
    "project_root = None\n",
    "\n",
    "# Look for project root markers (traversing upward from the notebook)\n",
    "for parent in [notebook_path] + list(notebook_path.parents):\n",
    "    if any((parent / marker).exists() for marker in ['.git', 'setup.py', 'requirements.txt']):\n",
    "        project_root = parent\n",
    "        break\n",
    "\n",
    "if project_root is None:\n",
    "    # If markers not found, assume we're in a subdirectory of the project\n",
    "    # and the parent directory is the project root\n",
    "    project_root = notebook_path.parent\n",
    "    print(\"‚ö†Ô∏è Could not definitively locate project root, using parent directory\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found project root: {project_root}\")\n",
    "\n",
    "# Add project root to Python path if not already there\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    print(f\"üìå Added {project_root} to Python path\")\n",
    "\n",
    "# Set environment variable for project root\n",
    "os.environ['PROJECT_ROOT'] = str(project_root)\n",
    "print(f\"üîÑ Set PROJECT_ROOT environment variable to {project_root}\")\n",
    "\n",
    "# Verify critical directories exist, create if needed\n",
    "critical_dirs = [\n",
    "    \"configs/environments\",\n",
    "    \"configs/prompts\",\n",
    "    \"src/config\",\n",
    "    \"src/models\",\n",
    "    \"src/prompts\",\n",
    "    \"src/execution\",\n",
    "    \"src/results\",\n",
    "    \"src/analysis\",\n",
    "    \"data/images\",\n",
    "    \"models/cache\",\n",
    "    \"results/raw\",\n",
    "    \"results/processed\",\n",
    "    \"results/visualizations\",\n",
    "    \"logs\"\n",
    "]\n",
    "\n",
    "for dir_path in critical_dirs:\n",
    "    full_path = project_root / dir_path\n",
    "    if not full_path.exists():\n",
    "        print(f\"üìÅ Creating directory: {dir_path}\")\n",
    "        full_path.mkdir(parents=True, exist_ok=True)\n",
    "    else:\n",
    "        print(f\"‚úÖ Directory exists: {dir_path}\")\n",
    "\n",
    "# Check for ground truth data file\n",
    "ground_truth_path = project_root / \"data\" / \"ground_truth.csv\"\n",
    "if os.path.exists(ground_truth_path):\n",
    "    print(f\"‚úÖ Ground truth data file found\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Ground truth data file not found at: {ground_truth_path}\")\n",
    "    print(\"   This will be needed for comparing extraction results.\")\n",
    "\n",
    "# Check for image files\n",
    "image_dir = project_root / \"data\" / \"images\"\n",
    "image_files = list(image_dir.glob(\"*.jpg\")) + list(image_dir.glob(\"*.png\"))\n",
    "if image_files:\n",
    "    print(f\"‚úÖ Found {len(image_files)} image files in {image_dir}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No image files found in {image_dir}\")\n",
    "    print(\"   This will be needed for extraction testing.\")\n",
    "\n",
    "# Set up results directory for this run with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_name = f\"pixtral_extraction_{timestamp}\"\n",
    "results_dir = project_root / \"results\" / experiment_name\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"üìä Created results directory for this run: {results_dir}\")\n",
    "\n",
    "# Create subdirectories in results directory\n",
    "for subdir in [\"raw\", \"processed\", \"visualizations\"]:\n",
    "    (results_dir / subdir).mkdir(exist_ok=True)\n",
    "\n",
    "# Define path utility functions\n",
    "def get_results_path(filename):\n",
    "    \"\"\"Get full path for a results file\"\"\"\n",
    "    return results_dir / filename\n",
    "\n",
    "def get_raw_path(filename):\n",
    "    \"\"\"Get full path for a raw results file\"\"\"\n",
    "    return results_dir / \"raw\" / filename\n",
    "\n",
    "def get_processed_path(filename):\n",
    "    \"\"\"Get full path for a processed results file\"\"\"\n",
    "    return results_dir / \"processed\" / filename\n",
    "\n",
    "def get_visualization_path(filename):\n",
    "    \"\"\"Get full path for a visualization file\"\"\"\n",
    "    return results_dir / \"visualizations\" / filename\n",
    "\n",
    "def get_image_paths():\n",
    "    \"\"\"Get all image paths from the images directory\"\"\"\n",
    "    return list(image_dir.glob(\"*.jpg\")) + list(image_dir.glob(\"*.png\"))\n",
    "\n",
    "# Define paths dictionary for easy access\n",
    "paths = {\n",
    "    \"project_root\": project_root,\n",
    "    \"data_dir\": project_root / \"data\",\n",
    "    \"images_dir\": image_dir,\n",
    "    \"ground_truth_path\": ground_truth_path,\n",
    "    \"models_cache_dir\": project_root / \"models\" / \"cache\",\n",
    "    \"results_dir\": results_dir,\n",
    "    \"raw_results_dir\": results_dir / \"raw\",\n",
    "    \"processed_results_dir\": results_dir / \"processed\",\n",
    "    \"visualizations_dir\": results_dir / \"visualizations\",\n",
    "    \"get_results_path\": get_results_path,\n",
    "    \"get_raw_path\": get_raw_path,\n",
    "    \"get_processed_path\": get_processed_path,\n",
    "    \"get_visualization_path\": get_visualization_path,\n",
    "    \"get_image_paths\": get_image_paths\n",
    "}\n",
    "\n",
    "print(f\"\\nüìã Path configuration complete!\")\n",
    "print(f\"   Project root: {paths['project_root']}\")\n",
    "print(f\"   Data directory: {paths['data_dir']}\")\n",
    "print(f\"   Results for this run: {paths['results_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Configuration\n",
    "### Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Environment Configuration\n",
    "\n",
    "# Import configuration modules if not already imported\n",
    "try:\n",
    "    from src.config.environment import get_environment_config, EnvironmentConfig\n",
    "except ImportError:\n",
    "    # Fallback implementation if modules aren't available yet\n",
    "    logger.warning(\"Using fallback environment configuration\")\n",
    "    \n",
    "    class EnvironmentConfig:\n",
    "        \"\"\"Fallback Environment Configuration class\"\"\"\n",
    "        def __init__(self, env_type=\"auto\"):\n",
    "            self.environment = \"runpod\" if is_runpod() else \"local\"\n",
    "            self.gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n",
    "            self.max_batch_size = 1 if self.environment == \"local\" else 4\n",
    "            self.optimize_memory = True\n",
    "            self.use_4bit_quantization = self.environment == \"runpod\" and self.gpu_memory_gb < 40\n",
    "            self.use_half_precision = True\n",
    "            \n",
    "        def print_summary(self):\n",
    "            \"\"\"Print a summary of the environment configuration\"\"\"\n",
    "            print(f\"Environment: {self.environment}\")\n",
    "            print(f\"GPU Memory: {self.gpu_memory_gb:.2f} GB\")\n",
    "            print(f\"Max Batch Size: {self.max_batch_size}\")\n",
    "            print(f\"Memory Optimization: {'Enabled' if self.optimize_memory else 'Disabled'}\")\n",
    "            print(f\"4-bit Quantization: {'Enabled' if self.use_4bit_quantization else 'Disabled'}\")\n",
    "            print(f\"Half Precision: {'Enabled' if self.use_half_precision else 'Disabled'}\")\n",
    "    \n",
    "    def get_environment_config(env_type=\"auto\"):\n",
    "        \"\"\"Fallback function to get environment configuration\"\"\"\n",
    "        return EnvironmentConfig(env_type)\n",
    "\n",
    "# Detect whether we're in a RunPod or local environment\n",
    "env_type = \"runpod\" if is_runpod() else \"local\"\n",
    "print(f\"üîç Detected environment type: {env_type}\")\n",
    "\n",
    "# Load appropriate configuration\n",
    "try:\n",
    "    # Get environment configuration\n",
    "    env_config = get_environment_config(env_type)\n",
    "    print(f\"üìå Loaded {env_config.environment} environment configuration\")\n",
    "    \n",
    "    # Load environment-specific settings from YAML if available\n",
    "    yaml_config_path = project_root / \"configs\" / \"environments\" / f\"{env_config.environment}.yaml\"\n",
    "    if os.path.exists(yaml_config_path):\n",
    "        import yaml\n",
    "        with open(yaml_config_path, 'r') as f:\n",
    "            yaml_config = yaml.safe_load(f)\n",
    "        print(f\"üìÑ Loaded environment settings from {yaml_config_path}\")\n",
    "        \n",
    "        # Override config settings with YAML values if present\n",
    "        for key, value in yaml_config.items():\n",
    "            if hasattr(env_config, key):\n",
    "                setattr(env_config, key, value)\n",
    "                print(f\"  ‚Ü≥ Set {key} = {value}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è No environment config file found at {yaml_config_path}, using defaults\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading environment configuration: {e}\")\n",
    "    print(\"‚ö†Ô∏è Using default environment settings\")\n",
    "    env_config = EnvironmentConfig(env_type)\n",
    "\n",
    "# Set up GPU memory optimization based on environment config\n",
    "if torch.cuda.is_available() and hasattr(env_config, 'optimize_memory') and env_config.optimize_memory:\n",
    "    # Set up memory optimization for better GPU usage\n",
    "    if hasattr(torch.cuda, 'empty_cache'):\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"üßπ Cleared CUDA cache\")\n",
    "    \n",
    "    # Set memory configuration based on environment\n",
    "    if env_config.environment == \"runpod\":\n",
    "        # More aggressive optimization for RunPod\n",
    "        torch.cuda.set_per_process_memory_fraction(0.95)  # Use up to 95% of GPU memory\n",
    "        print(\"‚öôÔ∏è Set GPU memory fraction to 95% for RunPod environment\")\n",
    "    else:\n",
    "        # More conservative for local environment\n",
    "        torch.cuda.set_per_process_memory_fraction(0.8)  # Use up to 80% of GPU memory\n",
    "        print(\"‚öôÔ∏è Set GPU memory fraction to 80% for local environment\")\n",
    "\n",
    "# Initialize experiment configuration\n",
    "experiment_name = f\"pixtral_extraction_{timestamp}\"\n",
    "print(f\"üß™ Initializing experiment: {experiment_name}\")\n",
    "\n",
    "# Create experiment configuration dictionary\n",
    "experiment_config = {\n",
    "    \"name\": experiment_name,\n",
    "    \"timestamp\": timestamp,\n",
    "    \"environment\": env_config.environment,\n",
    "    \"model_name\": \"pixtral-12b\",  # Default model\n",
    "    \"model_repo_id\": \"mistral-community/pixtral-12b\",  # Default repo\n",
    "    \"field_to_extract\": \"work_order\",  # What we're extracting from invoices\n",
    "    \"use_4bit_quantization\": env_config.use_4bit_quantization if hasattr(env_config, 'use_4bit_quantization') else False,\n",
    "    \"use_half_precision\": env_config.use_half_precision if hasattr(env_config, 'use_half_precision') else True,\n",
    "    \"max_batch_size\": env_config.max_batch_size if hasattr(env_config, 'max_batch_size') else 1,\n",
    "    \"description\": \"Extracting work order numbers from invoice images using Pixtral-12B\"\n",
    "}\n",
    "\n",
    "# Display environment and experiment configuration summary\n",
    "print(\"\\nüìã Environment Configuration Summary:\")\n",
    "env_config.print_summary()\n",
    "\n",
    "print(\"\\nüß™ Experiment Configuration Summary:\")\n",
    "for key, value in experiment_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Save configurations for reference\n",
    "config_path = get_results_path(\"experiment_config.json\")\n",
    "with open(config_path, \"w\") as f:\n",
    "    # Create serializable config by converting Path objects to strings\n",
    "    serializable_config = {\n",
    "        \"experiment\": experiment_config,\n",
    "        \"environment\": {k: str(v) if isinstance(v, Path) else v \n",
    "                        for k, v in vars(env_config).items() \n",
    "                        if not k.startswith('_') and not callable(v)},\n",
    "        \"paths\": {k: str(v) if isinstance(v, Path) else v \n",
    "                 for k, v in paths.items() \n",
    "                 if not callable(v)}\n",
    "    }\n",
    "    json.dump(serializable_config, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Configuration saved to: {config_path}\")\n",
    "print(\"‚úÖ Environment and experiment configuration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Experiment Parameters\n",
    "\n",
    "# Import experiment configuration utilities if available\n",
    "try:\n",
    "    from src.config.experiment import get_experiment_config, ExperimentConfig\n",
    "except ImportError:\n",
    "    logger.warning(\"Using fallback experiment configuration\")\n",
    "    # Continue with manual configuration\n",
    "\n",
    "# Define experiment details\n",
    "print(\"üß™ Setting up experiment parameters\")\n",
    "\n",
    "# Define experiment name and basic metadata\n",
    "experiment_details = {\n",
    "    \"name\": experiment_name,\n",
    "    \"timestamp\": timestamp,\n",
    "    \"description\": \"Extraction of work order numbers from invoice images using Pixtral-12B model\",\n",
    "    \"version\": \"1.0.0\",\n",
    "    \"owner\": os.environ.get(\"USER\", \"unknown\"),\n",
    "}\n",
    "\n",
    "# Model configuration settings\n",
    "model_config = {\n",
    "    \"name\": \"pixtral-12b\",\n",
    "    \"repo_id\": \"mistral-community/pixtral-12b\",\n",
    "    \"revision\": None,  # Use latest version\n",
    "    \"dtype\": \"bfloat16\" if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else \"float16\",\n",
    "    \"device_map\": \"auto\",  # Let the library decide the optimal device mapping\n",
    "    \"use_quantization\": env_config.use_4bit_quantization if hasattr(env_config, \"use_4bit_quantization\") else False,\n",
    "    \"quantization_bits\": 4 if hasattr(env_config, \"use_4bit_quantization\") and env_config.use_4bit_quantization else None,\n",
    "    \"cache_dir\": str(paths[\"models_cache_dir\"]),\n",
    "    \"trust_remote_code\": True,  # Needed for some models\n",
    "    \"use_flash_attention\": torch.cuda.is_available() and torch.__version__ >= \"2.0.0\",\n",
    "}\n",
    "\n",
    "# Extraction task parameters\n",
    "extraction_config = {\n",
    "    \"field_to_extract\": \"work_order\",  # What we're extracting from invoices\n",
    "    \"field_column_name\": \"Work Order Number/Numero de Orden\",  # Column name in ground truth CSV\n",
    "    \"prompt_category\": \"basic\",  # Starting with basic prompt type\n",
    "    \"prompt_name\": \"basic_work_order_extraction\",  # Will be used to look up prompt from registry\n",
    "    \"formatted_prompt\": \"<s>[INST]Extract the work order number from this invoice image.\\n[IMG][/INST]\",  # Known working format\n",
    "}\n",
    "\n",
    "# Evaluation metrics configuration\n",
    "metrics_config = {\n",
    "    \"primary_metric\": \"exact_match\",\n",
    "    \"secondary_metrics\": [\"character_error_rate\", \"levenshtein_distance\", \"processing_time\"],\n",
    "    \"exact_match_case_sensitive\": False,  # Ignore case when comparing\n",
    "    \"exact_match_strip_whitespace\": True,  # Ignore whitespace when comparing\n",
    "    \"cer_normalize\": True,  # Normalize character error rate by length\n",
    "}\n",
    "\n",
    "# Pipeline execution settings\n",
    "execution_config = {\n",
    "    \"max_batch_size\": env_config.max_batch_size if hasattr(env_config, \"max_batch_size\") else 1,\n",
    "    \"save_checkpoints\": True,\n",
    "    \"checkpoint_frequency\": 5,  # Save after every 5 images\n",
    "    \"resume_from_checkpoint\": True,\n",
    "    \"timeout_per_image\": 60,  # Maximum seconds per image\n",
    "    \"max_retries\": 3,  # Retry failed images\n",
    "    \"log_gpu_memory\": True,\n",
    "}\n",
    "\n",
    "# Combine all configurations into master experiment config\n",
    "full_experiment_config = {\n",
    "    \"experiment\": experiment_details,\n",
    "    \"model\": model_config,\n",
    "    \"extraction\": extraction_config,\n",
    "    \"metrics\": metrics_config,\n",
    "    \"execution\": execution_config,\n",
    "    \"environment\": env_config.environment,\n",
    "}\n",
    "\n",
    "# Display summary of experiment configuration\n",
    "print(\"\\nüìã Experiment Details:\")\n",
    "print(f\"   Name: {experiment_details['name']}\")\n",
    "print(f\"   Description: {experiment_details['description']}\")\n",
    "print(f\"   Timestamp: {experiment_details['timestamp']}\")\n",
    "\n",
    "print(\"\\nü§ñ Model Configuration:\")\n",
    "print(f\"   Model: {model_config['name']}\")\n",
    "print(f\"   Repository: {model_config['repo_id']}\")\n",
    "print(f\"   Data Type: {model_config['dtype']}\")\n",
    "print(f\"   Quantization: {'Enabled (' + str(model_config['quantization_bits']) + '-bit)' if model_config['use_quantization'] else 'Disabled'}\")\n",
    "print(f\"   Flash Attention: {'Enabled' if model_config['use_flash_attention'] else 'Disabled'}\")\n",
    "\n",
    "print(\"\\nüîç Extraction Task:\")\n",
    "print(f\"   Field to Extract: {extraction_config['field_to_extract']}\")\n",
    "print(f\"   Ground Truth Column: {extraction_config['field_column_name']}\")\n",
    "print(f\"   Prompt Category: {extraction_config['prompt_category']}\")\n",
    "\n",
    "print(\"\\nüìä Evaluation Metrics:\")\n",
    "print(f\"   Primary Metric: {metrics_config['primary_metric']}\")\n",
    "print(f\"   Secondary Metrics: {', '.join(metrics_config['secondary_metrics'])}\")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Execution Settings:\")\n",
    "print(f\"   Max Batch Size: {execution_config['max_batch_size']}\")\n",
    "print(f\"   Checkpointing: {'Enabled' if execution_config['save_checkpoints'] else 'Disabled'}\")\n",
    "if execution_config['save_checkpoints']:\n",
    "    print(f\"   Checkpoint Frequency: Every {execution_config['checkpoint_frequency']} images\")\n",
    "\n",
    "# Save full experiment configuration to file\n",
    "exp_config_path = get_results_path(\"full_experiment_config.json\")\n",
    "with open(exp_config_path, \"w\") as f:\n",
    "    json.dump(full_experiment_config, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Full experiment configuration saved to: {exp_config_path}\")\n",
    "print(\"‚úÖ Experiment parameters configured successfully!\")\n",
    "\n",
    "# Make config available for other cells\n",
    "experiment_config = full_experiment_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Prompt Selection\n",
    "\n",
    "# Import the prompt management system\n",
    "try:\n",
    "    from src.prompts.registry import get_prompt, list_available_prompts\n",
    "    from src.prompts.prompt_utils import analyze_prompt, format_prompt_for_model\n",
    "    print(\"‚úÖ Imported prompt management system\")\n",
    "    prompt_system_available = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Prompt management system not available, using fallback implementation\")\n",
    "    prompt_system_available = False\n",
    "    \n",
    "    # Define fallback prompt utilities if imports fail\n",
    "    def list_available_prompts():\n",
    "        \"\"\"Fallback function to list available prompts\"\"\"\n",
    "        return {\n",
    "            \"basic_work_order\": {\n",
    "                \"text\": \"Extract the work order number from this invoice image.\",\n",
    "                \"category\": \"basic\",\n",
    "                \"field\": \"work_order\"\n",
    "            },\n",
    "            \"detailed_work_order\": {\n",
    "                \"text\": \"Please identify and extract the work order number from this invoice image. The work order number is usually labeled as 'Work Order', 'Order Number', or 'Numero de Orden'.\",\n",
    "                \"category\": \"detailed\",\n",
    "                \"field\": \"work_order\"\n",
    "            },\n",
    "            \"positioned_work_order\": {\n",
    "                \"text\": \"Extract the work order number from this invoice image. It's typically located in the top section of the invoice near other reference numbers.\",\n",
    "                \"category\": \"positioned\",\n",
    "                \"field\": \"work_order\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    class PromptTemplate:\n",
    "        \"\"\"Fallback prompt template class\"\"\"\n",
    "        def __init__(self, name, text, category, field):\n",
    "            self.name = name\n",
    "            self.text = text\n",
    "            self.category = category\n",
    "            self.field_to_extract = field\n",
    "    \n",
    "    def get_prompt(prompt_name):\n",
    "        \"\"\"Fallback function to get a prompt by name\"\"\"\n",
    "        prompts = list_available_prompts()\n",
    "        if prompt_name in prompts:\n",
    "            prompt_data = prompts[prompt_name]\n",
    "            return PromptTemplate(\n",
    "                prompt_name, \n",
    "                prompt_data[\"text\"], \n",
    "                prompt_data[\"category\"], \n",
    "                prompt_data[\"field\"]\n",
    "            )\n",
    "        return None\n",
    "    \n",
    "    def analyze_prompt(prompt):\n",
    "        \"\"\"Fallback function to analyze a prompt\"\"\"\n",
    "        return {\n",
    "            \"word_count\": len(prompt.text.split()),\n",
    "            \"character_count\": len(prompt.text),\n",
    "            \"complexity_score\": len(prompt.text.split()) / 10,  # Simple complexity metric\n",
    "            \"is_interrogative\": any(prompt.text.startswith(w) for w in [\"what\", \"where\", \"when\", \"how\", \"extract\"]),\n",
    "            \"has_context\": \"invoice\" in prompt.text.lower()\n",
    "        }\n",
    "    \n",
    "    def format_prompt_for_model(prompt, model_name):\n",
    "        \"\"\"Fallback function to format a prompt for specific models\"\"\"\n",
    "        base_text = prompt.text\n",
    "        \n",
    "        # Known working format for Pixtral\n",
    "        if \"pixtral\" in model_name.lower():\n",
    "            return f\"<s>[INST]Extract the work order number from this invoice image.\\n[IMG][/INST]\"\n",
    "        \n",
    "        # Generic default\n",
    "        return f\"Extract the work order number from this invoice image.\"\n",
    "\n",
    "# Define the specific prompt that worked well on RunPod\n",
    "successful_prompt_name = extraction_config[\"prompt_name\"]\n",
    "print(f\"üîç Looking for prompt: {successful_prompt_name}\")\n",
    "\n",
    "# Get available prompts\n",
    "try:\n",
    "    available_prompts = list_available_prompts()\n",
    "    print(f\"üìã Found {len(available_prompts)} available prompts\")\n",
    "    \n",
    "    # Display list of available prompts\n",
    "    print(\"\\nAvailable prompts:\")\n",
    "    for i, (name, details) in enumerate(available_prompts.items(), 1):\n",
    "        if isinstance(details, dict):\n",
    "            # For fallback implementation\n",
    "            print(f\"  {i}. {name} ({details.get('category', 'unknown')}) - {details.get('field', 'unknown')}\")\n",
    "        else:\n",
    "            # For actual implementation (if details is an object)\n",
    "            print(f\"  {i}. {name} ({getattr(details, 'category', 'unknown')}) - {getattr(details, 'field_to_extract', 'unknown')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error listing available prompts: {e}\")\n",
    "    available_prompts = {}\n",
    "\n",
    "# Attempt to get the successful prompt from the registry\n",
    "try:\n",
    "    prompt = get_prompt(successful_prompt_name)\n",
    "    \n",
    "    if prompt is None:\n",
    "        print(f\"‚ö†Ô∏è Prompt '{successful_prompt_name}' not found in registry\")\n",
    "        # Create a default prompt if not found\n",
    "        default_text = extraction_config[\"formatted_prompt\"].replace(\"<s>[INST]\", \"\").replace(\"\\n[IMG][/INST]\", \"\")\n",
    "        prompt = PromptTemplate(\n",
    "            successful_prompt_name,\n",
    "            default_text,\n",
    "            extraction_config[\"prompt_category\"],\n",
    "            extraction_config[\"field_to_extract\"]\n",
    "        )\n",
    "        print(f\"‚ÑπÔ∏è Created default prompt: '{prompt.text}'\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Found prompt: '{prompt.name}'\")\n",
    "    \n",
    "    # Display prompt details\n",
    "    print(f\"\\nüìù Prompt: {prompt.name}\")\n",
    "    print(f\"   Category: {prompt.category}\")\n",
    "    print(f\"   Field: {prompt.field_to_extract}\")\n",
    "    print(f\"   Text: \\\"{prompt.text}\\\"\")\n",
    "    \n",
    "    # Format the prompt for the specific model\n",
    "    model_name = model_config[\"name\"]\n",
    "    if prompt_system_available:\n",
    "        formatted_prompt = format_prompt_for_model(prompt, model_name)\n",
    "    else:\n",
    "        # Use the known working format from the experiment config\n",
    "        formatted_prompt = extraction_config[\"formatted_prompt\"]\n",
    "    \n",
    "    print(f\"\\nüîÑ Formatted for {model_name}:\")\n",
    "    print(f\"   \\\"{formatted_prompt}\\\"\")\n",
    "    \n",
    "    # Analyze the prompt\n",
    "    prompt_analysis = analyze_prompt(prompt)\n",
    "    \n",
    "    print(f\"\\nüîç Prompt Analysis:\")\n",
    "    print(f\"   Word count: {prompt_analysis['word_count']}\")\n",
    "    print(f\"   Character count: {prompt_analysis['character_count']}\")\n",
    "    print(f\"   Complexity score: {prompt_analysis.get('complexity_score', 'N/A')}\")\n",
    "    print(f\"   Is interrogative: {prompt_analysis.get('is_interrogative', 'N/A')}\")\n",
    "    print(f\"   Has context: {prompt_analysis.get('has_context', 'N/A')}\")\n",
    "    \n",
    "    # Update extraction config with the formatted prompt\n",
    "    extraction_config[\"formatted_prompt\"] = formatted_prompt\n",
    "    \n",
    "    # Save prompt information for later use\n",
    "    prompt_info = {\n",
    "        \"name\": prompt.name,\n",
    "        \"text\": prompt.text,\n",
    "        \"formatted_text\": formatted_prompt,\n",
    "        \"category\": prompt.category,\n",
    "        \"field_to_extract\": prompt.field_to_extract,\n",
    "        \"analysis\": prompt_analysis\n",
    "    }\n",
    "    \n",
    "    # Save prompt info to a file\n",
    "    prompt_info_path = get_results_path(\"prompt_info.json\")\n",
    "    with open(prompt_info_path, \"w\") as f:\n",
    "        json.dump(prompt_info, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Prompt information saved to: {prompt_info_path}\")\n",
    "    print(\"‚úÖ Prompt selection complete!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during prompt selection: {e}\")\n",
    "    print(\"‚ö†Ô∏è Will use default formatted prompt from experiment configuration\")\n",
    "    prompt_info = {\n",
    "        \"name\": \"default_prompt\",\n",
    "        \"text\": extraction_config[\"formatted_prompt\"],\n",
    "        \"formatted_text\": extraction_config[\"formatted_prompt\"],\n",
    "        \"category\": extraction_config[\"prompt_category\"],\n",
    "        \"field_to_extract\": extraction_config[\"field_to_extract\"],\n",
    "        \"analysis\": {\"word_count\": len(extraction_config[\"formatted_prompt\"].split())}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "### Load Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Ground Truth Data Loading\n",
    "\n",
    "# Import data handling utilities\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Try to import project-specific data utilities\n",
    "try:\n",
    "    from src.data.loader import load_ground_truth, map_images_to_ground_truth\n",
    "    from src.execution.batch import prepare_batch_items\n",
    "    print(\"‚úÖ Imported data handling utilities\")\n",
    "    use_project_utils = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Project data utilities not available, using direct implementation\")\n",
    "    use_project_utils = False\n",
    "\n",
    "# Load ground truth CSV data\n",
    "print(\"\\nüìä Loading ground truth data...\")\n",
    "\n",
    "ground_truth_path = paths[\"ground_truth_path\"]\n",
    "if not os.path.exists(ground_truth_path):\n",
    "    print(f\"‚ùå Ground truth file not found at: {ground_truth_path}\")\n",
    "    raise FileNotFoundError(f\"Ground truth file not found: {ground_truth_path}\")\n",
    "\n",
    "try:\n",
    "    # Load the CSV file\n",
    "    ground_truth_df = pd.read_csv(ground_truth_path)\n",
    "    print(f\"‚úÖ Loaded ground truth data: {len(ground_truth_df)} records\")\n",
    "    \n",
    "    # Display basic information\n",
    "    print(f\"\\nFirst few rows of ground truth data:\")\n",
    "    display(ground_truth_df.head())\n",
    "    \n",
    "    # Display column information\n",
    "    print(f\"\\nColumns in ground truth data:\")\n",
    "    for i, col in enumerate(ground_truth_df.columns):\n",
    "        print(f\"  {i+1}. {col}\")\n",
    "    \n",
    "    # Check for any missing values in key fields\n",
    "    field_column = extraction_config[\"field_column_name\"]\n",
    "    if field_column not in ground_truth_df.columns:\n",
    "        print(f\"‚ùå Field column '{field_column}' not found in ground truth data\")\n",
    "        print(f\"   Available columns: {', '.join(ground_truth_df.columns)}\")\n",
    "        raise ValueError(f\"Field column '{field_column}' not found in ground truth data\")\n",
    "    \n",
    "    missing_values = ground_truth_df[field_column].isna().sum()\n",
    "    if missing_values > 0:\n",
    "        print(f\"‚ö†Ô∏è Found {missing_values} missing values in {field_column} field\")\n",
    "    \n",
    "    # Map images to ground truth values\n",
    "    print(\"\\nüîÑ Mapping images to ground truth values...\")\n",
    "    \n",
    "    # Get image files\n",
    "    image_dir = paths[\"images_dir\"]\n",
    "    image_files = list(image_dir.glob(\"*.jpg\")) + list(image_dir.glob(\"*.png\"))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"‚ùå No image files found in {image_dir}\")\n",
    "        raise FileNotFoundError(f\"No image files found in {image_dir}\")\n",
    "    \n",
    "    print(f\"üì∏ Found {len(image_files)} image files\")\n",
    "    \n",
    "    # Determine the image ID column (typically 'Invoice' or similar)\n",
    "    # This could be configurable in your extraction_config\n",
    "    image_id_column = \"Invoice\"  # Default, adjust if your CSV uses a different column\n",
    "    \n",
    "    if image_id_column not in ground_truth_df.columns:\n",
    "        print(f\"‚ö†Ô∏è Image ID column '{image_id_column}' not found, trying alternatives...\")\n",
    "        # Try to find a suitable column\n",
    "        for candidate in [\"invoice\", \"image\", \"filename\", \"file\", \"id\"]:\n",
    "            matches = [col for col in ground_truth_df.columns if candidate in col.lower()]\n",
    "            if matches:\n",
    "                image_id_column = matches[0]\n",
    "                print(f\"‚úÖ Using '{image_id_column}' as image ID column\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"‚ùå Could not find a suitable image ID column\")\n",
    "            print(f\"   Please specify the column that contains image identifiers\")\n",
    "            raise ValueError(\"Image ID column not found\")\n",
    "    \n",
    "    # Create mapping between images and ground truth\n",
    "    ground_truth_mapping = {}\n",
    "    unmatched_images = []\n",
    "    \n",
    "    for image_path in image_files:\n",
    "        image_id = image_path.stem  # Get filename without extension\n",
    "        \n",
    "        # Find matching row in ground truth\n",
    "        # Converting to string to ensure matching works with numeric IDs\n",
    "        matching_row = ground_truth_df[ground_truth_df[image_id_column].astype(str) == image_id]\n",
    "        \n",
    "        if len(matching_row) > 0:\n",
    "            row = matching_row.iloc[0]\n",
    "            ground_truth_mapping[image_id] = {\n",
    "                'image_path': str(image_path),\n",
    "                'work_order': str(row[field_column]).strip(),\n",
    "                'image_id': image_id\n",
    "            }\n",
    "        else:\n",
    "            unmatched_images.append(image_id)\n",
    "            print(f\"‚ö†Ô∏è No matching ground truth found for image {image_id}\")\n",
    "    \n",
    "    print(f\"‚úÖ Successfully mapped {len(ground_truth_mapping)} images to ground truth data\")\n",
    "    \n",
    "    if unmatched_images:\n",
    "        print(f\"‚ö†Ô∏è Found {len(unmatched_images)} images without ground truth data\")\n",
    "        if len(unmatched_images) < 10:\n",
    "            print(f\"   Unmatched images: {', '.join(unmatched_images)}\")\n",
    "        else:\n",
    "            print(f\"   First 10 unmatched images: {', '.join(unmatched_images[:10])}...\")\n",
    "    \n",
    "    # Create structured batch items for processing\n",
    "    print(\"\\nüîÑ Creating structured batch items for processing...\")\n",
    "    \n",
    "    batch_items = []\n",
    "    for image_id, data in ground_truth_mapping.items():\n",
    "        batch_items.append({\n",
    "            \"image_id\": image_id,\n",
    "            \"image_path\": data[\"image_path\"],\n",
    "            \"ground_truth\": data[\"work_order\"],\n",
    "            \"field_type\": extraction_config[\"field_to_extract\"]\n",
    "        })\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(batch_items)} batch items\")\n",
    "    \n",
    "    # Display statistics and sample mapping\n",
    "    print(\"\\nüìä Ground Truth Statistics:\")\n",
    "    \n",
    "    # Count unique work order values\n",
    "    unique_work_orders = len(set(data[\"work_order\"] for data in ground_truth_mapping.values()))\n",
    "    print(f\"   Total Images: {len(ground_truth_mapping)}\")\n",
    "    print(f\"   Unique Work Orders: {unique_work_orders}\")\n",
    "    \n",
    "    # Calculate average work order length\n",
    "    work_order_lengths = [len(data[\"work_order\"]) for data in ground_truth_mapping.values()]\n",
    "    avg_length = sum(work_order_lengths) / len(work_order_lengths) if work_order_lengths else 0\n",
    "    print(f\"   Average Work Order Length: {avg_length:.2f} characters\")\n",
    "    print(f\"   Min Length: {min(work_order_lengths) if work_order_lengths else 0} characters\")\n",
    "    print(f\"   Max Length: {max(work_order_lengths) if work_order_lengths else 0} characters\")\n",
    "    \n",
    "    # Display sample of the mapping\n",
    "    print(\"\\nüìã Sample of Ground Truth Mapping:\")\n",
    "    random_samples = random.sample(list(ground_truth_mapping.items()), min(5, len(ground_truth_mapping)))\n",
    "    for i, (img_id, data) in enumerate(random_samples, 1):\n",
    "        print(f\"   {i}. Image: {img_id}\")\n",
    "        print(f\"      ‚Ü≥ Path: {data['image_path']}\")\n",
    "        print(f\"      ‚Ü≥ Work Order: '{data['work_order']}'\")\n",
    "    \n",
    "    # Save the ground truth mapping for reference\n",
    "    mapping_path = get_results_path(\"ground_truth_mapping.json\")\n",
    "    with open(mapping_path, \"w\") as f:\n",
    "        # Convert to serializable format (paths need to be strings)\n",
    "        serializable_mapping = {\n",
    "            img_id: {k: v for k, v in data.items()}\n",
    "            for img_id, data in ground_truth_mapping.items()\n",
    "        }\n",
    "        json.dump(serializable_mapping, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Ground truth mapping saved to: {mapping_path}\")\n",
    "    print(\"‚úÖ Ground truth data loading complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading ground truth data: {e}\")\n",
    "    ground_truth_df = None\n",
    "    ground_truth_mapping = {}\n",
    "    batch_items = []\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "### GPU Verification & Memory Check / Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: GPU Verification\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Try to import project-specific utilities\n",
    "try:\n",
    "    from src.models.loader import get_gpu_memory_info, verify_gpu_compatibility\n",
    "    print(\"‚úÖ Imported GPU verification utilities\")\n",
    "    use_project_utils = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Project GPU utilities not available, using direct implementation\")\n",
    "    use_project_utils = False\n",
    "    \n",
    "    # Implement fallback functions\n",
    "    def get_gpu_memory_info():\n",
    "        \"\"\"Get GPU memory information\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return {\n",
    "                \"total_memory_gb\": 0,\n",
    "                \"allocated_memory_gb\": 0,\n",
    "                \"free_memory_gb\": 0,\n",
    "                \"utilization\": 0\n",
    "            }\n",
    "        \n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "        allocated_memory = torch.cuda.memory_allocated(0)\n",
    "        reserved_memory = torch.cuda.memory_reserved(0)\n",
    "        free_memory = total_memory - allocated_memory\n",
    "        \n",
    "        # Convert to GB\n",
    "        total_memory_gb = total_memory / 1e9\n",
    "        allocated_memory_gb = allocated_memory / 1e9\n",
    "        free_memory_gb = free_memory / 1e9\n",
    "        utilization = (allocated_memory / total_memory) * 100\n",
    "        \n",
    "        return {\n",
    "            \"total_memory_gb\": total_memory_gb,\n",
    "            \"allocated_memory_gb\": allocated_memory_gb,\n",
    "            \"free_memory_gb\": free_memory_gb,\n",
    "            \"utilization\": utilization\n",
    "        }\n",
    "    \n",
    "    def verify_gpu_compatibility(model_name):\n",
    "        \"\"\"Check if GPU is compatible with model requirements\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return {\n",
    "                \"compatible\": False,\n",
    "                \"reason\": \"No GPU available\",\n",
    "                \"model_requirements\": {\"minimum_memory_gb\": 24},\n",
    "                \"current_gpu\": {\"name\": \"None\", \"memory_gb\": 0}\n",
    "            }\n",
    "        \n",
    "        # Get GPU info\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        total_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        \n",
    "        # Define model requirements\n",
    "        # Pixtral-12B typically needs around 24GB without quantization\n",
    "        model_requirements = {\n",
    "            \"pixtral-12b\": {\"minimum_memory_gb\": 24},\n",
    "            \"pixtral-7b\": {\"minimum_memory_gb\": 16},\n",
    "            \"default\": {\"minimum_memory_gb\": 12}\n",
    "        }\n",
    "        \n",
    "        # Get requirements for this model\n",
    "        req = model_requirements.get(model_name.lower(), model_requirements[\"default\"])\n",
    "        \n",
    "        # Check compatibility\n",
    "        compatible = total_memory_gb >= req[\"minimum_memory_gb\"]\n",
    "        \n",
    "        return {\n",
    "            \"compatible\": compatible,\n",
    "            \"reason\": None if compatible else f\"Insufficient GPU memory: {total_memory_gb:.1f}GB available, {req['minimum_memory_gb']}GB required\",\n",
    "            \"model_requirements\": req,\n",
    "            \"current_gpu\": {\n",
    "                \"name\": gpu_name,\n",
    "                \"memory_gb\": total_memory_gb\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Check for GPU availability\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"‚ùå No GPU detected!\")\n",
    "    print(\"   This model requires a GPU for reasonable processing times.\")\n",
    "    print(\"   If you're on RunPod, verify that the GPU is properly allocated.\")\n",
    "    raise RuntimeError(\"No GPU available for model loading\")\n",
    "\n",
    "# Get model name from experiment configuration\n",
    "model_name = model_config[\"name\"]\n",
    "print(f\"üîç Checking GPU compatibility for {model_name}...\")\n",
    "\n",
    "# Check GPU compatibility\n",
    "compatibility = verify_gpu_compatibility(model_name)\n",
    "\n",
    "if compatibility[\"compatible\"]:\n",
    "    print(f\"‚úÖ GPU is compatible with {model_name}\")\n",
    "    print(f\"   GPU: {compatibility['current_gpu']['name']}\")\n",
    "    print(f\"   Memory: {compatibility['current_gpu']['memory_gb']:.1f} GB\")\n",
    "    print(f\"   Required: {compatibility['model_requirements']['minimum_memory_gb']} GB\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Warning: {compatibility['reason']}\")\n",
    "    print(f\"   Model requirements: {compatibility['model_requirements']['minimum_memory_gb']} GB\")\n",
    "    print(f\"   Available: {compatibility['current_gpu']['memory_gb']:.1f} GB\")\n",
    "    print(f\"   Proceeding with quantization enabled to reduce memory requirements\")\n",
    "    \n",
    "    # Enable quantization if not already enabled\n",
    "    if not model_config[\"use_quantization\"]:\n",
    "        print(\"üîÑ Automatically enabling 4-bit quantization to fit model in memory\")\n",
    "        model_config[\"use_quantization\"] = True\n",
    "        model_config[\"quantization_bits\"] = 4\n",
    "\n",
    "# Display pre-loading memory state\n",
    "print(\"\\nüß† Pre-loading GPU memory state:\")\n",
    "pre_memory = get_gpu_memory_info()\n",
    "print(f\"   Total memory: {pre_memory['total_memory_gb']:.2f} GB\")\n",
    "print(f\"   Allocated memory: {pre_memory['allocated_memory_gb']:.2f} GB\")\n",
    "print(f\"   Free memory: {pre_memory['free_memory_gb']:.2f} GB\")\n",
    "print(f\"   Utilization: {pre_memory['utilization']:.1f}%\")\n",
    "\n",
    "# Clear memory before loading\n",
    "print(\"\\nüßπ Cleaning up GPU memory before model loading...\")\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# Verify memory after cleanup\n",
    "post_cleanup_memory = get_gpu_memory_info()\n",
    "print(f\"   Memory after cleanup: {post_cleanup_memory['allocated_memory_gb']:.2f} GB allocated\")\n",
    "print(f\"   Free memory: {post_cleanup_memory['free_memory_gb']:.2f} GB\")\n",
    "\n",
    "# Configure model loading parameters based on compatibility check\n",
    "print(\"\\n‚öôÔ∏è Configuring model loading parameters...\")\n",
    "\n",
    "# Set up quantization configuration if enabled\n",
    "quantization_config = None\n",
    "if model_config[\"use_quantization\"]:\n",
    "    try:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        \n",
    "        bits = model_config[\"quantization_bits\"]\n",
    "        print(f\"   Using {bits}-bit quantization\")\n",
    "        \n",
    "        if bits == 4:\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "            print(\"   Configured 4-bit NormalFloat quantization\")\n",
    "        elif bits == 8:\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_8bit=True\n",
    "            )\n",
    "            print(\"   Configured 8-bit quantization\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è BitsAndBytesConfig not available, quantization will be limited\")\n",
    "        quantization_config = {\"load_in_4bit\": True}\n",
    "\n",
    "# Determine precision (dtype)\n",
    "if torch.cuda.is_available():\n",
    "    if torch.cuda.get_device_capability()[0] >= 8:  # Ampere or newer\n",
    "        dtype = torch.bfloat16\n",
    "        print(\"   Using bfloat16 precision (optimal for Ampere+ GPUs)\")\n",
    "    else:\n",
    "        dtype = torch.float16\n",
    "        print(\"   Using float16 precision (for older GPUs)\")\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "    print(\"   Using float32 precision (CPU mode)\")\n",
    "\n",
    "# Set up model loading configuration\n",
    "model_loading_config = {\n",
    "    \"model_name\": model_config[\"repo_id\"],\n",
    "    \"revision\": model_config[\"revision\"],\n",
    "    \"device_map\": model_config[\"device_map\"],\n",
    "    \"torch_dtype\": dtype,\n",
    "    \"quantization_config\": quantization_config,\n",
    "    \"cache_dir\": Path(model_config[\"cache_dir\"]),\n",
    "    \"trust_remote_code\": model_config[\"trust_remote_code\"]\n",
    "}\n",
    "\n",
    "# Save pre-loading state for comparison\n",
    "pre_loading_info = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model_name\": model_name,\n",
    "    \"gpu_info\": {\n",
    "        \"name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\",\n",
    "        \"total_memory_gb\": pre_memory[\"total_memory_gb\"],\n",
    "        \"allocated_memory_gb\": pre_memory[\"allocated_memory_gb\"],\n",
    "        \"free_memory_gb\": pre_memory[\"free_memory_gb\"]\n",
    "    },\n",
    "    \"quantization_enabled\": model_config[\"use_quantization\"],\n",
    "    \"quantization_bits\": model_config[\"quantization_bits\"],\n",
    "    \"precision\": str(dtype)\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "pre_loading_path = get_results_path(\"pre_loading_gpu_state.json\")\n",
    "with open(pre_loading_path, \"w\") as f:\n",
    "    # Convert non-serializable types\n",
    "    serializable_info = {**pre_loading_info}\n",
    "    serializable_info[\"precision\"] = str(serializable_info[\"precision\"])\n",
    "    json.dump(serializable_info, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Pre-loading GPU state saved to: {pre_loading_path}\")\n",
    "print(\"‚úÖ GPU verification complete and model loading configured!\")\n",
    "\n",
    "# Make model loading configuration available to next cell\n",
    "model_load_params = model_loading_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Model Loading\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, AutoModelForVision2Seq\n",
    "from datetime import datetime\n",
    "\n",
    "# Try to import project-specific model loading utilities\n",
    "try:\n",
    "    from src.models.loader import load_model_and_processor, get_gpu_memory_info\n",
    "    print(\"‚úÖ Imported model loading utilities\")\n",
    "    use_project_utils = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Project model utilities not available, using direct implementation\")\n",
    "    use_project_utils = False\n",
    "\n",
    "# Record start time for loading\n",
    "start_time = time.time()\n",
    "\n",
    "# Display loading message\n",
    "model_name = model_config[\"name\"]\n",
    "repo_id = model_config[\"repo_id\"]\n",
    "print(f\"‚è≥ Loading {model_name} from {repo_id}...\")\n",
    "print(f\"   This may take several minutes depending on your internet connection and GPU speed.\")\n",
    "\n",
    "try:\n",
    "    # Method 1: Use project utility if available\n",
    "    if use_project_utils:\n",
    "        print(\"Using project model loading utility...\")\n",
    "        model, processor = load_model_and_processor(\n",
    "            model_name=repo_id,\n",
    "            **model_load_params\n",
    "        )\n",
    "    \n",
    "    # Method 2: Direct loading if project utility not available\n",
    "    else:\n",
    "        print(\"Using direct model loading...\")\n",
    "        \n",
    "        # Clear memory before loading\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Load the model with appropriate configuration\n",
    "        print(f\"Loading model: {repo_id}\")\n",
    "        model = LlavaForConditionalGeneration.from_pretrained(\n",
    "            repo_id,\n",
    "            torch_dtype=model_load_params[\"torch_dtype\"],\n",
    "            device_map=model_load_params[\"device_map\"],\n",
    "            quantization_config=model_load_params[\"quantization_config\"],\n",
    "            cache_dir=model_load_params[\"cache_dir\"],\n",
    "            trust_remote_code=model_load_params[\"trust_remote_code\"]\n",
    "        )\n",
    "        \n",
    "        # Load the processor\n",
    "        print(f\"Loading processor for: {repo_id}\")\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            repo_id,\n",
    "            cache_dir=model_load_params[\"cache_dir\"],\n",
    "            trust_remote_code=model_load_params[\"trust_remote_code\"]\n",
    "        )\n",
    "    \n",
    "    # Calculate loading time\n",
    "    loading_time = time.time() - start_time\n",
    "    print(f\"‚úÖ Model loaded successfully in {loading_time:.2f} seconds!\")\n",
    "    \n",
    "    # Get model device information\n",
    "    if hasattr(model, \"device\"):\n",
    "        model_device = model.device\n",
    "    elif hasattr(model, \"hf_device_map\"):\n",
    "        model_device = str(model.hf_device_map)\n",
    "    else:\n",
    "        model_device = \"Unknown\"\n",
    "    \n",
    "    print(f\"üìç Model loaded on device: {model_device}\")\n",
    "    \n",
    "    # Get model dtype information\n",
    "    if hasattr(model, \"dtype\"):\n",
    "        model_dtype = str(model.dtype)\n",
    "    else:\n",
    "        # Try to infer from parameters\n",
    "        for param in model.parameters():\n",
    "            model_dtype = str(param.dtype)\n",
    "            break\n",
    "    \n",
    "    print(f\"üî¢ Model data type: {model_dtype}\")\n",
    "    \n",
    "    # Display post-loading memory state\n",
    "    print(\"\\nüß† Post-loading GPU memory state:\")\n",
    "    post_memory = get_gpu_memory_info()\n",
    "    print(f\"   Total memory: {post_memory['total_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Allocated memory: {post_memory['allocated_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Free memory: {post_memory['free_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Utilization: {post_memory['utilization']:.1f}%\")\n",
    "    \n",
    "    # Calculate memory usage for the model\n",
    "    memory_used = post_memory['allocated_memory_gb'] - pre_memory['allocated_memory_gb']\n",
    "    print(f\"üîç Model memory usage: {memory_used:.2f} GB\")\n",
    "    \n",
    "    # Store model metadata\n",
    "    model_metadata = {\n",
    "        \"name\": model_name,\n",
    "        \"repo_id\": repo_id,\n",
    "        \"loading_time_seconds\": loading_time,\n",
    "        \"device\": str(model_device),\n",
    "        \"dtype\": model_dtype,\n",
    "        \"loaded_at\": datetime.now().isoformat(),\n",
    "        \"memory_usage_gb\": memory_used,\n",
    "        \"quantization\": {\n",
    "            \"enabled\": model_config[\"use_quantization\"],\n",
    "            \"bits\": model_config[\"quantization_bits\"] if model_config[\"use_quantization\"] else None\n",
    "        },\n",
    "        \"gpu_info\": {\n",
    "            \"name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\",\n",
    "            \"pre_loading_memory_gb\": pre_memory[\"allocated_memory_gb\"],\n",
    "            \"post_loading_memory_gb\": post_memory[\"allocated_memory_gb\"],\n",
    "            \"total_memory_gb\": post_memory[\"total_memory_gb\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save model metadata\n",
    "    model_metadata_path = get_results_path(\"model_metadata.json\")\n",
    "    with open(model_metadata_path, \"w\") as f:\n",
    "        json.dump(model_metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Model metadata saved to: {model_metadata_path}\")\n",
    "    \n",
    "    # Verify processor functionality\n",
    "    print(\"\\nüîç Verifying processor functionality...\")\n",
    "    \n",
    "    # Simple test to verify processor\n",
    "    sample_text = \"Extract the work order number from this invoice image.\"\n",
    "    try:\n",
    "        # Just verify it can process a simple example without running inference\n",
    "        sample_input = processor(text=sample_text, return_tensors=\"pt\")\n",
    "        print(\"‚úÖ Processor verified: Can process text inputs\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Processor test failed for text processing: {e}\")\n",
    "    \n",
    "    print(\"‚úÖ Model and processor loaded successfully and ready for inference!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    # Attempt to free memory in case of loading failure\n",
    "    if 'model' in locals():\n",
    "        del model\n",
    "    if 'processor' in locals():\n",
    "        del processor\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    # Raise the exception to halt execution\n",
    "    raise RuntimeError(f\"Failed to load model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Pipeline\n",
    "### Pipeline Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Pipeline Initialization\n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Try to import project-specific pipeline utilities\n",
    "try:\n",
    "    from src.execution.pipeline import ExtractionPipeline\n",
    "    print(\"‚úÖ Imported extraction pipeline\")\n",
    "    use_project_pipeline = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Project pipeline not available, using direct implementation\")\n",
    "    use_project_pipeline = False\n",
    "    \n",
    "    # Define a simple pipeline class if the project one isn't available\n",
    "    class ExtractionPipeline:\n",
    "        \"\"\"Basic extraction pipeline implementation\"\"\"\n",
    "        def __init__(self, experiment_name):\n",
    "            self.experiment_name = experiment_name\n",
    "            self.config = {}\n",
    "            self.model = None\n",
    "            self.processor = None\n",
    "            self.ground_truth_mapping = {}\n",
    "            self.results = []\n",
    "            self.batch_size = 1\n",
    "            self.checkpoints = {}\n",
    "            self.current_batch = 0\n",
    "            \n",
    "        def configure(self, config):\n",
    "            \"\"\"Configure the pipeline with settings\"\"\"\n",
    "            self.config.update(config)\n",
    "            return self\n",
    "            \n",
    "        def setup_model(self, model=None, processor=None):\n",
    "            \"\"\"Set up model and processor\"\"\"\n",
    "            if model is not None:\n",
    "                self.model = model\n",
    "            if processor is not None:\n",
    "                self.processor = processor\n",
    "            return self\n",
    "            \n",
    "        def load_ground_truth(self, ground_truth_mapping):\n",
    "            \"\"\"Load ground truth data\"\"\"\n",
    "            self.ground_truth_mapping = ground_truth_mapping\n",
    "            return self\n",
    "            \n",
    "        def determine_optimal_batch_size(self, start_size=1, max_size=8):\n",
    "            \"\"\"Determine the optimal batch size for the GPU\"\"\"\n",
    "            # In a real implementation, this would test different batch sizes\n",
    "            # For this fallback, we'll just use the config value or default to 1\n",
    "            self.batch_size = self.config.get(\"batch_processing\", {}).get(\"batch_size\", 1)\n",
    "            return self.batch_size\n",
    "            \n",
    "        def run_extraction(self, batch_items, field_type, prompt, checkpoint_path=None, metrics=None, limit=None):\n",
    "            \"\"\"Run extraction on all items\"\"\"\n",
    "            # This is a placeholder - in a real implementation, this would process items\n",
    "            print(f\"Would process {len(batch_items)} items with batch size {self.batch_size}\")\n",
    "            return []\n",
    "            \n",
    "        def save_checkpoint(self, checkpoint_path, batch_idx, processed_items):\n",
    "            \"\"\"Save checkpoint for resumability\"\"\"\n",
    "            checkpoint = {\n",
    "                \"batch_idx\": batch_idx,\n",
    "                \"processed_items\": processed_items,\n",
    "                \"timestamp\": time.time()\n",
    "            }\n",
    "            with open(checkpoint_path, \"w\") as f:\n",
    "                json.dump(checkpoint, f)\n",
    "            return checkpoint\n",
    "            \n",
    "        def load_checkpoint(self, checkpoint_path):\n",
    "            \"\"\"Load checkpoint for resuming\"\"\"\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                with open(checkpoint_path, \"r\") as f:\n",
    "                    return json.load(f)\n",
    "            return None\n",
    "\n",
    "# Initialize pipeline\n",
    "print(\"üöÄ Initializing extraction pipeline...\")\n",
    "experiment_name = experiment_config[\"name\"]\n",
    "pipeline = ExtractionPipeline(experiment_name)\n",
    "\n",
    "# Configure pipeline settings\n",
    "print(\"\\n‚öôÔ∏è Configuring pipeline settings...\")\n",
    "\n",
    "# Set up batch processing configuration\n",
    "batch_config = {\n",
    "    \"auto_batch_size\": True,  # Determine optimal batch size automatically\n",
    "    \"max_batch_size\": 8,      # Maximum batch size to consider\n",
    "    \"default_batch_size\": 1,  # Default if auto-estimation fails\n",
    "    \"optimize_memory\": True   # Clean up memory between batches\n",
    "}\n",
    "\n",
    "# Set up checkpointing configuration\n",
    "checkpoint_config = {\n",
    "    \"enable_checkpoints\": True,\n",
    "    \"checkpoint_frequency\": 5,  # Save checkpoint after every 5 batches\n",
    "    \"checkpoint_path\": str(get_results_path(\"extraction_checkpoint.json\")),\n",
    "    \"resume_from_checkpoint\": True  # Resume from checkpoint if available\n",
    "}\n",
    "\n",
    "# Set up metrics configuration\n",
    "metrics_config = {\n",
    "    \"metrics\": [\"exact_match\", \"character_error_rate\", \"levenshtein_distance\", \"processing_time\"],\n",
    "    \"case_sensitive\": False,\n",
    "    \"strip_whitespace\": True\n",
    "}\n",
    "\n",
    "# Complete pipeline configuration\n",
    "pipeline_config = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"model_name\": model_config[\"name\"],\n",
    "    \"field_to_extract\": extraction_config[\"field_to_extract\"],\n",
    "    \"prompt_name\": prompt_info[\"name\"],\n",
    "    \"prompt_category\": prompt_info[\"category\"],\n",
    "    \"description\": experiment_config[\"description\"],\n",
    "    \"batch_processing\": batch_config,\n",
    "    \"checkpointing\": checkpoint_config,\n",
    "    \"metrics\": metrics_config,\n",
    "    \"environment\": env_config.environment,\n",
    "    \"timestamp\": timestamp\n",
    "}\n",
    "\n",
    "# Apply configuration to pipeline\n",
    "pipeline.configure(pipeline_config)\n",
    "\n",
    "# Set up the model and processor in the pipeline\n",
    "print(\"\\nüîÑ Setting up model and processor in pipeline...\")\n",
    "pipeline.setup_model(model=model, processor=processor)\n",
    "\n",
    "# Load ground truth data\n",
    "print(\"\\nüìä Loading ground truth data into pipeline...\")\n",
    "pipeline.load_ground_truth(ground_truth_mapping)\n",
    "\n",
    "# Determine optimal batch size if auto-batch is enabled\n",
    "if batch_config[\"auto_batch_size\"]:\n",
    "    print(\"\\nüßÆ Determining optimal batch size...\")\n",
    "    try:\n",
    "        # If the pipeline has a method to determine batch size\n",
    "        if hasattr(pipeline, \"determine_optimal_batch_size\"):\n",
    "            optimal_batch_size = pipeline.determine_optimal_batch_size(\n",
    "                start_size=1,\n",
    "                max_size=batch_config[\"max_batch_size\"]\n",
    "            )\n",
    "            print(f\"‚úÖ Determined optimal batch size: {optimal_batch_size}\")\n",
    "        else:\n",
    "            # Fallback to default\n",
    "            optimal_batch_size = batch_config[\"default_batch_size\"]\n",
    "            print(f\"‚ÑπÔ∏è Using default batch size: {optimal_batch_size}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error determining batch size: {e}\")\n",
    "        print(f\"‚ÑπÔ∏è Falling back to default batch size: {batch_config['default_batch_size']}\")\n",
    "        optimal_batch_size = batch_config[\"default_batch_size\"]\n",
    "    \n",
    "    # Update config with determined batch size\n",
    "    pipeline_config[\"batch_processing\"][\"batch_size\"] = optimal_batch_size\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è Using fixed batch size: {batch_config.get('batch_size', batch_config['default_batch_size'])}\")\n",
    "\n",
    "# Check for existing checkpoint\n",
    "checkpoint_path = checkpoint_config[\"checkpoint_path\"]\n",
    "if os.path.exists(checkpoint_path) and checkpoint_config[\"resume_from_checkpoint\"]:\n",
    "    print(f\"\\nüîÑ Found existing checkpoint at: {checkpoint_path}\")\n",
    "    try:\n",
    "        checkpoint = pipeline.load_checkpoint(checkpoint_path)\n",
    "        if checkpoint:\n",
    "            print(f\"‚ÑπÔ∏è Checkpoint from: {datetime.fromtimestamp(checkpoint['timestamp']).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            print(f\"‚ÑπÔ∏è Already processed {len(checkpoint['processed_items'])} items\")\n",
    "            print(f\"‚ÑπÔ∏è Will resume from batch {checkpoint['batch_idx']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading checkpoint: {e}\")\n",
    "        print(\"‚ÑπÔ∏è Will start processing from the beginning\")\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è No existing checkpoint found or resume disabled\")\n",
    "    print(\"‚ÑπÔ∏è Will start processing from the beginning\")\n",
    "\n",
    "# Prepare for extraction\n",
    "print(\"\\nüìã Pipeline initialization summary:\")\n",
    "print(f\"   Experiment: {pipeline_config['experiment_name']}\")\n",
    "print(f\"   Model: {pipeline_config['model_name']}\")\n",
    "print(f\"   Field to extract: {pipeline_config['field_to_extract']}\")\n",
    "print(f\"   Prompt: {pipeline_config['prompt_name']}\")\n",
    "print(f\"   Batch size: {pipeline_config['batch_processing'].get('batch_size', batch_config['default_batch_size'])}\")\n",
    "print(f\"   Checkpointing: {'Enabled' if checkpoint_config['enable_checkpoints'] else 'Disabled'}\")\n",
    "print(f\"   Images to process: {len(ground_truth_mapping)}\")\n",
    "\n",
    "# Save pipeline configuration\n",
    "pipeline_config_path = get_results_path(\"pipeline_config.json\")\n",
    "with open(pipeline_config_path, \"w\") as f:\n",
    "    # Convert any non-serializable objects to strings\n",
    "    serializable_config = json.loads(json.dumps(pipeline_config, default=str))\n",
    "    json.dump(serializable_config, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Pipeline configuration saved to: {pipeline_config_path}\")\n",
    "print(\"‚úÖ Pipeline initialization complete and ready for extraction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Image Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Single Image Test\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import time\n",
    "from PIL import Image\n",
    "from Levenshtein import distance\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Try to import project-specific inference utilities\n",
    "try:\n",
    "    from src.execution.inference import process_image_with_metrics\n",
    "    print(\"‚úÖ Imported inference utilities\")\n",
    "    use_project_utils = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Project inference utilities not available, using direct implementation\")\n",
    "    use_project_utils = False\n",
    "    \n",
    "    # Define a direct implementation for single image processing\n",
    "    def process_image_with_metrics(image_path, ground_truth, prompt, model_name, field_type, model, processor, metrics=None):\n",
    "        \"\"\"Process a single image and calculate metrics against ground truth\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Load the image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "            # Format the prompt with instruction tags\n",
    "            formatted_prompt = prompt[\"formatted_text\"]\n",
    "            \n",
    "            # Process using processor\n",
    "            inputs = processor(\n",
    "                text=formatted_prompt,\n",
    "                images=[image],  # Notice we're passing a list here\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Convert inputs to appropriate dtypes\n",
    "            for key in inputs:\n",
    "                if key == \"pixel_values\":\n",
    "                    inputs[key] = inputs[key].to(dtype=next(model.parameters()).dtype, device=model.device)\n",
    "                else:\n",
    "                    inputs[key] = inputs[key].to(device=model.device)\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_new_tokens=50, do_sample=False)\n",
    "            \n",
    "            # Decode the output\n",
    "            raw_extraction = processor.batch_decode(\n",
    "                outputs, \n",
    "                skip_special_tokens=True, \n",
    "                clean_up_tokenization_spaces=False\n",
    "            )[0]\n",
    "            \n",
    "            # Process the output - clean up whitespace and extract just numbers if needed\n",
    "            processed_extraction = raw_extraction.strip()\n",
    "            \n",
    "            # Calculate processing time\n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            # Calculate metrics\n",
    "            result = {\n",
    "                \"raw_extraction\": raw_extraction,\n",
    "                \"processed_extraction\": processed_extraction,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"processing_time\": processing_time\n",
    "            }\n",
    "            \n",
    "            # Calculate exact match\n",
    "            exact_match = processed_extraction.lower() == ground_truth.lower()\n",
    "            result[\"exact_match\"] = exact_match\n",
    "            \n",
    "            # Calculate character error rate (Levenshtein distance normalized by length)\n",
    "            if len(ground_truth) > 0:\n",
    "                cer = distance(processed_extraction, ground_truth) / len(ground_truth)\n",
    "            else:\n",
    "                cer = 1.0 if len(processed_extraction) > 0 else 0.0\n",
    "            result[\"character_error_rate\"] = cer\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            processing_time = time.time() - start_time\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"processing_time\": processing_time,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"raw_extraction\": \"ERROR\",\n",
    "                \"processed_extraction\": \"ERROR\",\n",
    "                \"exact_match\": False,\n",
    "                \"character_error_rate\": 1.0\n",
    "            }\n",
    "\n",
    "# Verify we have the pipeline, model, and ground truth data\n",
    "if 'pipeline' not in locals():\n",
    "    print(\"‚ùå Pipeline not initialized. Run the pipeline initialization cell first.\")\n",
    "    raise RuntimeError(\"Pipeline not initialized\")\n",
    "    \n",
    "if 'model' not in locals() or model is None:\n",
    "    print(\"‚ùå Model not loaded. Run the model loading cell first.\")\n",
    "    raise RuntimeError(\"Model not loaded\")\n",
    "    \n",
    "if len(ground_truth_mapping) == 0:\n",
    "    print(\"‚ùå No ground truth data available. Run the ground truth data loading cell first.\")\n",
    "    raise RuntimeError(\"No ground truth data available\")\n",
    "\n",
    "print(\"üß™ Running a test on a single image before full pipeline execution...\")\n",
    "\n",
    "# Select a sample image\n",
    "print(\"\\nüîç Selecting a test image...\")\n",
    "try:\n",
    "    # Get all image paths\n",
    "    image_paths = list(Path(paths[\"images_dir\"]).glob(\"*.jpg\")) + list(Path(paths[\"images_dir\"]).glob(\"*.png\"))\n",
    "    \n",
    "    if not image_paths:\n",
    "        raise FileNotFoundError(\"No image files found\")\n",
    "    \n",
    "    # Choose a random image that has ground truth data\n",
    "    valid_images = [img for img in image_paths if img.stem in ground_truth_mapping]\n",
    "    \n",
    "    if not valid_images:\n",
    "        print(\"‚ö†Ô∏è No images with ground truth found, using a random image\")\n",
    "        test_image = random.choice(image_paths)\n",
    "    else:\n",
    "        test_image = random.choice(valid_images)\n",
    "    \n",
    "    # Get ground truth for the test image\n",
    "    test_image_id = test_image.stem\n",
    "    ground_truth = ground_truth_mapping.get(test_image_id, {}).get('work_order', 'Unknown')\n",
    "    \n",
    "    print(f\"‚úÖ Selected test image: {test_image.name}\")\n",
    "    print(f\"üìã Ground truth value: '{ground_truth}'\")\n",
    "    \n",
    "    # Display the test image\n",
    "    print(\"\\nüì∏ Test Image:\")\n",
    "    img = Image.open(test_image)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Test Invoice: {test_image_id}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Run the test using the pipeline or direct implementation\n",
    "    print(\"\\n‚è≥ Processing test image...\")\n",
    "    \n",
    "    test_start_time = time.time()\n",
    "    \n",
    "    if hasattr(pipeline, \"process_single_image\") and callable(getattr(pipeline, \"process_single_image\")):\n",
    "        # Use pipeline's method if available\n",
    "        result = pipeline.process_single_image(\n",
    "            image_path=str(test_image),\n",
    "            ground_truth=ground_truth,\n",
    "            prompt=prompt_info,\n",
    "            metrics=[\"exact_match\", \"character_error_rate\"]\n",
    "        )\n",
    "    else:\n",
    "        # Use direct implementation\n",
    "        result = process_image_with_metrics(\n",
    "            image_path=str(test_image),\n",
    "            ground_truth=ground_truth,\n",
    "            prompt=prompt_info,\n",
    "            model_name=model_config[\"name\"],\n",
    "            field_type=extraction_config[\"field_to_extract\"],\n",
    "            model=model,\n",
    "            processor=processor,\n",
    "            metrics=[\"exact_match\", \"character_error_rate\"]\n",
    "        )\n",
    "    \n",
    "    test_total_time = time.time() - test_start_time\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nüìä Test Results:\")\n",
    "    print(f\"   Raw Extraction: '{result.get('raw_extraction', 'ERROR')}'\")\n",
    "    print(f\"   Processed Extraction: '{result.get('processed_extraction', 'ERROR')}'\")\n",
    "    print(f\"   Ground Truth: '{result.get('ground_truth', 'Unknown')}'\")\n",
    "    print(f\"   Exact Match: {result.get('exact_match', False)}\")\n",
    "    print(f\"   Character Error Rate: {result.get('character_error_rate', 1.0):.4f}\")\n",
    "    print(f\"   Processing Time: {result.get('processing_time', 0):.2f} seconds\")\n",
    "    print(f\"   Total Test Time: {test_total_time:.2f} seconds\")\n",
    "    \n",
    "    # Provide assessment of test quality\n",
    "    if \"error\" in result:\n",
    "        print(f\"\\n‚ùå Test failed with error: {result['error']}\")\n",
    "        print(\"   Fix the error before proceeding to full extraction\")\n",
    "    elif result.get(\"exact_match\", False):\n",
    "        print(\"\\n‚úÖ Test successful! Extraction matches ground truth.\")\n",
    "        print(\"   Ready to proceed with full extraction.\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Test extraction does not exactly match ground truth.\")\n",
    "        \n",
    "        # Analyze the difference\n",
    "        gt = result.get('ground_truth', '')\n",
    "        extracted = result.get('processed_extraction', '')\n",
    "        \n",
    "        if result.get('character_error_rate', 1.0) < 0.2:\n",
    "            print(\"   ‚ö†Ô∏è Minor differences detected - extraction is close to ground truth\")\n",
    "            print(f\"   Ground Truth: '{gt}'\")\n",
    "            print(f\"   Extraction:   '{extracted}'\")\n",
    "            # Show differing characters\n",
    "            diff_positions = [i for i in range(min(len(gt), len(extracted))) if gt[i] != extracted[i]]\n",
    "            if diff_positions:\n",
    "                print(f\"   Differences at positions: {diff_positions}\")\n",
    "                for pos in diff_positions:\n",
    "                    print(f\"   Position {pos}: '{gt[pos]}' vs '{extracted[pos]}'\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è Significant differences detected\")\n",
    "            print(f\"   Ground Truth: '{gt}'\")\n",
    "            print(f\"   Extraction:   '{extracted}'\")\n",
    "            print(f\"   Character Error Rate: {result.get('character_error_rate', 1.0):.4f}\")\n",
    "    \n",
    "    # Save test result\n",
    "    test_result_path = get_results_path(\"single_image_test_result.json\")\n",
    "    with open(test_result_path, \"w\") as f:\n",
    "        # Add image information to result\n",
    "        result[\"image_id\"] = test_image_id\n",
    "        result[\"image_path\"] = str(test_image)\n",
    "        result[\"test_timestamp\"] = datetime.now().isoformat()\n",
    "        result[\"total_test_time\"] = test_total_time\n",
    "        \n",
    "        # Convert to serializable format\n",
    "        import json\n",
    "        json.dump(result, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Test result saved to: {test_result_path}\")\n",
    "    \n",
    "    # Store the result for reference\n",
    "    test_result = result\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during single image test: {e}\")\n",
    "    test_result = {\"error\": str(e)}\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Extraction Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Full Extraction Run\n",
    "\n",
    "# Import required libraries\n",
    "import time\n",
    "import torch\n",
    "import gc\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Verify pipeline, model, and other requirements are available\n",
    "if 'pipeline' not in locals():\n",
    "    print(\"‚ùå Pipeline not initialized. Run the pipeline initialization cell first.\")\n",
    "    raise RuntimeError(\"Pipeline not initialized\")\n",
    "    \n",
    "if 'model' not in locals() or model is None:\n",
    "    print(\"‚ùå Model not loaded. Run the model loading cell first.\")\n",
    "    raise RuntimeError(\"Model not loaded\")\n",
    "    \n",
    "if 'batch_items' not in locals() or len(batch_items) == 0:\n",
    "    print(\"‚ùå No batch items available. Run the ground truth data loading cell first.\")\n",
    "    raise RuntimeError(\"No batch items available\")\n",
    "\n",
    "# Verify test was successful before proceeding\n",
    "if 'test_result' not in locals() or 'error' in test_result:\n",
    "    print(\"‚ö†Ô∏è Warning: Single image test did not complete successfully.\")\n",
    "    proceed = input(\"Do you want to proceed with full extraction anyway? (y/n): \").strip().lower()\n",
    "    if proceed != 'y':\n",
    "        raise RuntimeError(\"Full extraction aborted by user. Run the single image test first.\")\n",
    "\n",
    "# Define utility functions for the extraction run\n",
    "def format_eta(seconds):\n",
    "    \"\"\"Format seconds into a human-readable ETA string\"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.0f} seconds\"\n",
    "    elif seconds < 3600:\n",
    "        minutes = seconds / 60\n",
    "        return f\"{minutes:.1f} minutes\"\n",
    "    else:\n",
    "        hours = seconds / 3600\n",
    "        return f\"{hours:.2f} hours\"\n",
    "\n",
    "def estimate_completion_time(current, total, elapsed_time):\n",
    "    \"\"\"Estimate completion time based on progress\"\"\"\n",
    "    if current == 0 or elapsed_time <= 0:\n",
    "        return \"Calculating...\"\n",
    "    \n",
    "    items_per_second = current / elapsed_time\n",
    "    remaining_items = total - current\n",
    "    \n",
    "    if items_per_second > 0:\n",
    "        remaining_seconds = remaining_items / items_per_second\n",
    "        eta = format_eta(remaining_seconds)\n",
    "        completion_time = datetime.now() + timedelta(seconds=remaining_seconds)\n",
    "        formatted_time = completion_time.strftime(\"%H:%M:%S\")\n",
    "        return f\"{eta} (completion around {formatted_time})\"\n",
    "    else:\n",
    "        return \"Unable to estimate\"\n",
    "\n",
    "def log_memory_usage(step, info=None):\n",
    "    \"\"\"Log GPU memory usage for monitoring\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    \n",
    "    memory_info = {\n",
    "        \"step\": step,\n",
    "        \"timestamp\": time.time(),\n",
    "        \"total_memory_gb\": torch.cuda.get_device_properties(0).total_memory / 1e9,\n",
    "        \"allocated_memory_gb\": torch.cuda.memory_allocated(0) / 1e9,\n",
    "        \"reserved_memory_gb\": torch.cuda.memory_reserved(0) / 1e9,\n",
    "        \"info\": info\n",
    "    }\n",
    "    \n",
    "    # Calculate free memory (total - allocated)\n",
    "    memory_info[\"free_memory_gb\"] = memory_info[\"total_memory_gb\"] - memory_info[\"allocated_memory_gb\"]\n",
    "    \n",
    "    return memory_info\n",
    "\n",
    "# Set up checkpoint path for resuming\n",
    "checkpoint_path = pipeline_config[\"checkpointing\"][\"checkpoint_path\"]\n",
    "checkpoint_frequency = pipeline_config[\"checkpointing\"][\"checkpoint_frequency\"]\n",
    "enable_checkpoints = pipeline_config[\"checkpointing\"][\"enable_checkpoints\"]\n",
    "\n",
    "# Check for existing checkpoint if resuming is enabled\n",
    "processed_items = []\n",
    "start_batch = 0\n",
    "if os.path.exists(checkpoint_path) and pipeline_config[\"checkpointing\"][\"resume_from_checkpoint\"]:\n",
    "    try:\n",
    "        with open(checkpoint_path, \"r\") as f:\n",
    "            checkpoint = json.load(f)\n",
    "        \n",
    "        processed_items = checkpoint.get(\"processed_items\", [])\n",
    "        start_batch = checkpoint.get(\"batch_idx\", 0)\n",
    "        \n",
    "        print(f\"üìÇ Resuming from checkpoint: {len(processed_items)} items already processed\")\n",
    "        print(f\"   Starting from batch {start_batch}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading checkpoint: {e}\")\n",
    "        print(\"   Starting from the beginning\")\n",
    "        processed_items = []\n",
    "        start_batch = 0\n",
    "\n",
    "# Determine batch size to use\n",
    "batch_size = pipeline_config[\"batch_processing\"].get(\"batch_size\", 1)\n",
    "print(f\"üî¢ Using batch size: {batch_size}\")\n",
    "\n",
    "# Determine number of items to process\n",
    "total_items = len(batch_items)\n",
    "if 'limit' in locals() and limit is not None and limit < total_items:\n",
    "    print(f\"‚öôÔ∏è Limiting extraction to first {limit} items (out of {total_items})\")\n",
    "    total_items = limit\n",
    "    batch_items = batch_items[:limit]\n",
    "else:\n",
    "    print(f\"‚öôÔ∏è Processing all {total_items} items\")\n",
    "\n",
    "# Remove already processed items if resuming\n",
    "if processed_items:\n",
    "    # Get IDs of already processed items\n",
    "    processed_ids = [item[\"image_id\"] for item in processed_items]\n",
    "    # Filter out already processed items\n",
    "    batch_items = [item for item in batch_items if item[\"image_id\"] not in processed_ids]\n",
    "    print(f\"üîÑ Removed {len(processed_ids)} already processed items\")\n",
    "    print(f\"   Remaining items to process: {len(batch_items)}\")\n",
    "\n",
    "# Check if all items have already been processed\n",
    "if not batch_items:\n",
    "    print(\"‚úÖ All items have already been processed.\")\n",
    "    print(f\"üìä Total processed items: {len(processed_items)}\")\n",
    "    \n",
    "    # Calculate final statistics from processed items\n",
    "    exact_matches = sum(1 for r in processed_items if r.get(\"exact_match\", False))\n",
    "    overall_accuracy = (exact_matches / len(processed_items) * 100) if processed_items and len(processed_items) > 0 else 0\n",
    "    \n",
    "    print(f\"üìä Exact Match Accuracy: {exact_matches}/{len(processed_items)} ({overall_accuracy:.1f}%)\")\n",
    "    \n",
    "    # Save final results\n",
    "    results_path = get_results_path(\"extraction_results.json\")\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(processed_items, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Results saved to: {results_path}\")\n",
    "    \n",
    "    # Store results for analysis\n",
    "    extraction_results = processed_items\n",
    "    print(\"üíæ No need to run extraction again.\")\n",
    "    \n",
    "else:\n",
    "    # Set up memory monitoring\n",
    "    memory_log = []\n",
    "    memory_log.append(log_memory_usage(\"start\", \"Before extraction starts\"))\n",
    "\n",
    "    # Display confirmation and execution plan\n",
    "    print(\"\\nüöÄ Starting full extraction pipeline...\")\n",
    "    print(f\"üìä Processing {len(batch_items)} items with batch size {batch_size}\")\n",
    "    print(f\"üíæ Checkpointing: {'Enabled' if enable_checkpoints else 'Disabled'}\")\n",
    "    if enable_checkpoints:\n",
    "        print(f\"   Saving checkpoint every {checkpoint_frequency} batches to: {checkpoint_path}\")\n",
    "\n",
    "    # Ask for confirmation before proceeding\n",
    "    proceed = input(\"\\nüí¨ Proceed with full extraction? This may take a while. (y/n): \").strip().lower()\n",
    "    if proceed != 'y':\n",
    "        print(\"‚ùå Extraction canceled by user.\")\n",
    "        raise RuntimeError(\"Full extraction canceled by user.\")\n",
    "\n",
    "    # Record start time for the full extraction\n",
    "    full_start_time = time.time()\n",
    "\n",
    "    # Prepare for batch processing\n",
    "    all_results = processed_items.copy()  # Start with already processed items if any\n",
    "    num_batches = math.ceil(len(batch_items) / batch_size)\n",
    "\n",
    "    print(f\"\\n‚è≥ Processing {len(batch_items)} items in {num_batches} batches...\")\n",
    "\n",
    "    try:\n",
    "        # Process in batches\n",
    "        for batch_idx in range(start_batch, num_batches):\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            # Get current batch\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(batch_items))\n",
    "            current_batch = batch_items[start_idx:end_idx]\n",
    "            \n",
    "            # Skip if batch is empty (shouldn't happen, but just in case)\n",
    "            if not current_batch:\n",
    "                print(f\"‚ö†Ô∏è Batch {batch_idx+1}/{num_batches} is empty, skipping\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nüîÑ Processing Batch {batch_idx+1}/{num_batches} ({len(current_batch)} items)\")\n",
    "            \n",
    "            # Log memory before batch\n",
    "            memory_log.append(log_memory_usage(f\"batch_{batch_idx}_start\", f\"Before batch {batch_idx+1}\"))\n",
    "            \n",
    "            # Process each item in the batch\n",
    "            batch_results = []\n",
    "            for i, item in enumerate(current_batch):\n",
    "                item_start_time = time.time()\n",
    "                \n",
    "                # Display progress\n",
    "                overall_progress = len(all_results) + len(batch_results) + 1\n",
    "                elapsed_time = time.time() - full_start_time\n",
    "                if elapsed_time > 0 and overall_progress > 0:\n",
    "                    eta = estimate_completion_time(overall_progress, len(batch_items) + len(processed_items), elapsed_time)\n",
    "                else:\n",
    "                    eta = \"Calculating...\"\n",
    "                \n",
    "                print(f\"\\n‚è≥ Item {overall_progress}/{len(batch_items) + len(processed_items)} | \" \n",
    "                      f\"Batch {batch_idx+1}.{i+1}/{batch_idx+1}.{len(current_batch)} | ETA: {eta}\")\n",
    "                \n",
    "                # Process the item\n",
    "                try:\n",
    "                    if hasattr(pipeline, \"process_single_image\") and callable(getattr(pipeline, \"process_single_image\")):\n",
    "                        # Use pipeline's method if available\n",
    "                        result = pipeline.process_single_image(\n",
    "                            image_path=item[\"image_path\"],\n",
    "                            ground_truth=item[\"ground_truth\"],\n",
    "                            prompt=prompt_info,\n",
    "                            metrics=pipeline_config[\"metrics\"][\"metrics\"]\n",
    "                        )\n",
    "                    else:\n",
    "                        # Use direct implementation\n",
    "                        result = process_image_with_metrics(\n",
    "                            image_path=item[\"image_path\"],\n",
    "                            ground_truth=item[\"ground_truth\"],\n",
    "                            prompt=prompt_info,\n",
    "                            model_name=model_config[\"name\"],\n",
    "                            field_type=extraction_config[\"field_to_extract\"],\n",
    "                            model=model,\n",
    "                            processor=processor,\n",
    "                            metrics=pipeline_config[\"metrics\"][\"metrics\"]\n",
    "                        )\n",
    "                    \n",
    "                    # Add image ID to result\n",
    "                    result[\"image_id\"] = item[\"image_id\"]\n",
    "                    \n",
    "                    # Add timing information\n",
    "                    result[\"processing_timestamp\"] = datetime.now().isoformat()\n",
    "                    result[\"batch_idx\"] = batch_idx\n",
    "                    result[\"item_idx\"] = i\n",
    "                    \n",
    "                    # Display result\n",
    "                    exact_match = result.get(\"exact_match\", False)\n",
    "                    match_symbol = \"‚úì\" if exact_match else \"‚úó\"\n",
    "                    print(f\"   Result: {match_symbol} | Extracted: '{result.get('processed_extraction', 'ERROR')}' | \"\n",
    "                          f\"GT: '{result.get('ground_truth', 'Unknown')}' | \"\n",
    "                          f\"CER: {result.get('character_error_rate', 1.0):.4f} | \"\n",
    "                          f\"Time: {result.get('processing_time', 0):.2f}s\")\n",
    "                    \n",
    "                    # Add to batch results\n",
    "                    batch_results.append(result)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error processing item {item['image_id']}: {e}\")\n",
    "                    # Add error result\n",
    "                    error_result = {\n",
    "                        \"image_id\": item[\"image_id\"],\n",
    "                        \"error\": str(e),\n",
    "                        \"processing_timestamp\": datetime.now().isoformat(),\n",
    "                        \"batch_idx\": batch_idx,\n",
    "                        \"item_idx\": i,\n",
    "                        \"ground_truth\": item[\"ground_truth\"],\n",
    "                        \"exact_match\": False,\n",
    "                        \"character_error_rate\": 1.0\n",
    "                    }\n",
    "                    batch_results.append(error_result)\n",
    "            \n",
    "            # Add batch results to all results\n",
    "            all_results.extend(batch_results)\n",
    "            \n",
    "            # Log memory after batch\n",
    "            memory_log.append(log_memory_usage(f\"batch_{batch_idx}_end\", f\"After batch {batch_idx+1}\"))\n",
    "            \n",
    "            # Calculate batch statistics\n",
    "            batch_time = time.time() - batch_start_time\n",
    "            # Ensure we don't divide by zero\n",
    "            if current_batch and len(current_batch) > 0:\n",
    "                avg_item_time = batch_time / len(current_batch)\n",
    "                exact_matches = sum(1 for r in batch_results if r.get(\"exact_match\", False))\n",
    "                accuracy = (exact_matches / len(current_batch) * 100)\n",
    "            else:\n",
    "                avg_item_time = 0\n",
    "                exact_matches = 0\n",
    "                accuracy = 0\n",
    "            \n",
    "            print(f\"\\nüìä Batch {batch_idx+1} Summary:\")\n",
    "            print(f\"   Processed {len(current_batch)} items in {batch_time:.2f}s ({avg_item_time:.2f}s per item)\")\n",
    "            print(f\"   Exact Match: {exact_matches}/{len(current_batch)} ({accuracy:.1f}%)\")\n",
    "            \n",
    "            # Clean up memory between batches if configured\n",
    "            if pipeline_config[\"batch_processing\"].get(\"optimize_memory\", True):\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    print(\"   üßπ Cleaned up memory\")\n",
    "            \n",
    "            # Save checkpoint if enabled\n",
    "            if enable_checkpoints and (batch_idx + 1) % checkpoint_frequency == 0:\n",
    "                if hasattr(pipeline, \"save_checkpoint\") and callable(getattr(pipeline, \"save_checkpoint\")):\n",
    "                    # Use pipeline's method if available\n",
    "                    checkpoint = pipeline.save_checkpoint(checkpoint_path, batch_idx + 1, all_results)\n",
    "                else:\n",
    "                    # Direct implementation\n",
    "                    checkpoint = {\n",
    "                        \"batch_idx\": batch_idx + 1,\n",
    "                        \"processed_items\": all_results,\n",
    "                        \"timestamp\": time.time()\n",
    "                    }\n",
    "                    with open(checkpoint_path, \"w\") as f:\n",
    "                        json.dump(checkpoint, f)\n",
    "                \n",
    "                print(f\"   üíæ Saved checkpoint after batch {batch_idx+1}\")\n",
    "        \n",
    "        # Log final memory usage\n",
    "        memory_log.append(log_memory_usage(\"end\", \"After extraction completes\"))\n",
    "        \n",
    "        # Save final results\n",
    "        results_path = get_results_path(\"extraction_results.json\")\n",
    "        with open(results_path, \"w\") as f:\n",
    "            json.dump(all_results, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nüíæ Final results saved to: {results_path}\")\n",
    "        \n",
    "        # Save memory log\n",
    "        memory_log_path = get_results_path(\"memory_usage_log.json\")\n",
    "        with open(memory_log_path, \"w\") as f:\n",
    "            json.dump(memory_log, f, indent=2)\n",
    "        \n",
    "        print(f\"üíæ Memory usage log saved to: {memory_log_path}\")\n",
    "        \n",
    "        # Calculate final statistics\n",
    "        total_time = time.time() - full_start_time\n",
    "        # Ensure we don't divide by zero\n",
    "        if all_results and len(all_results) > 0:\n",
    "            avg_time = total_time / len(all_results)\n",
    "            exact_matches = sum(1 for r in all_results if r.get(\"exact_match\", False))\n",
    "            overall_accuracy = (exact_matches / len(all_results) * 100)\n",
    "        else:\n",
    "            avg_time = 0\n",
    "            exact_matches = 0\n",
    "            overall_accuracy = 0\n",
    "        \n",
    "        print(\"\\nüìä Extraction Complete!\")\n",
    "        print(f\"   Processed {len(all_results)} items in {total_time:.2f}s ({avg_time:.2f}s per item)\")\n",
    "        print(f\"   Exact Match Accuracy: {exact_matches}/{len(all_results)} ({overall_accuracy:.1f}%)\")\n",
    "        \n",
    "        # Store results for analysis\n",
    "        extraction_results = all_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error during extraction: {e}\")\n",
    "        \n",
    "        # Try to save partial results if available\n",
    "        if len(all_results) > 0:\n",
    "            partial_results_path = get_results_path(\"partial_extraction_results.json\")\n",
    "            with open(partial_results_path, \"w\") as f:\n",
    "                json.dump(all_results, f, indent=2)\n",
    "            \n",
    "            print(f\"üíæ Partial results saved to: {partial_results_path}\")\n",
    "        \n",
    "        # Try to save memory log if available\n",
    "        if len(memory_log) > 0:\n",
    "            memory_log.append(log_memory_usage(\"error\", f\"Error: {str(e)}\"))\n",
    "            memory_log_path = get_results_path(\"error_memory_usage_log.json\")\n",
    "            with open(memory_log_path, \"w\") as f:\n",
    "                json.dump(memory_log, f, indent=2)\n",
    "            \n",
    "            print(f\"üíæ Memory usage log saved to: {memory_log_path}\")\n",
    "        \n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        # Clean up memory regardless of success or failure\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results / Analysis\n",
    "### Basic Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Basic Metrics Calculation\n",
    "\n",
    "# Import required libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Verify we have extraction results\n",
    "if 'extraction_results' not in locals() or not extraction_results:\n",
    "    print(\"‚ùå No extraction results found. Run the extraction cell first.\")\n",
    "    raise RuntimeError(\"No extraction results available for analysis\")\n",
    "\n",
    "print(\"üìä Calculating metrics from extraction results...\")\n",
    "print(f\"   Total results: {len(extraction_results)}\")\n",
    "\n",
    "# Convert results to pandas DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(extraction_results)\n",
    "print(f\"‚úÖ Converted {len(results_df)} results to DataFrame\")\n",
    "\n",
    "# Display dataframe info\n",
    "print(\"\\nüìã Results DataFrame Information:\")\n",
    "print(f\"   Shape: {results_df.shape}\")\n",
    "print(f\"   Columns: {', '.join(results_df.columns)}\")\n",
    "\n",
    "# Basic cleaning and type conversion if needed\n",
    "if 'exact_match' in results_df.columns:\n",
    "    results_df['exact_match'] = results_df['exact_match'].astype(bool)\n",
    "\n",
    "if 'processing_time' in results_df.columns:\n",
    "    # Ensure processing_time is numeric\n",
    "    results_df['processing_time'] = pd.to_numeric(results_df['processing_time'], errors='coerce')\n",
    "\n",
    "# Check for error cases\n",
    "error_cases = results_df[results_df['error'].notna()] if 'error' in results_df.columns else pd.DataFrame()\n",
    "valid_results = results_df[~results_df.index.isin(error_cases.index)] if not error_cases.empty else results_df\n",
    "\n",
    "print(f\"\\nüîç Found {len(error_cases)} error cases and {len(valid_results)} valid extraction results\")\n",
    "\n",
    "# Calculate basic accuracy metrics\n",
    "accuracy_metrics = {}\n",
    "\n",
    "if 'exact_match' in valid_results.columns:\n",
    "    exact_matches = valid_results['exact_match'].sum()\n",
    "    total_valid = len(valid_results)\n",
    "    exact_match_rate = exact_matches / total_valid if total_valid > 0 else 0\n",
    "    \n",
    "    accuracy_metrics['exact_match_count'] = int(exact_matches)\n",
    "    accuracy_metrics['total_valid_count'] = total_valid\n",
    "    accuracy_metrics['exact_match_rate'] = exact_match_rate\n",
    "    \n",
    "    print(f\"\\nüìà Exact Match Accuracy: {exact_matches}/{total_valid} ({exact_match_rate:.2%})\")\n",
    "\n",
    "# Calculate character error rate statistics\n",
    "if 'character_error_rate' in valid_results.columns:\n",
    "    cer_values = valid_results['character_error_rate'].dropna()\n",
    "    \n",
    "    if len(cer_values) > 0:\n",
    "        min_cer = cer_values.min()\n",
    "        max_cer = cer_values.max()\n",
    "        mean_cer = cer_values.mean()\n",
    "        median_cer = cer_values.median()\n",
    "        \n",
    "        accuracy_metrics['character_error_rate'] = {\n",
    "            'min': float(min_cer),\n",
    "            'max': float(max_cer),\n",
    "            'mean': float(mean_cer),\n",
    "            'median': float(median_cer)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìâ Character Error Rate Statistics:\")\n",
    "        print(f\"   Min: {min_cer:.4f}\")\n",
    "        print(f\"   Max: {max_cer:.4f}\")\n",
    "        print(f\"   Mean: {mean_cer:.4f}\")\n",
    "        print(f\"   Median: {median_cer:.4f}\")\n",
    "\n",
    "# Calculate timing statistics\n",
    "if 'processing_time' in valid_results.columns:\n",
    "    time_values = valid_results['processing_time'].dropna()\n",
    "    \n",
    "    if len(time_values) > 0:\n",
    "        min_time = time_values.min()\n",
    "        max_time = time_values.max()\n",
    "        mean_time = time_values.mean()\n",
    "        median_time = time_values.median()\n",
    "        total_time = time_values.sum()\n",
    "        \n",
    "        timing_metrics = {\n",
    "            'min_seconds': float(min_time),\n",
    "            'max_seconds': float(max_time),\n",
    "            'mean_seconds': float(mean_time),\n",
    "            'median_seconds': float(median_time),\n",
    "            'total_seconds': float(total_time)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚è±Ô∏è Processing Time Statistics:\")\n",
    "        print(f\"   Min: {min_time:.2f} seconds\")\n",
    "        print(f\"   Max: {max_time:.2f} seconds\")\n",
    "        print(f\"   Mean: {mean_time:.2f} seconds\")\n",
    "        print(f\"   Median: {median_time:.2f} seconds\")\n",
    "        print(f\"   Total: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "    else:\n",
    "        timing_metrics = {}\n",
    "else:\n",
    "    timing_metrics = {}\n",
    "\n",
    "# Analyze error types\n",
    "if not error_cases.empty and 'error' in error_cases.columns:\n",
    "    # Extract error message patterns\n",
    "    error_types = {}\n",
    "    \n",
    "    for error_msg in error_cases['error']:\n",
    "        if not isinstance(error_msg, str):\n",
    "            continue\n",
    "            \n",
    "        # Extract the first part of the error message (typically the error type)\n",
    "        error_type = error_msg.split(':')[0] if ':' in error_msg else error_msg\n",
    "        error_types[error_type] = error_types.get(error_type, 0) + 1\n",
    "    \n",
    "    print(f\"\\n‚ùå Error Type Analysis:\")\n",
    "    for error_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   {error_type}: {count} occurrences\")\n",
    "    \n",
    "    error_analysis = {\n",
    "        'total_errors': len(error_cases),\n",
    "        'error_types': error_types\n",
    "    }\n",
    "else:\n",
    "    error_analysis = {\n",
    "        'total_errors': 0,\n",
    "        'error_types': {}\n",
    "    }\n",
    "\n",
    "# Categorize results by success/failure\n",
    "if 'exact_match' in valid_results.columns:\n",
    "    successes = valid_results[valid_results['exact_match'] == True]\n",
    "    failures = valid_results[valid_results['exact_match'] == False]\n",
    "    \n",
    "    print(f\"\\nüéØ Result Categories:\")\n",
    "    print(f\"   Successful extractions: {len(successes)}\")\n",
    "    print(f\"   Failed extractions: {len(failures)}\")\n",
    "    print(f\"   Error cases: {len(error_cases)}\")\n",
    "    \n",
    "    result_categories = {\n",
    "        'successful_count': len(successes),\n",
    "        'failed_count': len(failures),\n",
    "        'error_count': len(error_cases)\n",
    "    }\n",
    "else:\n",
    "    result_categories = {}\n",
    "\n",
    "# Combine all metrics\n",
    "analysis_results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'total_results': len(extraction_results),\n",
    "    'accuracy_metrics': accuracy_metrics,\n",
    "    'timing_metrics': timing_metrics,\n",
    "    'error_analysis': error_analysis,\n",
    "    'result_categories': result_categories\n",
    "}\n",
    "\n",
    "# Save metrics to file\n",
    "metrics_path = get_results_path(\"extraction_metrics.json\")\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    json.dump(analysis_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Analysis metrics saved to: {metrics_path}\")\n",
    "\n",
    "# Store the metrics and categorized results for next cells\n",
    "metrics_results = analysis_results\n",
    "\n",
    "# Store categorized DataFrames for error analysis\n",
    "if 'exact_match' in valid_results.columns:\n",
    "    categorized_results = {\n",
    "        'successes': successes,\n",
    "        'failures': failures,\n",
    "        'errors': error_cases,\n",
    "        'all_results': results_df\n",
    "    }\n",
    "else:\n",
    "    categorized_results = {\n",
    "        'all_results': results_df\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Basic metrics calculation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Error Analysis\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from Levenshtein import distance\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Verify we have categorized results\n",
    "if 'categorized_results' not in locals() or not categorized_results:\n",
    "    print(\"‚ùå No categorized results found. Run the metrics calculation cell first.\")\n",
    "    raise RuntimeError(\"No categorized results available for analysis\")\n",
    "\n",
    "print(\"üîç Analyzing extraction errors and failure patterns...\")\n",
    "\n",
    "# Check if we have failure cases to analyze\n",
    "if 'failures' not in categorized_results or categorized_results['failures'].empty:\n",
    "    print(\"‚ÑπÔ∏è No failed extractions to analyze - all extractions were successful or had errors!\")\n",
    "    # Create empty dataframes to avoid errors in later cells\n",
    "    error_patterns = pd.DataFrame(columns=['pattern', 'count', 'percentage'])\n",
    "    failure_examples = pd.DataFrame(columns=['image_id', 'ground_truth', 'extracted_text', 'character_error_rate'])\n",
    "else:\n",
    "    # Get the failures dataframe\n",
    "    failures_df = categorized_results['failures']\n",
    "    print(f\"üìã Analyzing {len(failures_df)} failed extractions\")\n",
    "    \n",
    "    # Create a detailed analysis function for extraction failures\n",
    "    def analyze_extraction_difference(ground_truth, extraction):\n",
    "        \"\"\"Analyze the difference between ground truth and extraction\"\"\"\n",
    "        if not isinstance(ground_truth, str) or not isinstance(extraction, str):\n",
    "            return \"Invalid input types\"\n",
    "        \n",
    "        # Clean strings\n",
    "        gt = ground_truth.strip()\n",
    "        ext = extraction.strip()\n",
    "        \n",
    "        # Handle empty cases\n",
    "        if not gt and not ext:\n",
    "            return \"Both empty\"\n",
    "        if not gt:\n",
    "            return \"Ground truth empty\"\n",
    "        if not ext:\n",
    "            return \"Extraction empty\"\n",
    "        \n",
    "        # Calculate length difference\n",
    "        len_diff = len(ext) - len(gt)\n",
    "        \n",
    "        # Check for case differences\n",
    "        if gt.lower() == ext.lower():\n",
    "            return \"Case difference only\"\n",
    "        \n",
    "        # Check for whitespace-only differences\n",
    "        if gt.replace(\" \", \"\") == ext.replace(\" \", \"\"):\n",
    "            return \"Whitespace difference only\"\n",
    "            \n",
    "        # Check for character type mismatches (e.g., letter vs. number)\n",
    "        gt_alphas = sum(c.isalpha() for c in gt)\n",
    "        ext_alphas = sum(c.isalpha() for c in ext)\n",
    "        gt_digits = sum(c.isdigit() for c in gt)\n",
    "        ext_digits = sum(c.isdigit() for c in ext)\n",
    "        \n",
    "        if abs(gt_alphas - ext_alphas) > 0.5 * max(gt_alphas, ext_alphas) or \\\n",
    "           abs(gt_digits - ext_digits) > 0.5 * max(gt_digits, ext_digits):\n",
    "            return \"Character type mismatch\"\n",
    "            \n",
    "        # Check for substring relationship\n",
    "        if gt in ext:\n",
    "            return \"Ground truth is substring of extraction\"\n",
    "        if ext in gt:\n",
    "            return \"Extraction is substring of ground truth\"\n",
    "            \n",
    "        # Check for digits-only case\n",
    "        if gt.isdigit() and ext.isdigit():\n",
    "            if len(gt) == len(ext):\n",
    "                return \"Digit transposition or substitution\"\n",
    "            elif len(gt) > len(ext):\n",
    "                return \"Missing digits\"\n",
    "            else:\n",
    "                return \"Extra digits\"\n",
    "                \n",
    "        # For more complex cases, calculate edit distance and characterize\n",
    "        edit_dist = distance(gt, ext)\n",
    "        if edit_dist <= 2:\n",
    "            return \"Minor edit distance (1-2 chars)\"\n",
    "        elif edit_dist <= len(gt) * 0.3:\n",
    "            return \"Moderate edit distance (‚â§30%)\"\n",
    "        else:\n",
    "            return \"Major difference (>30% edit distance)\"\n",
    "    \n",
    "    # Add error pattern analysis to failures dataframe\n",
    "    pattern_column = []\n",
    "    details_column = []\n",
    "    \n",
    "    for _, row in failures_df.iterrows():\n",
    "        gt = row.get('ground_truth', '')\n",
    "        extracted = row.get('processed_extraction', '')\n",
    "        \n",
    "        if not isinstance(gt, str):\n",
    "            gt = str(gt) if gt is not None else ''\n",
    "        if not isinstance(extracted, str):\n",
    "            extracted = str(extracted) if extracted is not None else ''\n",
    "        \n",
    "        # Get error pattern\n",
    "        pattern = analyze_extraction_difference(gt, extracted)\n",
    "        pattern_column.append(pattern)\n",
    "        \n",
    "        # Create detailed comparison\n",
    "        if len(gt) > 0 and len(extracted) > 0:\n",
    "            # Highlight differences character by character\n",
    "            max_len = max(len(gt), len(extracted))\n",
    "            detail_text = f\"GT: '{gt}'\\nEX: '{extracted}'\\nDF: '\"\n",
    "            \n",
    "            for i in range(max_len):\n",
    "                if i < len(gt) and i < len(extracted):\n",
    "                    if gt[i] == extracted[i]:\n",
    "                        detail_text += \" \"\n",
    "                    else:\n",
    "                        detail_text += \"^\"\n",
    "                elif i < len(gt):\n",
    "                    detail_text += \"-\"  # Missing in extraction\n",
    "                else:\n",
    "                    detail_text += \"+\"  # Extra in extraction\n",
    "            \n",
    "            detail_text += \"'\"\n",
    "        else:\n",
    "            if len(gt) == 0:\n",
    "                detail_text = \"Ground truth is empty\"\n",
    "            elif len(extracted) == 0:\n",
    "                detail_text = \"Extraction is empty\"\n",
    "            else:\n",
    "                detail_text = \"Unknown comparison\"\n",
    "        \n",
    "        details_column.append(detail_text)\n",
    "    \n",
    "    # Add columns to dataframe\n",
    "    failures_df['error_pattern'] = pattern_column\n",
    "    failures_df['difference_details'] = details_column\n",
    "    \n",
    "    # Count and analyze error patterns\n",
    "    pattern_counts = Counter(pattern_column)\n",
    "    total_failures = len(failures_df)\n",
    "    \n",
    "    # Create a dataframe of error patterns\n",
    "    error_patterns = pd.DataFrame({\n",
    "        'pattern': list(pattern_counts.keys()),\n",
    "        'count': list(pattern_counts.values())\n",
    "    })\n",
    "    \n",
    "    # Add percentage column\n",
    "    error_patterns['percentage'] = error_patterns['count'] / total_failures * 100\n",
    "    \n",
    "    # Sort by count descending\n",
    "    error_patterns = error_patterns.sort_values('count', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Display error patterns\n",
    "    print(\"\\nüìä Error Pattern Analysis:\")\n",
    "    for i, row in error_patterns.iterrows():\n",
    "        print(f\"   {row['pattern']}: {row['count']} occurrences ({row['percentage']:.1f}%)\")\n",
    "    \n",
    "    # Select representative examples of each error pattern\n",
    "    print(\"\\nüìã Representative Examples of Error Patterns:\")\n",
    "    \n",
    "    # For each error pattern, find an example\n",
    "    examples = []\n",
    "    for pattern in error_patterns['pattern']:\n",
    "        # Get rows with this pattern\n",
    "        pattern_rows = failures_df[failures_df['error_pattern'] == pattern]\n",
    "        \n",
    "        if not pattern_rows.empty:\n",
    "            # Select a representative example (one with the median CER)\n",
    "            if 'character_error_rate' in pattern_rows.columns:\n",
    "                pattern_rows = pattern_rows.sort_values('character_error_rate')\n",
    "                example_row = pattern_rows.iloc[len(pattern_rows) // 2]  # Median\n",
    "            else:\n",
    "                # Just take the first one if no CER\n",
    "                example_row = pattern_rows.iloc[0]\n",
    "            \n",
    "            examples.append({\n",
    "                'pattern': pattern,\n",
    "                'image_id': example_row.get('image_id', 'Unknown'),\n",
    "                'ground_truth': example_row.get('ground_truth', ''),\n",
    "                'extracted_text': example_row.get('processed_extraction', ''),\n",
    "                'character_error_rate': example_row.get('character_error_rate', None),\n",
    "                'difference_details': example_row.get('difference_details', '')\n",
    "            })\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    failure_examples = pd.DataFrame(examples)\n",
    "    \n",
    "    # Display examples\n",
    "    for i, row in failure_examples.iterrows():\n",
    "        print(f\"\\n   Example of '{row['pattern']}':\")\n",
    "        print(f\"      Image: {row['image_id']}\")\n",
    "        print(f\"      Ground Truth: '{row['ground_truth']}'\")\n",
    "        print(f\"      Extracted: '{row['extracted_text']}'\")\n",
    "        if row['character_error_rate'] is not None:\n",
    "            print(f\"      Character Error Rate: {row['character_error_rate']:.4f}\")\n",
    "        print(f\"      {row['difference_details']}\")\n",
    "    \n",
    "    # Analyze length characteristics\n",
    "    if 'ground_truth' in failures_df.columns and 'processed_extraction' in failures_df.columns:\n",
    "        failures_df['gt_length'] = failures_df['ground_truth'].astype(str).apply(len)\n",
    "        failures_df['extraction_length'] = failures_df['processed_extraction'].astype(str).apply(len)\n",
    "        failures_df['length_diff'] = failures_df['extraction_length'] - failures_df['gt_length']\n",
    "        \n",
    "        # Create length analysis\n",
    "        len_stats = {\n",
    "            'avg_gt_length': failures_df['gt_length'].mean(),\n",
    "            'avg_extraction_length': failures_df['extraction_length'].mean(),\n",
    "            'avg_abs_length_diff': failures_df['length_diff'].abs().mean(),\n",
    "            'shorter_extractions': (failures_df['length_diff'] < 0).sum(),\n",
    "            'longer_extractions': (failures_df['length_diff'] > 0).sum(),\n",
    "            'same_length': (failures_df['length_diff'] == 0).sum()\n",
    "        }\n",
    "        \n",
    "        print(\"\\nüìè Length Characteristics in Failed Extractions:\")\n",
    "        print(f\"   Average ground truth length: {len_stats['avg_gt_length']:.1f} characters\")\n",
    "        print(f\"   Average extraction length: {len_stats['avg_extraction_length']:.1f} characters\")\n",
    "        print(f\"   Average absolute length difference: {len_stats['avg_abs_length_diff']:.1f} characters\")\n",
    "        print(f\"   Extractions shorter than ground truth: {len_stats['shorter_extractions']} ({len_stats['shorter_extractions']/total_failures*100:.1f}%)\")\n",
    "        print(f\"   Extractions longer than ground truth: {len_stats['longer_extractions']} ({len_stats['longer_extractions']/total_failures*100:.1f}%)\")\n",
    "        print(f\"   Extractions same length as ground truth: {len_stats['same_length']} ({len_stats['same_length']/total_failures*100:.1f}%)\")\n",
    "\n",
    "# Prepare the error analysis data for the report\n",
    "error_analysis_data = {\n",
    "    'total_failures': len(categorized_results.get('failures', pd.DataFrame())),\n",
    "    'error_patterns': error_patterns.to_dict('records') if 'error_patterns' in locals() else [],\n",
    "    'examples': failure_examples.to_dict('records') if 'failure_examples' in locals() else [],\n",
    "    'length_stats': len_stats if 'len_stats' in locals() else {}\n",
    "}\n",
    "\n",
    "# Save error analysis to file\n",
    "error_analysis_path = get_results_path(\"error_analysis.json\")\n",
    "with open(error_analysis_path, \"w\") as f:\n",
    "    json.dump(error_analysis_data, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Error analysis saved to: {error_analysis_path}\")\n",
    "print(\"‚úÖ Error analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "\n",
    "# Import required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Verify we have results to visualize\n",
    "if 'categorized_results' not in locals() or not categorized_results:\n",
    "    print(\"‚ùå No results to visualize. Run the metrics calculation cell first.\")\n",
    "    raise RuntimeError(\"No results available for visualization\")\n",
    "\n",
    "print(\"üìä Creating visualizations from extraction results...\")\n",
    "\n",
    "# Create directory for visualizations if needed\n",
    "vis_dir = get_results_path(\"visualizations\")\n",
    "os.makedirs(vis_dir, exist_ok=True)\n",
    "print(f\"üìÅ Saving visualizations to: {vis_dir}\")\n",
    "\n",
    "# Function to save figures\n",
    "def save_figure(fig, filename):\n",
    "    \"\"\"Save a figure to the visualizations directory\"\"\"\n",
    "    filepath = os.path.join(vis_dir, filename)\n",
    "    fig.savefig(filepath, bbox_inches='tight', dpi=150)\n",
    "    return filepath\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 1. Create accuracy bar chart\n",
    "# -------------------------------------------------------------------------------\n",
    "print(\"üìä Creating accuracy bar chart...\")\n",
    "\n",
    "# Get the counts from metrics\n",
    "all_results = categorized_results['all_results']\n",
    "success_count = len(categorized_results.get('successes', pd.DataFrame()))\n",
    "failure_count = len(categorized_results.get('failures', pd.DataFrame()))\n",
    "error_count = len(categorized_results.get('errors', pd.DataFrame()))\n",
    "total_count = success_count + failure_count + error_count\n",
    "\n",
    "# Create the figure\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Define categories and counts\n",
    "categories = ['Exact Match', 'Non-Match', 'Error']\n",
    "counts = [success_count, failure_count, error_count]\n",
    "colors = ['#2ecc71', '#e74c3c', '#f39c12']  # Green, Red, Orange\n",
    "\n",
    "# Create bar chart\n",
    "bars = ax.bar(categories, counts, color=colors)\n",
    "\n",
    "# Add title and labels\n",
    "ax.set_title('Extraction Results Breakdown', fontsize=16)\n",
    "ax.set_ylabel('Number of Images', fontsize=14)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    percentage = height / total_count * 100\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "            f'{int(height)}\\n({percentage:.1f}%)', \n",
    "            ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "# Save the figure\n",
    "accuracy_chart_path = save_figure(fig, \"accuracy_chart.png\")\n",
    "plt.close(fig)\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 2. Create character error rate histogram\n",
    "# -------------------------------------------------------------------------------\n",
    "print(\"üìä Creating character error rate histogram...\")\n",
    "\n",
    "# Filter out error cases and get valid CER values\n",
    "valid_results = all_results[~all_results.index.isin(categorized_results.get('errors', pd.DataFrame()).index)]\n",
    "if 'character_error_rate' in valid_results.columns:\n",
    "    cer_values = valid_results['character_error_rate'].dropna()\n",
    "    \n",
    "    if len(cer_values) > 0:\n",
    "        # Create the figure\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        # Create histogram with custom bins\n",
    "        bins = np.linspace(0, min(1.0, cer_values.max() * 1.1), 20)\n",
    "        n, bins, patches = ax.hist(cer_values, bins=bins, color='#3498db', alpha=0.7, edgecolor='black')\n",
    "        \n",
    "        # Add title and labels\n",
    "        ax.set_title('Character Error Rate Distribution', fontsize=16)\n",
    "        ax.set_xlabel('Character Error Rate', fontsize=14)\n",
    "        ax.set_ylabel('Number of Images', fontsize=14)\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Add vertical line for mean and median\n",
    "        mean_cer = cer_values.mean()\n",
    "        median_cer = cer_values.median()\n",
    "        ax.axvline(x=mean_cer, color='#e74c3c', linestyle='--', \n",
    "                   label=f'Mean: {mean_cer:.4f}')\n",
    "        ax.axvline(x=median_cer, color='#2ecc71', linestyle='-', \n",
    "                   label=f'Median: {median_cer:.4f}')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Save the figure\n",
    "        cer_chart_path = save_figure(fig, \"character_error_rate_histogram.png\")\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No valid character error rate values to plot\")\n",
    "        cer_chart_path = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No character error rate data found in results\")\n",
    "    cer_chart_path = None\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 3. Create processing time histogram\n",
    "# -------------------------------------------------------------------------------\n",
    "print(\"üìä Creating processing time histogram...\")\n",
    "\n",
    "# Get processing time values\n",
    "if 'processing_time' in all_results.columns:\n",
    "    time_values = all_results['processing_time'].dropna()\n",
    "    \n",
    "    if len(time_values) > 0:\n",
    "        # Create the figure\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        # Create histogram\n",
    "        bins = 20\n",
    "        n, bins, patches = ax.hist(time_values, bins=bins, color='#9b59b6', alpha=0.7, edgecolor='black')\n",
    "        \n",
    "        # Add title and labels\n",
    "        ax.set_title('Processing Time Distribution', fontsize=16)\n",
    "        ax.set_xlabel('Processing Time (seconds)', fontsize=14)\n",
    "        ax.set_ylabel('Number of Images', fontsize=14)\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Add vertical line for mean and median\n",
    "        mean_time = time_values.mean()\n",
    "        median_time = time_values.median()\n",
    "        ax.axvline(x=mean_time, color='#e74c3c', linestyle='--', \n",
    "                  label=f'Mean: {mean_time:.2f}s')\n",
    "        ax.axvline(x=median_time, color='#2ecc71', linestyle='-', \n",
    "                  label=f'Median: {median_time:.2f}s')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Save the figure\n",
    "        time_chart_path = save_figure(fig, \"processing_time_histogram.png\")\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No valid processing time values to plot\")\n",
    "        time_chart_path = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No processing time data found in results\")\n",
    "    time_chart_path = None\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 4. Create error patterns bar chart (if available)\n",
    "# -------------------------------------------------------------------------------\n",
    "print(\"üìä Creating error patterns chart...\")\n",
    "\n",
    "if 'error_patterns' in locals() and not error_patterns.empty:\n",
    "    # Create the figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Get top N patterns (to avoid overcrowding)\n",
    "    top_n = min(8, len(error_patterns))\n",
    "    top_patterns = error_patterns.head(top_n)\n",
    "    \n",
    "    # Create horizontal bar chart\n",
    "    bars = ax.barh(top_patterns['pattern'], top_patterns['count'], color='#e74c3c', alpha=0.7)\n",
    "    \n",
    "    # Add title and labels\n",
    "    ax.set_title('Top Error Patterns', fontsize=16)\n",
    "    ax.set_xlabel('Number of Occurrences', fontsize=14)\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        percentage = top_patterns.iloc[i]['percentage']\n",
    "        ax.text(width + 0.5, bar.get_y() + bar.get_height()/2.,\n",
    "                f'{int(width)} ({percentage:.1f}%)', \n",
    "                ha='left', va='center', fontsize=12)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    error_patterns_chart_path = save_figure(fig, \"error_patterns_chart.png\")\n",
    "    plt.close(fig)\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No error patterns data available to plot\")\n",
    "    error_patterns_chart_path = None\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 5. Generate HTML summary dashboard\n",
    "# -------------------------------------------------------------------------------\n",
    "print(\"üìÑ Generating HTML summary dashboard...\")\n",
    "\n",
    "# Get metadata for the dashboard\n",
    "experiment_name = extraction_config[\"experiment_name\"] if \"extraction_config\" in locals() else \"Extraction Experiment\"\n",
    "model_name = model_config[\"name\"] if \"model_config\" in locals() else \"Unknown Model\"\n",
    "prompt_text = prompt_info[\"text\"] if \"prompt_info\" in locals() else \"Unknown Prompt\"\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Calculate summary metrics\n",
    "total_images = len(all_results)\n",
    "success_rate = success_count / total_images * 100 if total_images > 0 else 0\n",
    "avg_time = all_results['processing_time'].mean() if 'processing_time' in all_results.columns else 0\n",
    "avg_cer = valid_results['character_error_rate'].mean() if 'character_error_rate' in valid_results.columns else 0\n",
    "\n",
    "# Create HTML content\n",
    "html_content = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Extraction Results Dashboard</title>\n",
    "    <style>\n",
    "        body {{\n",
    "            font-family: Arial, sans-serif;\n",
    "            margin: 0;\n",
    "            padding: 20px;\n",
    "            background-color: #f5f5f5;\n",
    "        }}\n",
    "        .container {{\n",
    "            max-width: 1200px;\n",
    "            margin: 0 auto;\n",
    "            background-color: white;\n",
    "            padding: 20px;\n",
    "            border-radius: 5px;\n",
    "            box-shadow: 0 0 10px rgba(0,0,0,0.1);\n",
    "        }}\n",
    "        h1, h2, h3 {{\n",
    "            color: #333;\n",
    "        }}\n",
    "        .metrics {{\n",
    "            display: flex;\n",
    "            flex-wrap: wrap;\n",
    "            margin: 20px 0;\n",
    "        }}\n",
    "        .metric-card {{\n",
    "            background-color: white;\n",
    "            border-radius: 5px;\n",
    "            box-shadow: 0 0 5px rgba(0,0,0,0.1);\n",
    "            padding: 15px;\n",
    "            margin: 10px;\n",
    "            min-width: 200px;\n",
    "            flex: 1;\n",
    "        }}\n",
    "        .metric-title {{\n",
    "            font-size: 14px;\n",
    "            color: #666;\n",
    "            margin-bottom: 5px;\n",
    "        }}\n",
    "        .metric-value {{\n",
    "            font-size: 24px;\n",
    "            font-weight: bold;\n",
    "            color: #333;\n",
    "        }}\n",
    "        .chart-container {{\n",
    "            margin: 20px 0;\n",
    "            text-align: center;\n",
    "        }}\n",
    "        .chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "### Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Results Summary\n",
    "\n",
    "# Import required libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "# Verify we have analysis results\n",
    "if 'metrics_results' not in locals() or not metrics_results:\n",
    "    print(\"‚ùå No metrics results found. Run the metrics calculation cell first.\")\n",
    "    raise RuntimeError(\"No metrics results available for summary\")\n",
    "\n",
    "print(\"üìã Generating comprehensive experiment summary...\")\n",
    "\n",
    "# Gather all experiment details\n",
    "experiment_details = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"experiment_name\": extraction_config[\"experiment_name\"] if \"extraction_config\" in locals() else \"Extraction Experiment\",\n",
    "    \"model\": {\n",
    "        \"name\": model_config[\"name\"] if \"model_config\" in locals() else \"Unknown Model\",\n",
    "        \"repo_id\": model_config[\"repo_id\"] if \"model_config\" in locals() else \"Unknown Repo\",\n",
    "        \"quantization\": model_config.get(\"use_quantization\", False) if \"model_config\" in locals() else False,\n",
    "        \"quantization_bits\": model_config.get(\"quantization_bits\", None) if \"model_config\" in locals() else None,\n",
    "        \"dtype\": model_metadata.get(\"dtype\", \"Unknown\") if \"model_metadata\" in locals() else \"Unknown\",\n",
    "        \"device\": model_metadata.get(\"device\", \"Unknown\") if \"model_metadata\" in locals() else \"Unknown\"\n",
    "    },\n",
    "    \"prompt\": {\n",
    "        \"text\": prompt_info[\"text\"] if \"prompt_info\" in locals() else \"Unknown Prompt\",\n",
    "        \"name\": prompt_info[\"name\"] if \"prompt_info\" in locals() else \"Unknown\",\n",
    "        \"category\": prompt_info[\"category\"] if \"prompt_info\" in locals() else \"Unknown\"\n",
    "    },\n",
    "    \"execution\": {\n",
    "        \"batch_size\": pipeline_config[\"batch_processing\"].get(\"batch_size\", 1) if \"pipeline_config\" in locals() else 1,\n",
    "        \"checkpointing\": pipeline_config[\"checkpointing\"].get(\"enable_checkpoints\", False) if \"pipeline_config\" in locals() else False,\n",
    "        \"environment\": env_config.environment if \"env_config\" in locals() else \"Unknown\"\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"total_images\": len(categorized_results['all_results']) if 'categorized_results' in locals() else 0,\n",
    "        \"field_to_extract\": extraction_config[\"field_to_extract\"] if \"extraction_config\" in locals() else \"work_order\",\n",
    "        \"ground_truth_column\": extraction_config.get(\"field_column_name\", \"Unknown\") if \"extraction_config\" in locals() else \"Unknown\"\n",
    "    },\n",
    "    \"results\": metrics_results\n",
    "}\n",
    "\n",
    "# Save the comprehensive summary\n",
    "summary_path = get_results_path(\"experiment_summary.json\")\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump(experiment_details, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Comprehensive summary saved to: {summary_path}\")\n",
    "\n",
    "# Display summary table\n",
    "print(\"\\nüìä Experiment Results Summary\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Experiment: {experiment_details['experiment_name']}\")\n",
    "print(f\"Model: {experiment_details['model']['name']}\")\n",
    "print(f\"Conducted: {datetime.fromisoformat(experiment_details['timestamp']).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Environment: {experiment_details['execution']['environment']}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display dataset information\n",
    "print(\"\\nüìä Dataset Information:\")\n",
    "print(f\"Total Images: {experiment_details['dataset']['total_images']}\")\n",
    "print(f\"Field to Extract: {experiment_details['dataset']['field_to_extract']}\")\n",
    "\n",
    "# Display model information\n",
    "print(\"\\nü§ñ Model Information:\")\n",
    "print(f\"Model: {experiment_details['model']['name']}\")\n",
    "print(f\"Repository: {experiment_details['model']['repo_id']}\")\n",
    "print(f\"Dtype: {experiment_details['model']['dtype']}\")\n",
    "print(f\"Device: {experiment_details['model']['device']}\")\n",
    "if experiment_details['model']['quantization']:\n",
    "    print(f\"Quantization: {experiment_details['model']['quantization_bits']}-bit\")\n",
    "else:\n",
    "    print(\"Quantization: Disabled\")\n",
    "\n",
    "# Display prompt information\n",
    "print(\"\\nüìù Prompt Information:\")\n",
    "print(f\"Name: {experiment_details['prompt']['name']}\")\n",
    "print(f\"Category: {experiment_details['prompt']['category']}\")\n",
    "print(f\"Text: \\\"{experiment_details['prompt']['text']}\\\"\")\n",
    "\n",
    "# Display execution information\n",
    "print(\"\\n‚öôÔ∏è Execution Information:\")\n",
    "print(f\"Batch Size: {experiment_details['execution']['batch_size']}\")\n",
    "print(f\"Checkpointing: {'Enabled' if experiment_details['execution']['checkpointing'] else 'Disabled'}\")\n",
    "\n",
    "# Display performance metrics\n",
    "print(\"\\nüìà Performance Metrics:\")\n",
    "accuracy = experiment_details['results']['accuracy_metrics'].get('exact_match_rate', 0) * 100\n",
    "exact_matches = experiment_details['results']['accuracy_metrics'].get('exact_match_count', 0)\n",
    "total_valid = experiment_details['results']['accuracy_metrics'].get('total_valid_count', 0)\n",
    "avg_cer = experiment_details['results']['accuracy_metrics'].get('character_error_rate', {}).get('mean', 0)\n",
    "avg_time = experiment_details['results']['timing_metrics'].get('mean_seconds', 0)\n",
    "total_time = experiment_details['results']['timing_metrics'].get('total_seconds', 0)\n",
    "\n",
    "print(f\"Exact Match: {exact_matches}/{total_valid} ({accuracy:.2f}%)\")\n",
    "print(f\"Average Character Error Rate: {avg_cer:.4f}\")\n",
    "print(f\"Average Processing Time: {avg_time:.2f} seconds per image\")\n",
    "print(f\"Total Processing Time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "\n",
    "# Display error statistics\n",
    "error_count = experiment_details['results']['error_analysis'].get('total_errors', 0)\n",
    "error_pct = error_count / experiment_details['dataset']['total_images'] * 100 if experiment_details['dataset']['total_images'] > 0 else 0\n",
    "print(f\"Error Cases: {error_count} ({error_pct:.2f}%)\")\n",
    "\n",
    "# Check if there are previous runs to compare with\n",
    "results_dir = os.path.dirname(get_results_path(\"\"))\n",
    "parent_dir = os.path.dirname(results_dir)\n",
    "all_result_dirs = glob.glob(os.path.join(parent_dir, \"*\"))\n",
    "previous_summaries = []\n",
    "\n",
    "for result_dir in all_result_dirs:\n",
    "    # Skip current results directory\n",
    "    if result_dir == results_dir:\n",
    "        continue\n",
    "    \n",
    "    # Look for summary file\n",
    "    summary_file = os.path.join(result_dir, \"experiment_summary.json\")\n",
    "    if os.path.exists(summary_file):\n",
    "        try:\n",
    "            with open(summary_file, \"r\") as f:\n",
    "                previous_summary = json.load(f)\n",
    "            previous_summaries.append({\n",
    "                \"dir\": os.path.basename(result_dir),\n",
    "                \"summary\": previous_summary\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading summary from {result_dir}: {e}\")\n",
    "\n",
    "# Compare with previous runs if available\n",
    "if previous_summaries:\n",
    "    print(\"\\nüîÑ Comparison with Previous Runs:\")\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_data = []\n",
    "    for prev in previous_summaries:\n",
    "        # Get key metrics\n",
    "        prev_accuracy = prev[\"summary\"].get(\"results\", {}).get(\"accuracy_metrics\", {}).get(\"exact_match_rate\", 0) * 100\n",
    "        prev_avg_cer = prev[\"summary\"].get(\"results\", {}).get(\"accuracy_metrics\", {}).get(\"character_error_rate\", {}).get(\"mean\", 0)\n",
    "        prev_avg_time = prev[\"summary\"].get(\"results\", {}).get(\"timing_metrics\", {}).get(\"mean_seconds\", 0)\n",
    "        prev_model = prev[\"summary\"].get(\"model\", {}).get(\"name\", \"Unknown\")\n",
    "        prev_prompt_category = prev[\"summary\"].get(\"prompt\", {}).get(\"category\", \"Unknown\")\n",
    "        prev_timestamp = datetime.fromisoformat(prev[\"summary\"].get(\"timestamp\", datetime.now().isoformat())).strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        comparison_data.append({\n",
    "            \"run\": prev[\"dir\"],\n",
    "            \"date\": prev_timestamp,\n",
    "            \"model\": prev_model,\n",
    "            \"prompt_category\": prev_prompt_category,\n",
    "            \"accuracy\": prev_accuracy,\n",
    "            \"avg_cer\": prev_avg_cer,\n",
    "            \"avg_time\": prev_avg_time\n",
    "        })\n",
    "    \n",
    "    # Add current run\n",
    "    comparison_data.append({\n",
    "        \"run\": os.path.basename(results_dir),\n",
    "        \"date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"model\": experiment_details[\"model\"][\"name\"],\n",
    "        \"prompt_category\": experiment_details[\"prompt\"][\"category\"],\n",
    "        \"accuracy\": accuracy,\n",
    "        \"avg_cer\": avg_cer,\n",
    "        \"avg_time\": avg_time,\n",
    "        \"current\": True\n",
    "    })\n",
    "    \n",
    "    # Convert to dataframe and sort by date\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.sort_values(\"date\", ascending=False)\n",
    "    \n",
    "    # Display comparison table\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(\"\\nComparison Table:\")\n",
    "    display(comparison_df)\n",
    "    \n",
    "    # Calculate improvements\n",
    "    if len(comparison_df) > 1:\n",
    "        # Find the most recent previous run\n",
    "        previous_runs = comparison_df[comparison_df[\"current\"] != True]\n",
    "        if not previous_runs.empty:\n",
    "            most_recent = previous_runs.iloc[0]\n",
    "            \n",
    "            # Calculate changes\n",
    "            accuracy_change = accuracy - most_recent[\"accuracy\"]\n",
    "            cer_change = avg_cer - most_recent[\"avg_cer\"]\n",
    "            time_change = avg_time - most_recent[\"avg_time\"]\n",
    "            \n",
    "            print(\"\\nComparison with Most Recent Run:\")\n",
    "            print(f\"Accuracy Change: {accuracy_change:+.2f}% ({'+' if accuracy_change >= 0 else ''}{accuracy_change/most_recent['accuracy']*100:.2f}% relative)\")\n",
    "            print(f\"CER Change: {cer_change:+.4f} ({'+' if cer_change >= 0 else ''}{cer_change/most_recent['avg_cer']*100:.2f}% relative)\")\n",
    "            print(f\"Processing Time Change: {time_change:+.2f}s ({'+' if time_change >= 0 else ''}{time_change/most_recent['avg_time']*100:.2f}% relative)\")\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è No previous runs available for comparison\")\n",
    "\n",
    "# Generate conclusions\n",
    "print(\"\\nüìù Experiment Conclusions:\")\n",
    "\n",
    "# Accuracy assessment\n",
    "if accuracy >= 95:\n",
    "    accuracy_assessment = \"Excellent accuracy achieved, suitable for production use.\"\n",
    "elif accuracy >= 85:\n",
    "    accuracy_assessment = \"Good accuracy achieved, may need review for critical cases.\"\n",
    "elif accuracy >= 70:\n",
    "    accuracy_assessment = \"Moderate accuracy, needs improvement for production use.\"\n",
    "else:\n",
    "    accuracy_assessment = \"Low accuracy, significant improvements needed.\"\n",
    "\n",
    "print(f\"1. {accuracy_assessment}\")\n",
    "\n",
    "# CER assessment\n",
    "if avg_cer <= 0.05:\n",
    "    cer_assessment = \"Very low character error rate, indicating high extraction quality.\"\n",
    "elif avg_cer <= 0.15:\n",
    "    cer_assessment = \"Acceptable character error rate, minor extraction errors present.\"\n",
    "elif avg_cer <= 0.3:\n",
    "    cer_assessment = \"Moderate character error rate, review needed for many extractions.\"\n",
    "else:\n",
    "    cer_assessment = \"High character error rate, indicating significant extraction issues.\"\n",
    "\n",
    "print(f\"2. {cer_assessment}\")\n",
    "\n",
    "# Performance assessment\n",
    "if avg_time <= 0.5:\n",
    "    perf_assessment = \"Excellent processing speed, suitable for high volume processing.\"\n",
    "elif avg_time <= 1.5:\n",
    "    perf_assessment = \"Good processing speed, acceptable for most use cases.\"\n",
    "elif avg_time <= 3.0:\n",
    "    perf_assessment = \"Moderate processing speed, may need optimization for large volumes.\"\n",
    "else:\n",
    "    perf_assessment = \"Slow processing speed, optimization needed for production use.\"\n",
    "\n",
    "print(f\"3. {perf_assessment}\")\n",
    "\n",
    "# Error assessment\n",
    "if error_pct <= 1:\n",
    "    error_assessment = \"Very few errors encountered, indicating stable extraction pipeline.\"\n",
    "elif error_pct <= 5:\n",
    "    error_assessment = \"Low error rate, acceptable for most use cases.\"\n",
    "elif error_pct <= 10:\n",
    "    error_assessment = \"Moderate error rate, needs investigation for production use.\"\n",
    "else:\n",
    "    error_assessment = \"High error rate, indicating significant pipeline stability issues.\"\n",
    "\n",
    "print(f\"4. {error_assessment}\")\n",
    "\n",
    "# Overall conclusion\n",
    "print(\"\\nOverall Conclusion:\")\n",
    "if accuracy >= 85 and avg_cer <= 0.15 and error_pct <= 5:\n",
    "    overall = \"This extraction pipeline demonstrates strong performance and is suitable for production use with minimal oversight.\"\n",
    "elif accuracy >= 70 and avg_cer <= 0.3 and error_pct <= 10:\n",
    "    overall = \"This extraction pipeline shows promise but needs further refinement before full production deployment.\"\n",
    "else:\n",
    "    overall = \"This extraction pipeline requires significant improvements in accuracy and stability before production use.\"\n",
    "\n",
    "print(overall)\n",
    "\n",
    "# Save conclusions\n",
    "conclusions = {\n",
    "    \"accuracy_assessment\": accuracy_assessment,\n",
    "    \"cer_assessment\": cer_assessment,\n",
    "    \"performance_assessment\": perf_assessment,\n",
    "    \"error_assessment\": error_assessment,\n",
    "    \"overall_conclusion\": overall\n",
    "}\n",
    "\n",
    "# Add conclusions to the experiment summary\n",
    "experiment_details[\"conclusions\"] = conclusions\n",
    "\n",
    "# Save updated summary\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump(experiment_details, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Updated summary with conclusions saved to: {summary_path}\")\n",
    "print(\"\\n‚úÖ Experiment summary complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Resource Cleanup\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üßπ Cleaning up resources and recording final state...\")\n",
    "\n",
    "# Record initial memory state\n",
    "initial_memory = None\n",
    "if torch.cuda.is_available():\n",
    "    initial_memory = {\n",
    "        \"total_memory_gb\": torch.cuda.get_device_properties(0).total_memory / 1e9,\n",
    "        \"allocated_memory_gb\": torch.cuda.memory_allocated(0) / 1e9,\n",
    "        \"reserved_memory_gb\": torch.cuda.memory_reserved(0) / 1e9,\n",
    "        \"free_memory_gb\": (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1e9\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüß† Initial GPU Memory State:\")\n",
    "    print(f\"   Total Memory: {initial_memory['total_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Allocated Memory: {initial_memory['allocated_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Reserved Memory: {initial_memory['reserved_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Free Memory: {initial_memory['free_memory_gb']:.2f} GB\")\n",
    "\n",
    "# Release model from memory\n",
    "print(\"\\nüîÑ Releasing model from memory...\")\n",
    "if 'model' in locals():\n",
    "    try:\n",
    "        # Delete the model\n",
    "        del model\n",
    "        print(\"   ‚úì Model deleted\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error deleting model: {e}\")\n",
    "else:\n",
    "    print(\"   ‚ÑπÔ∏è No model in memory to delete\")\n",
    "\n",
    "# Release processor from memory\n",
    "if 'processor' in locals():\n",
    "    try:\n",
    "        # Delete the processor\n",
    "        del processor\n",
    "        print(\"   ‚úì Processor deleted\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error deleting processor: {e}\")\n",
    "else:\n",
    "    print(\"   ‚ÑπÔ∏è No processor in memory to delete\")\n",
    "\n",
    "# Release pipeline from memory\n",
    "if 'pipeline' in locals():\n",
    "    try:\n",
    "        # Delete the pipeline\n",
    "        del pipeline\n",
    "        print(\"   ‚úì Pipeline deleted\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error deleting pipeline: {e}\")\n",
    "else:\n",
    "    print(\"   ‚ÑπÔ∏è No pipeline in memory to delete\")\n",
    "\n",
    "# Release other large variables\n",
    "large_variables = ['extraction_results', 'categorized_results', 'all_results', 'metrics_results']\n",
    "for var_name in large_variables:\n",
    "    if var_name in locals():\n",
    "        try:\n",
    "            # Use exec to delete the variable\n",
    "            exec(f\"del {var_name}\")\n",
    "            print(f\"   ‚úì Deleted {var_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error deleting {var_name}: {e}\")\n",
    "\n",
    "# Run garbage collection\n",
    "print(\"\\nüßπ Running garbage collection...\")\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"   ‚úì CUDA cache emptied and synchronized\")\n",
    "else:\n",
    "    print(\"   ‚ÑπÔ∏è No CUDA available for cache emptying\")\n",
    "\n",
    "# Clean up temporary files\n",
    "print(\"\\nüóëÔ∏è Cleaning up temporary files...\")\n",
    "\n",
    "# Define temporary directories to clean\n",
    "temp_dirs = []\n",
    "\n",
    "# Clean up any temporary files in the results directory\n",
    "temp_files = []  # Add any specific temp files pattern here if needed\n",
    "\n",
    "# Remove temporary files if any were specified\n",
    "for file_pattern in temp_files:\n",
    "    matching_files = []  # Add glob or similar to find matching files\n",
    "    for file_path in matching_files:\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"   ‚úì Removed temporary file: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error removing file {file_path}: {e}\")\n",
    "\n",
    "# Record final memory state\n",
    "final_memory = None\n",
    "if torch.cuda.is_available():\n",
    "    final_memory = {\n",
    "        \"total_memory_gb\": torch.cuda.get_device_properties(0).total_memory / 1e9,\n",
    "        \"allocated_memory_gb\": torch.cuda.memory_allocated(0) / 1e9,\n",
    "        \"reserved_memory_gb\": torch.cuda.memory_reserved(0) / 1e9,\n",
    "        \"free_memory_gb\": (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1e9\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüß† Final GPU Memory State:\")\n",
    "    print(f\"   Total Memory: {final_memory['total_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Allocated Memory: {final_memory['allocated_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Reserved Memory: {final_memory['reserved_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Free Memory: {final_memory['free_memory_gb']:.2f} GB\")\n",
    "    \n",
    "    if initial_memory:\n",
    "        memory_freed = initial_memory['allocated_memory_gb'] - final_memory['allocated_memory_gb']\n",
    "        print(f\"   Memory Freed: {memory_freed:.2f} GB\")\n",
    "\n",
    "# Record final environment state\n",
    "final_state = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"torch_version\": torch.__version__,\n",
    "    \"cuda_available\": torch.cuda.is_available(),\n",
    "    \"initial_memory\": initial_memory,\n",
    "    \"final_memory\": final_memory,\n",
    "    \"cleanup_success\": True\n",
    "}\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    final_state[\"cuda_version\"] = torch.version.cuda\n",
    "    final_state[\"device_name\"] = torch.cuda.get_device_name(0)\n",
    "\n",
    "# Save final state\n",
    "try:\n",
    "    final_state_path = get_results_path(\"final_environment_state.json\")\n",
    "    with open(final_state_path, \"w\") as f:\n",
    "        json.dump(final_state, f, indent=2)\n",
    "    print(f\"\\nüíæ Final environment state saved to: {final_state_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Error saving final state: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Resource cleanup complete!\")\n",
    "print(\"\\nüìù Experiment execution finished. Results and visualizations are available in the results directory.\")\n",
    "print(f\"   Results directory: {get_results_path('')}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
