{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Single Model, Single Prompt Test\n",
        "\n",
        "## Overview\n",
        "This notebook demonstrates the extraction of work order numbers from invoice images using a single model (Pixtral-12B) and a single prompt approach. It serves as a replication of the successful RunPod experiment using our new structured framework.\n",
        "\n",
        "## Objectives\n",
        "- Load the Pixtral-12B model using our model management framework\n",
        "- Apply a specific prompt that was effective in previous tests\n",
        "- Process invoice images to extract work order numbers\n",
        "- Evaluate extraction accuracy against ground truth data\n",
        "- Establish a baseline for further prompt and model experimentation\n",
        "\n",
        "## Approach\n",
        "We'll use the basic prompt that worked well in our initial tests:\n",
        "```\n",
        "Extract the work order number from this invoice image.\n",
        "```\n",
        "\n",
        "This will be formatted appropriately for the Pixtral model using our prompt management system.\n",
        "\n",
        "## Expected Results\n",
        "- Extraction accuracy metrics (exact match rate, character error rate)\n",
        "- Processing time analysis\n",
        "- Comparison with previous RunPod experiment results\n",
        "- Foundation for systematic prompt and model comparisons\n",
        "\n",
        "This notebook represents Step 1 in our experimental workflow, focusing on reproducing known successful results before expanding to prompt comparison (Step 2) and model comparison (Step 3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "import platform\n",
        "import torch\n",
        "import transformers\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "import logging\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "# Configure basic logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger('single_model_test')\n",
        "\n",
        "# Display basic system information\n",
        "print(f\"üîç Environment Setup and Verification\")\n",
        "print(f\"üêç Python version: {platform.python_version()}\")\n",
        "print(f\"üìä PyTorch version: {torch.__version__}\")\n",
        "print(f\"ü§ñ Transformers version: {transformers.__version__}\")\n",
        "print(f\"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Set up project paths\n",
        "# This cell finds the project root and initializes path configuration\n",
        "\n",
        "# First, try to find the project root directory\n",
        "notebook_path = Path().resolve()\n",
        "project_root = None\n",
        "\n",
        "# Look for project root markers (traversing upward from the notebook)\n",
        "for parent in [notebook_path] + list(notebook_path.parents):\n",
        "    if any((parent / marker).exists() for marker in ['.git', 'setup.py', 'requirements.txt']):\n",
        "        project_root = parent\n",
        "        break\n",
        "\n",
        "if project_root is None:\n",
        "    # If markers not found, assume we're in a subdirectory of the project\n",
        "    # and the parent directory is the project root\n",
        "    project_root = notebook_path.parent\n",
        "    print(\"‚ö†Ô∏è Could not definitively locate project root, using parent directory\")\n",
        "else:\n",
        "    print(f\"‚úÖ Found project root: {project_root}\")\n",
        "\n",
        "# Add project root to Python path if not already there\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "    print(f\"üìå Added {project_root} to Python path\")\n",
        "\n",
        "# Set environment variable for project root\n",
        "os.environ['PROJECT_ROOT'] = str(project_root)\n",
        "print(f\"üîÑ Set PROJECT_ROOT environment variable to {project_root}\")\n",
        "\n",
        "# Verify critical directories exist, create if needed\n",
        "critical_dirs = [\n",
        "    \"configs/environments\",\n",
        "    \"configs/prompts\",       # For prompt configurations\n",
        "    \"src/config\",\n",
        "    \"data/images\",\n",
        "    \"models/cache\",\n",
        "    \"results/raw\",\n",
        "    \"results/visualizations\", # For result visualizations\n",
        "    \"logs\"\n",
        "]\n",
        "\n",
        "for dir_path in critical_dirs:\n",
        "    full_path = project_root / dir_path\n",
        "    if not full_path.exists():\n",
        "        print(f\"üìÅ Creating directory: {dir_path}\")\n",
        "        full_path.mkdir(parents=True, exist_ok=True)\n",
        "    else:\n",
        "        print(f\"‚úÖ Directory exists: {dir_path}\")\n",
        "\n",
        "# Check for ground truth data file\n",
        "ground_truth_path = project_root / \"data\" / \"ground_truth.csv\"\n",
        "if os.path.exists(ground_truth_path):\n",
        "    print(f\"‚úÖ Ground truth data file found\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Ground truth data file not found at: {ground_truth_path}\")\n",
        "    print(\"   This will be needed for comparing extraction results.\")\n",
        "\n",
        "# Check for GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    device_name = torch.cuda.get_device_name(0)\n",
        "    memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"üñ•Ô∏è GPU: {device_name} ({memory:.2f} GB)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected - running in CPU mode\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import and initialize environment configuration\n",
        "# This cell loads appropriate configuration for local or RunPod environment\n",
        "\n",
        "try:\n",
        "    # Import configuration modules\n",
        "    from src.config.environment import get_environment_config\n",
        "    from src.config.paths import get_path_config\n",
        "    \n",
        "    # Get environment configuration\n",
        "    env_config = get_environment_config()\n",
        "    print(f\"üìå Detected environment: {env_config.environment}\")\n",
        "    \n",
        "    # Get path configuration for this experiment\n",
        "    experiment_name = f\"single_model_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    paths = get_path_config(experiment_name=experiment_name)\n",
        "    \n",
        "    # Display configurations\n",
        "    print(\"\\nüîç Environment Configuration Summary:\")\n",
        "    env_config.print_summary()\n",
        "    \n",
        "    print(\"\\nüìÇ Path Configuration:\")\n",
        "    print(paths)\n",
        "    \n",
        "    # Verify paths are valid\n",
        "    if paths.verify(verbose=True):\n",
        "        print(\"‚úÖ All critical paths verified\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Some paths could not be verified\")\n",
        "    \n",
        "    # Import experiment configuration\n",
        "    from src.config.experiment import get_experiment_config\n",
        "    \n",
        "    # Create experiment configuration for this test\n",
        "    experiment_config = get_experiment_config(\n",
        "        experiment_type=\"single_model\",\n",
        "        overrides={\n",
        "            \"experiment.name\": experiment_name,\n",
        "            \"experiment.description\": \"Testing work order extraction with Pixtral-12B using a single prompt\",\n",
        "            \"experiment.model_name\": \"pixtral-12b\",\n",
        "            \"experiment.field_to_extract\": \"work_order\",\n",
        "            \"experiment.prompt_category\": \"specific\",  # Using a specific prompt\n",
        "            \"experiment.specific_prompt\": \"basic_work_order\"  # The prompt that worked in RunPod\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # Initialize prompt system\n",
        "    try:\n",
        "        from src.prompts import initialize_prompt_system\n",
        "        \n",
        "        # Initialize the prompt system\n",
        "        prompt_init_result = initialize_prompt_system()\n",
        "        if prompt_init_result[\"status\"] == \"success\":\n",
        "            print(f\"‚úÖ Prompt system initialized with {prompt_init_result['prompt_count']} prompts\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Prompt system initialization issue: {prompt_init_result.get('error', 'Unknown error')}\")\n",
        "        \n",
        "        # Print experiment configuration summary\n",
        "        print(\"\\nüß™ Experiment Configuration:\")\n",
        "        experiment_config.print_summary()\n",
        "        \n",
        "    except ImportError as e:\n",
        "        print(f\"‚ùå Error importing prompt system: {e}\")\n",
        "        print(\"üìù Make sure the src/prompts directory is properly set up\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Error importing configuration modules: {e}\")\n",
        "    print(\"üìù This suggests there might be an issue with your project structure or Python path\")\n",
        "    print(\"üí° Make sure the src/config directory exists and contains the necessary files\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error initializing configuration: {e}\")\n",
        "    print(\"üí° Check your YAML configuration files for syntax errors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final verification and setup confirmation\n",
        "# This cell confirms the environment is ready for experiments\n",
        "\n",
        "print(\"üîç Performing final environment checks...\")\n",
        "\n",
        "# Check for critical data files\n",
        "gt_path = paths.ground_truth_path\n",
        "if os.path.exists(gt_path):\n",
        "    # Load and show basic info about ground truth data\n",
        "    try:\n",
        "        ground_truth_df = pd.read_csv(gt_path)\n",
        "        print(f\"‚úÖ Ground truth data loaded: {len(ground_truth_df)} records\")\n",
        "        print(f\"   Columns: {', '.join(ground_truth_df.columns[:5])}{'...' if len(ground_truth_df.columns) > 5 else ''}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Ground truth file exists but couldn't be loaded: {e}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Ground truth file not found at: {gt_path}\")\n",
        "    print(\"   You'll need to add this before running experiments.\")\n",
        "\n",
        "# Check for images\n",
        "image_paths = list(paths.get_image_paths())\n",
        "if image_paths:\n",
        "    print(f\"‚úÖ Found {len(image_paths)} invoice images\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No invoice images found in data directory\")\n",
        "    print(\"   You'll need to add images before running experiments.\")\n",
        "\n",
        "# Save setup confirmation\n",
        "setup_info = {\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"environment\": env_config.environment,\n",
        "    \"python_version\": platform.python_version(),\n",
        "    \"pytorch_version\": torch.__version__,\n",
        "    \"transformers_version\": transformers.__version__,\n",
        "    \"gpu_available\": torch.cuda.is_available(),\n",
        "    \"images_found\": len(image_paths),\n",
        "    \"ground_truth_records\": len(ground_truth_df) if 'ground_truth_df' in locals() else 0\n",
        "}\n",
        "\n",
        "# Save setup information for future reference\n",
        "setup_info_path = paths.get_results_path(\"setup_info.json\")\n",
        "import json\n",
        "with open(setup_info_path, \"w\") as f:\n",
        "    json.dump(setup_info, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚ú® Setup information saved to: {setup_info_path}\")\n",
        "print(\"\\nüìã Environment setup complete!\")\n",
        "print(\"   You're now ready to run experiments in the execution notebook.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prompt Selection\n",
        "The following cell selects an initial, basic prompt for loading into the experiment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prompt Selection and Verification\n",
        "# This cell selects and formats the specific prompt for our experiment\n",
        "\n",
        "try:\n",
        "    # Import prompt utilities\n",
        "    from src.prompts import get_prompt, format_prompt, analyze_prompt\n",
        "    \n",
        "    # Get the specific prompt that worked in RunPod\n",
        "    prompt_name = experiment_config.get(\"experiment.specific_prompt\", \"basic_work_order\")\n",
        "    selected_prompt = get_prompt(prompt_name)\n",
        "    \n",
        "    if selected_prompt:\n",
        "        print(f\"‚úÖ Selected prompt: \\\"{selected_prompt.name}\\\"\")\n",
        "        print(f\"   Category: {selected_prompt.category}\")\n",
        "        print(f\"   Field: {selected_prompt.field_to_extract}\")\n",
        "        print(f\"   Prompt text: \\\"{selected_prompt.text}\\\"\")\n",
        "        \n",
        "        # Format the prompt for the Pixtral model\n",
        "        model_name = experiment_config.get(\"experiment.model_name\", \"pixtral-12b\")\n",
        "        formatted_prompt = format_prompt(selected_prompt, model_name)\n",
        "        \n",
        "        print(f\"\\nüìù Formatted for {model_name}:\")\n",
        "        print(f\"   \\\"{formatted_prompt}\\\"\")\n",
        "        \n",
        "        # Analyze the prompt\n",
        "        prompt_analysis = analyze_prompt(selected_prompt)\n",
        "        print(f\"\\nüîç Prompt Analysis:\")\n",
        "        print(f\"   Word count: {prompt_analysis['word_count']}\")\n",
        "        print(f\"   Character count: {prompt_analysis['character_count']}\")\n",
        "        print(f\"   Complexity score: {prompt_analysis['complexity_score']:.2f}\")\n",
        "        \n",
        "        # Store for later use\n",
        "        prompt_info = {\n",
        "            \"name\": selected_prompt.name,\n",
        "            \"text\": selected_prompt.text,\n",
        "            \"formatted_text\": formatted_prompt,\n",
        "            \"category\": selected_prompt.category,\n",
        "            \"field_to_extract\": selected_prompt.field_to_extract,\n",
        "            \"analysis\": prompt_analysis\n",
        "        }\n",
        "        \n",
        "    else:\n",
        "        print(f\"‚ùå Error: Prompt '{prompt_name}' not found in registry\")\n",
        "        print(\"   Please check your prompt configuration and registry initialization\")\n",
        "        prompt_info = None\n",
        "        \n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Error importing prompt modules: {e}\")\n",
        "    print(\"   Make sure your prompt management system is properly installed\")\n",
        "    prompt_info = None\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error during prompt selection: {e}\")\n",
        "    prompt_info = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Final Verification check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final verification and setup confirmation\n",
        "# This cell confirms the environment is ready for experiments\n",
        "\n",
        "print(\"üîç Performing final environment checks...\")\n",
        "\n",
        "# Check for critical data files\n",
        "gt_path = paths.ground_truth_path\n",
        "if os.path.exists(gt_path):\n",
        "    # Load and show basic info about ground truth data\n",
        "    try:\n",
        "        ground_truth_df = pd.read_csv(gt_path)\n",
        "        print(f\"‚úÖ Ground truth data loaded: {len(ground_truth_df)} records\")\n",
        "        print(f\"   Columns: {', '.join(ground_truth_df.columns[:5])}{'...' if len(ground_truth_df.columns) > 5 else ''}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Ground truth file exists but couldn't be loaded: {e}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Ground truth file not found at: {gt_path}\")\n",
        "    print(\"   You'll need to add this before running experiments.\")\n",
        "\n",
        "# Check for images\n",
        "image_paths = list(paths.get_image_paths())\n",
        "if image_paths:\n",
        "    print(f\"‚úÖ Found {len(image_paths)} invoice images\")\n",
        "    # Display a sample image path\n",
        "    if len(image_paths) > 0:\n",
        "        print(f\"   Sample image: {image_paths[0].name}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No invoice images found in data directory\")\n",
        "    print(\"   You'll need to add images before running experiments.\")\n",
        "\n",
        "# Verify prompt selection was successful\n",
        "if 'prompt_info' in locals() and prompt_info is not None:\n",
        "    print(f\"‚úÖ Prompt selected and formatted successfully: {prompt_info['name']}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Prompt selection failed. Check previous cell for errors.\")\n",
        "\n",
        "# Save setup confirmation with complete experiment details\n",
        "setup_info = {\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"environment\": env_config.environment,\n",
        "    \"python_version\": platform.python_version(),\n",
        "    \"pytorch_version\": torch.__version__},\n",
        "    \"transformers_version\": transformers.__version__,\n",
        "    \"gpu_available\": torch.cuda.is_available(),\n",
        "    \"images_found\": len(image_paths),\n",
        "    \"ground_truth_records\": len(ground_truth_df) if 'ground_truth_df' in locals() else 0,\n",
        "    # Add experiment details\n",
        "    \"experiment_name\": experiment_name,\n",
        "    \"experiment_type\": experiment_config.experiment_type,\n",
        "    \"model_name\": experiment_config.get(\"experiment.model_name\", \"unknown\")\n",
        "}\n",
        "\n",
        "# Add prompt information if available\n",
        "if 'prompt_info' in locals() and prompt_info is not None:\n",
        "    setup_info[\"prompt\"] = {\n",
        "        \"name\": prompt_info[\"name\"],\n",
        "        \"text\": prompt_info[\"text\"],\n",
        "        \"category\": prompt_info[\"category\"],\n",
        "        \"field_to_extract\": prompt_info[\"field_to_extract\"],\n",
        "        \"word_count\": prompt_info[\"analysis\"][\"word_count\"]\n",
        "    }\n",
        "\n",
        "# Save setup information for future reference\n",
        "setup_info_path = paths.get_results_path(\"setup_info.json\")\n",
        "import json\n",
        "with open(setup_info_path, \"w\") as f:\n",
        "    json.dump(setup_info, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚ú® Setup information saved to: {setup_info_path}\")\n",
        "print(\"\\nüìã Environment setup complete!\")\n",
        "print(\"   You're now ready to proceed with the model loading and extraction test.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the Test on the Model\n",
        "### Model Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Loading Cell\n",
        "# This cell loads the Pixtral-12B model using our model loading framework\n",
        "\n",
        "try:\n",
        "    # Import model loading utilities\n",
        "    from src.models.loader import load_model_and_processor, get_gpu_memory_info, verify_gpu_compatibility\n",
        "    \n",
        "    # Get model name from experiment config\n",
        "    model_name = experiment_config.get(\"experiment.model_name\", \"pixtral-12b\")\n",
        "    \n",
        "    # Check GPU compatibility before attempting to load\n",
        "    compatibility = verify_gpu_compatibility(model_name)\n",
        "    if not compatibility[\"compatible\"]:\n",
        "        print(f\"‚ùå Warning: {compatibility['reason']}\")\n",
        "        print(f\"   Model requirements: {compatibility['model_requirements']}\")\n",
        "        print(f\"   Current GPU: {compatibility['current_gpu']}\")\n",
        "        print(\"   Proceeding anyway, but may encounter memory issues\")\n",
        "    else:\n",
        "        print(f\"‚úÖ GPU compatible with {model_name} requirements\")\n",
        "    \n",
        "    # Display pre-loading memory state\n",
        "    if torch.cuda.is_available():\n",
        "        pre_memory = get_gpu_memory_info()\n",
        "        print(f\"üß† Pre-loading GPU memory: {pre_memory['allocated_memory_gb']:.2f} GB / {pre_memory['total_memory_gb']:.2f} GB\")\n",
        "    \n",
        "    # Load the model with appropriate configuration\n",
        "    print(f\"‚è≥ Loading {model_name}... (this may take a minute)\")\n",
        "    model, processor = load_model_and_processor(\n",
        "        model_name=model_name,\n",
        "        # Optional: uncomment the following lines to configure model loading\n",
        "        # quantization=\"4bit\",  # Quantization strategy (None, \"4bit\", \"8bit\")\n",
        "        # cache_dir=paths.model_cache_dir  # Specify cache directory\n",
        "    )\n",
        "    \n",
        "    # Display post-loading memory state\n",
        "    if torch.cuda.is_available():\n",
        "        post_memory = get_gpu_memory_info()\n",
        "        print(f\"üß† Post-loading GPU memory: {post_memory['allocated_memory_gb']:.2f} GB / {post_memory['total_memory_gb']:.2f} GB\")\n",
        "        print(f\"   Model memory usage: {post_memory['allocated_memory_gb'] - pre_memory['allocated_memory_gb']:.2f} GB\")\n",
        "    \n",
        "    # Store model info for later reference\n",
        "    model_info = {\n",
        "        \"name\": model_name,\n",
        "        \"device\": model.device,\n",
        "        \"dtype\": str(next(model.parameters()).dtype),\n",
        "        \"loaded_at\": datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    print(f\"‚úÖ Successfully loaded {model_name}\")\n",
        "    print(f\"   Model dtype: {model_info['dtype']}\")\n",
        "    print(f\"   Model device: {model_info['device']}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading model: {e}\")\n",
        "    print(\"   Check your model configuration and GPU setup\")\n",
        "    # Set model/processor to None if loading failed\n",
        "    model, processor = None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare Ground Truth for Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ground Truth Mapping Cell\n",
        "# This cell creates a mapping between image IDs and ground truth values\n",
        "\n",
        "try:\n",
        "    # Import needed utilities\n",
        "    from src.execution.batch import prepare_batch_items\n",
        "    \n",
        "    print(\"üîÑ Preparing ground truth mapping...\")\n",
        "    \n",
        "    # Define the field to extract from experiment config\n",
        "    field_type = experiment_config.get(\"experiment.field_to_extract\", \"work_order\")\n",
        "    \n",
        "    # Determine the column name in the CSV for the field type\n",
        "    field_mapping = {\n",
        "        \"work_order\": \"Work Order Number/Numero de Orden\",\n",
        "        \"cost\": \"Total\",\n",
        "        # Add more field types as needed\n",
        "    }\n",
        "    \n",
        "    field_column = field_mapping.get(field_type, field_type)\n",
        "    image_id_column = \"Invoice\"  # Column containing image IDs\n",
        "    \n",
        "    print(f\"üìä Using field column: '{field_column}' for {field_type} extraction\")\n",
        "    \n",
        "    # Create mapping from image ID to ground truth\n",
        "    ground_truth_mapping = {}\n",
        "    unmatched_images = []\n",
        "    \n",
        "    # Ensure ground truth data is loaded\n",
        "    if 'ground_truth_df' not in locals() or ground_truth_df is None:\n",
        "        ground_truth_df = pd.read_csv(paths.ground_truth_path)\n",
        "        print(f\"üìÇ Loaded ground truth data: {len(ground_truth_df)} records\")\n",
        "    \n",
        "    # Create the mapping\n",
        "    for _, row in ground_truth_df.iterrows():\n",
        "        # Convert image ID to string and remove any file extension\n",
        "        image_id = str(row[image_id_column])\n",
        "        image_id = Path(image_id).stem  # Get just the filename without extension\n",
        "        \n",
        "        # Store the ground truth value\n",
        "        if field_column in row:\n",
        "            # Convert to string and strip whitespace\n",
        "            ground_truth_mapping[image_id] = str(row[field_column]).strip()\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Field column '{field_column}' not found in row for image {image_id}\")\n",
        "    \n",
        "    # Get list of image paths\n",
        "    image_paths = list(paths.get_image_paths())\n",
        "    \n",
        "    # Check if all images have ground truth\n",
        "    for img_path in image_paths:\n",
        "        img_id = img_path.stem\n",
        "        if img_id not in ground_truth_mapping:\n",
        "            unmatched_images.append(img_id)\n",
        "    \n",
        "    # Prepare structured batch items for processing\n",
        "    batch_items = prepare_batch_items(image_paths, ground_truth_mapping)\n",
        "    \n",
        "    # Display statistics\n",
        "    print(f\"‚úÖ Created ground truth mapping for {len(ground_truth_mapping)} images\")\n",
        "    print(f\"üì∏ Total images available: {len(image_paths)}\")\n",
        "    print(f\"üîÑ Prepared {len(batch_items)} items for processing\")\n",
        "    \n",
        "    if unmatched_images:\n",
        "        print(f\"‚ö†Ô∏è Found {len(unmatched_images)} images without ground truth data\")\n",
        "        if len(unmatched_images) < 10:\n",
        "            print(f\"   Unmatched images: {', '.join(unmatched_images)}\")\n",
        "        else:\n",
        "            print(f\"   First 10 unmatched images: {', '.join(unmatched_images[:10])}...\")\n",
        "    else:\n",
        "        print(\"‚úÖ All images have matching ground truth data\")\n",
        "    \n",
        "    # Display a few examples from the mapping\n",
        "    print(\"\\nüìã Sample of ground truth mapping:\")\n",
        "    for i, (img_id, gt_value) in enumerate(list(ground_truth_mapping.items())[:5]):\n",
        "        print(f\"   {img_id}: '{gt_value}'\")\n",
        "    \n",
        "    # Store mapping info for later reference\n",
        "    mapping_info = {\n",
        "        \"total_images\": len(image_paths),\n",
        "        \"mapped_images\": len(ground_truth_mapping),\n",
        "        \"unmapped_images\": len(unmatched_images),\n",
        "        \"field_type\": field_type,\n",
        "        \"field_column\": field_column\n",
        "    }\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error creating ground truth mapping: {e}\")\n",
        "    print(\"   Check your ground truth CSV file and field column names\")\n",
        "    # Create empty mapping in case of error\n",
        "    ground_truth_mapping = {}\n",
        "    batch_items = []\n",
        "    mapping_info = {\"error\": str(e)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize the Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pipeline Initialization Cell\n",
        "# This cell initializes the extraction pipeline with experiment settings\n",
        "\n",
        "try:\n",
        "    # Import the extraction pipeline\n",
        "    from src.execution.pipeline import ExtractionPipeline\n",
        "    \n",
        "    print(\"üöÄ Initializing extraction pipeline...\")\n",
        "    \n",
        "    # Gather experiment parameters\n",
        "    experiment_params = {\n",
        "        \"experiment_name\": experiment_name,\n",
        "        \"model_name\": experiment_config.get(\"experiment.model_name\", \"pixtral-12b\"),\n",
        "        \"field_to_extract\": experiment_config.get(\"experiment.field_to_extract\", \"work_order\"),\n",
        "        \"prompt_name\": prompt_info[\"name\"] if prompt_info else None,\n",
        "        \"prompt_category\": experiment_config.get(\"experiment.prompt_category\", \"specific\"),\n",
        "        \"description\": experiment_config.get(\"experiment.description\", \"Single model test\"),\n",
        "    }\n",
        "    \n",
        "    # Set up the pipeline configuration\n",
        "    pipeline_config = {\n",
        "        # General experiment settings\n",
        "        \"experiment_name\": experiment_params[\"experiment_name\"],\n",
        "        \"model_name\": experiment_params[\"model_name\"],\n",
        "        \"field_to_extract\": experiment_params[\"field_to_extract\"],\n",
        "        \"prompt_name\": experiment_params[\"prompt_name\"],\n",
        "        \"prompt_category\": experiment_params[\"prompt_category\"],\n",
        "        \"description\": experiment_params[\"description\"],\n",
        "        \n",
        "        # Batch processing settings\n",
        "        \"batch_processing\": {\n",
        "            \"auto_batch_size\": True,  # Estimate optimal batch size\n",
        "            \"max_batch_size\": 8,      # Maximum batch size to consider\n",
        "            \"default_batch_size\": 1,  # Default if auto-estimation fails\n",
        "            \"optimize_between_batches\": True  # Clean up memory between batches\n",
        "        },\n",
        "        \n",
        "        # Checkpointing settings\n",
        "        \"enable_checkpointing\": True,\n",
        "        \"checkpoint_frequency\": 5,  # Save checkpoint after every 5 batches\n",
        "        \"resume_from_checkpoint\": True,  # Resume from existing checkpoint if available\n",
        "        \n",
        "        # Output settings\n",
        "        \"show_progress\": True,\n",
        "        \"metrics\": [\"exact_match\", \"character_error_rate\"],\n",
        "        \n",
        "        # Additional settings specific to this experiment\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"environment\": env_config.environment\n",
        "    }\n",
        "    \n",
        "    # Initialize the pipeline\n",
        "    pipeline = ExtractionPipeline(experiment_name=experiment_params[\"experiment_name\"])\n",
        "    \n",
        "    # Update pipeline config\n",
        "    for key, value in pipeline_config.items():\n",
        "        pipeline.config[key] = value\n",
        "    \n",
        "    # Set the model and processor if already loaded\n",
        "    if 'model' in locals() and model is not None and 'processor' in locals() and processor is not None:\n",
        "        pipeline.model = model\n",
        "        pipeline.processor = processor\n",
        "        print(\"‚úÖ Using pre-loaded model and processor\")\n",
        "    \n",
        "    # Set the ground truth mapping if already created\n",
        "    if 'ground_truth_mapping' in locals() and ground_truth_mapping:\n",
        "        pipeline.ground_truth_mapping = ground_truth_mapping\n",
        "        print(f\"‚úÖ Using pre-created ground truth mapping with {len(ground_truth_mapping)} entries\")\n",
        "    \n",
        "    # Get the prompt that will be used\n",
        "    if prompt_info:\n",
        "        from src.prompts.registry import get_prompt\n",
        "        pipeline_prompt = get_prompt(prompt_info[\"name\"])\n",
        "        print(f\"‚úÖ Using prompt: {pipeline_prompt.name}\")\n",
        "        print(f\"   Prompt text: \\\"{pipeline_prompt.text}\\\"\")\n",
        "    else:\n",
        "        pipeline_prompt = None\n",
        "        print(\"‚ö†Ô∏è No specific prompt selected, pipeline will use default\")\n",
        "    \n",
        "    # Save the pipeline configuration\n",
        "    config_path = paths.get_results_path(\"pipeline_config.json\")\n",
        "    with open(config_path, \"w\") as f:\n",
        "        json.dump(pipeline.config, f, indent=2)\n",
        "    \n",
        "    print(f\"üíæ Saved pipeline configuration to {config_path}\")\n",
        "    print(\"‚úÖ Pipeline initialization complete!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error initializing pipeline: {e}\")\n",
        "    print(\"   Check your pipeline implementation and configuration\")\n",
        "    pipeline = None\n",
        "    pipeline_prompt = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test on just a single image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single Image Test Cell\n",
        "# This cell tests the pipeline with a single image before running the full extraction\n",
        "\n",
        "try:\n",
        "    if pipeline is None:\n",
        "        raise ValueError(\"Pipeline not initialized. Run the previous cell first.\")\n",
        "    \n",
        "    print(\"üß™ Testing extraction on a single image...\")\n",
        "    \n",
        "    # Select a sample image\n",
        "    sample_images = list(paths.get_image_paths())[:5]  # Get first 5 images\n",
        "    if not sample_images:\n",
        "        raise ValueError(\"No images found. Add images to your data directory.\")\n",
        "    \n",
        "    # Choose the first image that has ground truth data\n",
        "    sample_image = None\n",
        "    for img_path in sample_images:\n",
        "        img_id = img_path.stem\n",
        "        if img_id in pipeline.ground_truth_mapping:\n",
        "            sample_image = img_path\n",
        "            break\n",
        "    \n",
        "    if sample_image is None:\n",
        "        # If no image with ground truth found, just use the first image\n",
        "        sample_image = sample_images[0]\n",
        "        print(f\"‚ö†Ô∏è No image with ground truth found in first 5 images, using: {sample_image.name}\")\n",
        "    else:\n",
        "        print(f\"‚úÖ Selected test image: {sample_image.name}\")\n",
        "    \n",
        "    # Get ground truth for the sample image\n",
        "    sample_id = sample_image.stem\n",
        "    ground_truth = pipeline.ground_truth_mapping.get(sample_id, \"Unknown\")\n",
        "    print(f\"üìã Ground truth value: '{ground_truth}'\")\n",
        "    \n",
        "    # Create a test item\n",
        "    test_item = {\n",
        "        \"image_id\": sample_id,\n",
        "        \"image_path\": str(sample_image),\n",
        "        \"ground_truth\": ground_truth\n",
        "    }\n",
        "    \n",
        "    # Get the model and processor from pipeline or load them\n",
        "    if pipeline.model is None or pipeline.processor is None:\n",
        "        print(\"‚è≥ Loading model for test...\")\n",
        "        model, processor = pipeline.setup_model(model_name=pipeline.config.get(\"model_name\", \"pixtral-12b\"))\n",
        "    else:\n",
        "        model, processor = pipeline.model, pipeline.processor\n",
        "    \n",
        "    # Get the prompt\n",
        "    if pipeline_prompt is None:\n",
        "        print(\"‚è≥ Getting prompt from registry...\")\n",
        "        prompt = pipeline.get_experiment_prompt()\n",
        "    else:\n",
        "        prompt = pipeline_prompt\n",
        "    \n",
        "    print(f\"üîç Running extraction with prompt: {getattr(prompt, 'name', 'custom')}\")\n",
        "    \n",
        "    # Import the single-image processor\n",
        "    from src.execution.inference import process_image_with_metrics\n",
        "    \n",
        "    # Process the single image\n",
        "    print(\"‚è≥ Processing image... (This may take a few seconds)\")\n",
        "    result = process_image_with_metrics(\n",
        "        image_path=test_item[\"image_path\"],\n",
        "        ground_truth=test_item[\"ground_truth\"],\n",
        "        prompt=prompt,\n",
        "        model_name=pipeline.config.get(\"model_name\", \"pixtral-12b\"),\n",
        "        field_type=pipeline.config.get(\"field_to_extract\", \"work_order\"),\n",
        "        model=model,\n",
        "        processor=processor,\n",
        "        metrics=[\"exact_match\", \"character_error_rate\"]\n",
        "    )\n",
        "    \n",
        "    # Display detailed results\n",
        "    print(\"\\nüìä Extraction Results:\")\n",
        "    print(f\"   Raw extraction: '{result['raw_extraction']}'\")\n",
        "    print(f\"   Processed text: '{result['processed_extraction']}'\")\n",
        "    print(f\"   Ground truth:   '{result['ground_truth']}'\")\n",
        "    print(f\"   Exact match:     {result['exact_match']}\")\n",
        "    print(f\"   Character error: {result['character_error_rate']:.4f}\")\n",
        "    print(f\"   Processing time: {result['processing_time']:.2f} seconds\")\n",
        "    \n",
        "    # Save the test result\n",
        "    test_result_path = paths.get_results_path(\"single_image_test_result.json\")\n",
        "    with open(test_result_path, \"w\") as f:\n",
        "        json.dump(result, f, indent=2)\n",
        "    \n",
        "    print(f\"üíæ Test result saved to {test_result_path}\")\n",
        "    \n",
        "    # Store the test result for later reference\n",
        "    single_test_result = result\n",
        "    \n",
        "    # Provide assessment of test\n",
        "    if result[\"exact_match\"]:\n",
        "        print(\"‚úÖ Test successful! Extraction matches ground truth.\")\n",
        "        print(\"   Ready to proceed with full extraction.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Test extraction does not exactly match ground truth.\")\n",
        "        print(\"   Review the results to determine if this is acceptable.\")\n",
        "        if result[\"character_error_rate\"] < 0.3:\n",
        "            print(\"   Character error rate is relatively low, may still produce useful results.\")\n",
        "        else:\n",
        "            print(\"   Character error rate is high, consider reviewing prompt or model.\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error during single image test: {e}\")\n",
        "    print(\"   Fix the error before proceeding to full extraction.\")\n",
        "    single_test_result = {\"error\": str(e)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Full Test Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full Extraction Run Cell\n",
        "# This cell runs the complete extraction pipeline on all images\n",
        "\n",
        "try:\n",
        "    if pipeline is None:\n",
        "        raise ValueError(\"Pipeline not initialized. Run the pipeline initialization cell first.\")\n",
        "    \n",
        "    # Check if we have at least a successful test before proceeding\n",
        "    if 'single_test_result' not in locals() or 'error' in single_test_result:\n",
        "        print(\"‚ö†Ô∏è Warning: No successful single image test found.\")\n",
        "        proceed = input(\"Do you want to proceed with full extraction anyway? (y/n): \").strip().lower()\n",
        "        if proceed != 'y':\n",
        "            raise ValueError(\"Full extraction aborted by user. Run the single image test first.\")\n",
        "        print(\"Proceeding with full extraction...\")\n",
        "    \n",
        "    print(\"\\nüöÄ Starting full extraction pipeline...\")\n",
        "    \n",
        "    # Determine how many images to process\n",
        "    image_count = len(list(paths.get_image_paths()))\n",
        "    limit = None  # Process all images by default\n",
        "    \n",
        "    # Optional: Uncomment to limit processing to a subset of images\n",
        "    # limit = 10  # Set to None to process all images\n",
        "    \n",
        "    if limit:\n",
        "        print(f\"‚ÑπÔ∏è Processing limited to first {limit} images (out of {image_count} total)\")\n",
        "    else:\n",
        "        print(f\"‚ÑπÔ∏è Processing all {image_count} images\")\n",
        "    \n",
        "    # Set up checkpoint path for resumability\n",
        "    checkpoint_path = paths.get_results_path(\"extraction_checkpoint.json\")\n",
        "    \n",
        "    # Record start time for benchmarking\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Display configuration summary\n",
        "    print(\"\\nüìã Extraction Configuration:\")\n",
        "    print(f\"   Model: {pipeline.config.get('model_name', 'pixtral-12b')}\")\n",
        "    print(f\"   Field: {pipeline.config.get('field_to_extract', 'work_order')}\")\n",
        "    print(f\"   Prompt: {getattr(pipeline_prompt, 'name', 'default')}\")\n",
        "    print(f\"   Checkpointing: {'Enabled' if pipeline.config.get('enable_checkpointing', True) else 'Disabled'}\")\n",
        "    print(f\"   Auto batch size: {'Enabled' if pipeline.config.get('batch_processing', {}).get('auto_batch_size', True) else 'Disabled'}\")\n",
        "    \n",
        "    # Run the extraction pipeline\n",
        "    print(\"\\n‚è≥ Running extraction (this may take some time)...\")\n",
        "    results = pipeline.run_extraction(\n",
        "        field_type=pipeline.config.get(\"field_to_extract\", \"work_order\"),\n",
        "        prompt=pipeline_prompt,\n",
        "        batch_size=None,  # Auto-determine batch size\n",
        "        checkpoint_path=checkpoint_path,\n",
        "        metrics=[\"exact_match\", \"character_error_rate\"],\n",
        "        limit=limit  # Limit the number of images if specified\n",
        "    )\n",
        "    \n",
        "    # Calculate total processing time\n",
        "    total_time = time.time() - start_time\n",
        "    images_processed = len(results)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Extraction complete!\")\n",
        "    print(f\"   Processed {images_processed} images in {total_time:.2f} seconds\")\n",
        "    print(f\"   Average time per image: {total_time/images_processed:.2f} seconds\")\n",
        "    \n",
        "    # Get quick summary statistics\n",
        "    exact_matches = sum(1 for r in results if r.get(\"exact_match\", False))\n",
        "    accuracy = (exact_matches / images_processed * 100) if images_processed > 0 else 0\n",
        "    \n",
        "    print(f\"\\nüìä Quick Results Summary:\")\n",
        "    print(f\"   Exact match accuracy: {accuracy:.2f}% ({exact_matches}/{images_processed})\")\n",
        "    \n",
        "    # Store results for later analysis\n",
        "    extraction_results = results\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error during extraction: {e}\")\n",
        "    print(\"   Check the error and previous cells before retrying\")\n",
        "    extraction_results = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Report on any error in processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# After running the extraction\n",
        "if extraction_results:\n",
        "    # Check for errors in results\n",
        "    errors = [r for r in extraction_results if \"error\" in r]\n",
        "    if errors:\n",
        "        print(f\"\\n‚ö†Ô∏è Warning: {len(errors)} images failed during extraction\")\n",
        "        print(\"   Top 3 errors:\")\n",
        "        for i, error in enumerate(errors[:3]):\n",
        "            print(f\"      {i+1}. Image: {error.get('image_id', 'unknown')}\")\n",
        "            print(f\"         Error: {error.get('error', 'Unknown error')}\")\n",
        "    else:\n",
        "        print(\"‚úÖ All images processed successfully without errors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis\n",
        "Basic Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Results Analysis Cell\n",
        "# This cell analyzes the extraction results and computes summary metrics\n",
        "\n",
        "try:\n",
        "    if 'extraction_results' not in locals() or not extraction_results:\n",
        "        raise ValueError(\"No extraction results found. Run the extraction cell first.\")\n",
        "    \n",
        "    print(\"üìä Analyzing extraction results...\")\n",
        "    \n",
        "    # If the pipeline has results, use those - otherwise use the extraction_results variable\n",
        "    results = pipeline.results if hasattr(pipeline, 'results') and pipeline.results else extraction_results\n",
        "    \n",
        "    # Get the field type and model name for reference\n",
        "    field_type = pipeline.config.get(\"field_to_extract\", \"work_order\")\n",
        "    model_name = pipeline.config.get(\"model_name\", \"pixtral-12b\")\n",
        "    \n",
        "    # Calculate basic metrics\n",
        "    total_images = len(results)\n",
        "    exact_matches = sum(1 for r in results if r.get(\"exact_match\", False))\n",
        "    exact_match_accuracy = (exact_matches / total_images * 100) if total_images > 0 else 0\n",
        "    \n",
        "    # Calculate character error rate statistics\n",
        "    all_cer = [r.get(\"character_error_rate\", 1.0) for r in results]\n",
        "    avg_cer = sum(all_cer) / total_images if total_images > 0 else 1.0\n",
        "    min_cer = min(all_cer) if all_cer else 1.0\n",
        "    max_cer = max(all_cer) if all_cer else 1.0\n",
        "    \n",
        "    # Calculate timing statistics\n",
        "    all_times = [r.get(\"processing_time\", 0.0) for r in results]\n",
        "    avg_time = sum(all_times) / total_images if total_images > 0 else 0.0\n",
        "    min_time = min(all_times) if all_times else 0.0\n",
        "    max_time = max(all_times) if all_times else 0.0\n",
        "    total_time = sum(all_times)\n",
        "    \n",
        "    # Identify errors\n",
        "    errors = [r for r in results if \"error\" in r]\n",
        "    error_count = len(errors)\n",
        "    \n",
        "    # Find examples of failures (non-exact matches with highest CER)\n",
        "    failures = [r for r in results if not r.get(\"exact_match\", False) and \"error\" not in r]\n",
        "    failures.sort(key=lambda x: x.get(\"character_error_rate\", 0.0), reverse=True)  # Sort by highest CER\n",
        "    \n",
        "    # Find examples of successes\n",
        "    successes = [r for r in results if r.get(\"exact_match\", True) and \"error\" not in r]\n",
        "    \n",
        "    # Create comprehensive summary\n",
        "    summary = {\n",
        "        \"experiment_name\": pipeline.experiment_name,\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"model_name\": model_name,\n",
        "        \"field_type\": field_type,\n",
        "        \"accuracy\": {\n",
        "            \"total_images\": total_images,\n",
        "            \"exact_match_count\": exact_matches,\n",
        "            \"exact_match_accuracy\": round(exact_match_accuracy, 2),\n",
        "            \"error_count\": error_count\n",
        "        },\n",
        "        \"character_error_rate\": {\n",
        "            \"average\": round(avg_cer, 4),\n",
        "            \"minimum\": round(min_cer, 4),\n",
        "            \"maximum\": round(max_cer, 4)\n",
        "        },\n",
        "        \"timing\": {\n",
        "            \"average_seconds\": round(avg_time, 2),\n",
        "            \"minimum_seconds\": round(min_time, 2),\n",
        "            \"maximum_seconds\": round(max_time, 2),\n",
        "            \"total_seconds\": round(total_time, 2)\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Print summary report\n",
        "    print(\"\\nüìà Extraction Results Summary:\")\n",
        "    print(f\"   Model: {model_name}\")\n",
        "    print(f\"   Field: {field_type}\")\n",
        "    print(f\"   Total images processed: {total_images}\")\n",
        "    print(f\"   Exact match accuracy: {exact_match_accuracy:.2f}% ({exact_matches}/{total_images})\")\n",
        "    print(f\"   Average character error rate: {avg_cer:.4f}\")\n",
        "    print(f\"   Processing errors: {error_count}\")\n",
        "    print(f\"   Average processing time: {avg_time:.2f} seconds per image\")\n",
        "    print(f\"   Total processing time: {total_time:.2f} seconds\")\n",
        "    \n",
        "    # Display examples of failures and successes\n",
        "    print(\"\\nüîç Analysis of Non-Exact Matches:\")\n",
        "    if failures:\n",
        "        print(f\"   Found {len(failures)} non-exact matches\")\n",
        "        print(\"   Top 3 worst extractions:\")\n",
        "        for i, failure in enumerate(failures[:3]):\n",
        "            print(f\"      {i+1}. Image: {failure.get('image_id', 'unknown')}\")\n",
        "            print(f\"         Ground truth:   '{failure.get('ground_truth', '')}'\")\n",
        "            print(f\"         Extracted text: '{failure.get('processed_extraction', '')}'\")\n",
        "            print(f\"         Character error rate: {failure.get('character_error_rate', 1.0):.4f}\")\n",
        "    else:\n",
        "        print(\"   No non-exact matches found! Perfect accuracy. üéâ\")\n",
        "    \n",
        "    # Save detailed results to raw directory\n",
        "    os.makedirs(paths.raw_dir, exist_ok=True)\n",
        "    raw_results_path = paths.get_raw_path(f\"{model_name}_{field_type}_results.json\")\n",
        "    with open(raw_results_path, \"w\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    print(f\"\\nüíæ Raw results saved to {raw_results_path}\")\n",
        "    \n",
        "    # Save summary to processed directory\n",
        "    os.makedirs(paths.processed_dir, exist_ok=True)\n",
        "    summary_path = paths.get_processed_path(f\"{model_name}_{field_type}_summary.json\")\n",
        "    with open(summary_path, \"w\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    print(f\"üíæ Summary saved to {summary_path}\")\n",
        "    \n",
        "    # Save failures analysis to processed directory\n",
        "    if failures:\n",
        "        failures_path = paths.get_processed_path(f\"{model_name}_{field_type}_failures.json\")\n",
        "        with open(failures_path, \"w\") as f:\n",
        "            json.dump({\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"model_name\": model_name,\n",
        "                \"field_type\": field_type,\n",
        "                \"total_failures\": len(failures),\n",
        "                \"failures\": failures[:10]  # Save top 10 worst failures\n",
        "            }, f, indent=2)\n",
        "        print(f\"üíæ Failure analysis saved to {failures_path}\")\n",
        "    \n",
        "    # Store analysis results for later use\n",
        "    analysis_results = {\n",
        "        \"summary\": summary,\n",
        "        \"failures\": failures,\n",
        "        \"successes\": successes,\n",
        "        \"errors\": errors\n",
        "    }\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error analyzing results: {e}\")\n",
        "    print(\"   Check that extraction results are available and valid\")\n",
        "    analysis_results = {\"error\": str(e)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization Cell\n",
        "# This cell creates visualizations of the extraction results\n",
        "\n",
        "try:\n",
        "    if 'analysis_results' not in locals() or 'summary' not in analysis_results:\n",
        "        raise ValueError(\"No analysis results found. Run the analysis cell first.\")\n",
        "    \n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from matplotlib.colors import LinearSegmentedColormap\n",
        "    \n",
        "    print(\"üìä Creating visualizations of extraction results...\")\n",
        "    \n",
        "    # Get key information from analysis\n",
        "    summary = analysis_results[\"summary\"]\n",
        "    total_images = summary[\"accuracy\"][\"total_images\"]\n",
        "    exact_matches = summary[\"accuracy\"][\"exact_match_count\"]\n",
        "    model_name = summary[\"model_name\"]\n",
        "    field_type = summary[\"field_type\"]\n",
        "    \n",
        "    # Create visualizations directory if it doesn't exist\n",
        "    os.makedirs(paths.visualizations_dir, exist_ok=True)\n",
        "    \n",
        "    # 1. Create accuracy bar chart\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    categories = ['Exact Match', 'Non-Match']\n",
        "    counts = [exact_matches, total_images - exact_matches]\n",
        "    colors = ['#2ecc71', '#e74c3c']\n",
        "    \n",
        "    plt.bar(categories, counts, color=colors)\n",
        "    plt.title(f'Extraction Accuracy for {field_type} using {model_name}', fontsize=14)\n",
        "    plt.ylabel('Number of Images', fontsize=12)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    \n",
        "    # Add count and percentage labels on bars\n",
        "    for i, count in enumerate(counts):\n",
        "        percentage = (count / total_images) * 100\n",
        "        plt.text(i, count/2, f\"{count}\\n({percentage:.1f}%)\", \n",
        "                 ha='center', va='center', color='white', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    accuracy_chart_path = paths.get_visualization_path(\"accuracy_chart.png\")\n",
        "    plt.savefig(accuracy_chart_path)\n",
        "    plt.close()\n",
        "    \n",
        "    # 2. Create character error rate histogram\n",
        "    if 'extraction_results' in locals() and extraction_results:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        all_cer = [r.get(\"character_error_rate\", 1.0) for r in extraction_results \n",
        "                  if \"error\" not in r]  # Exclude error cases\n",
        "        \n",
        "        plt.hist(all_cer, bins=20, color='#3498db', alpha=0.7, edgecolor='black')\n",
        "        plt.title(f'Character Error Rate Distribution for {field_type}', fontsize=14)\n",
        "        plt.xlabel('Character Error Rate', fontsize=12)\n",
        "        plt.ylabel('Number of Images', fontsize=12)\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "        plt.axvline(x=np.mean(all_cer), color='#e74c3c', linestyle='--', \n",
        "                    label=f'Mean: {np.mean(all_cer):.4f}')\n",
        "        plt.legend()\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        cer_chart_path = paths.get_visualization_path(\"character_error_rate_histogram.png\")\n",
        "        plt.savefig(cer_chart_path)\n",
        "        plt.close()\n",
        "    \n",
        "    # 3. Create processing time chart\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    all_times = [r.get(\"processing_time\", 0.0) for r in extraction_results \n",
        "                if \"error\" not in r]  # Exclude error cases\n",
        "    \n",
        "    plt.hist(all_times, bins=20, color='#9b59b6', alpha=0.7, edgecolor='black')\n",
        "    plt.title(f'Processing Time Distribution for {field_type}', fontsize=14)\n",
        "    plt.xlabel('Processing Time (seconds)', fontsize=12)\n",
        "    plt.ylabel('Number of Images', fontsize=12)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "    plt.axvline(x=np.mean(all_times), color='#e74c3c', linestyle='--', \n",
        "                label=f'Mean: {np.mean(all_times):.2f}s')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    time_chart_path = paths.get_visualization_path(\"processing_time_histogram.png\")\n",
        "    plt.savefig(time_chart_path)\n",
        "    plt.close()\n",
        "    \n",
        "    # 4. Create comparison table of failures\n",
        "    if analysis_results.get(\"failures\"):\n",
        "        top_failures = analysis_results[\"failures\"][:10]  # Top 10 failures\n",
        "        \n",
        "        failure_data = []\n",
        "        for f in top_failures:\n",
        "            failure_data.append({\n",
        "                \"Image ID\": f.get(\"image_id\", \"unknown\"),\n",
        "                \"Ground Truth\": f.get(\"ground_truth\", \"\"),\n",
        "                \"Extraction\": f.get(\"processed_extraction\", \"\"),\n",
        "                \"CER\": f.get(\"character_error_rate\", 1.0)\n",
        "            })\n",
        "        \n",
        "        failure_df = pd.DataFrame(failure_data)\n",
        "        \n",
        "        # Create a visually appealing HTML table\n",
        "        html_table = failure_df.to_html(index=False, classes=\"table table-striped\")\n",
        "        styled_html = f\"\"\"\n",
        "        <html>\n",
        "        <head>\n",
        "            <style>\n",
        "                .table {{\n",
        "                    width: 100%;\n",
        "                    border-collapse: collapse;\n",
        "                    font-family: Arial, sans-serif;\n",
        "                }}\n",
        "                .table-striped tr:nth-child(even) {{\n",
        "                    background-color: #f2f2f2;\n",
        "                }}\n",
        "                th {{\n",
        "                    background-color: #4CAF50;\n",
        "                    color: white;\n",
        "                    padding: 12px;\n",
        "                    text-align: left;\n",
        "                }}\n",
        "                td {{\n",
        "                    padding: 8px;\n",
        "                    text-align: left;\n",
        "                    border-bottom: 1px solid #ddd;\n",
        "                }}\n",
        "                .title {{\n",
        "                    text-align: center;\n",
        "                    font-size: 24px;\n",
        "                    margin: 20px 0;\n",
        "                    font-family: Arial, sans-serif;\n",
        "                }}\n",
        "            </style>\n",
        "        </head>\n",
        "        <body>\n",
        "            <div class=\"title\">Top Extraction Failures</div>\n",
        "            {html_table}\n",
        "        </body>\n",
        "        </html>\n",
        "        \"\"\"\n",
        "        \n",
        "        failure_table_path = paths.get_visualization_path(\"top_failures.html\")\n",
        "        with open(failure_table_path, \"w\") as f:\n",
        "            f.write(styled_html)\n",
        "    \n",
        "    # 5. Create model info visualization if metadata is available\n",
        "    if any(\"metadata\" in r for r in extraction_results):\n",
        "        # Try to gather memory usage data across records\n",
        "        memory_data = []\n",
        "        for r in extraction_results:\n",
        "            if \"metadata\" in r and \"gpu_info\" in r[\"metadata\"]:\n",
        "                gpu_info = r[\"metadata\"][\"gpu_info\"]\n",
        "                if \"allocated_memory_gb\" in gpu_info and \"total_memory_gb\" in gpu_info:\n",
        "                    memory_data.append({\n",
        "                        \"image_id\": r.get(\"image_id\", \"unknown\"),\n",
        "                        \"allocated_gb\": gpu_info[\"allocated_memory_gb\"],\n",
        "                        \"total_gb\": gpu_info[\"total_memory_gb\"],\n",
        "                        \"utilization\": (gpu_info[\"allocated_memory_gb\"] / gpu_info[\"total_memory_gb\"]) * 100\n",
        "                    })\n",
        "        \n",
        "        if memory_data:\n",
        "            # Create memory usage chart\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            df = pd.DataFrame(memory_data)\n",
        "            plt.plot(range(len(df)), df[\"allocated_gb\"], 'b-', label=\"Allocated Memory (GB)\")\n",
        "            plt.axhline(y=df[\"total_gb\"].iloc[0], color='r', linestyle='--', \n",
        "                       label=f\"Total GPU Memory: {df['total_gb'].iloc[0]:.1f} GB\")\n",
        "            plt.xlabel(\"Image Index\")\n",
        "            plt.ylabel(\"GPU Memory (GB)\")\n",
        "            plt.title(f\"GPU Memory Usage During {model_name} Extraction\")\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.tight_layout()\n",
        "            \n",
        "            memory_chart_path = paths.get_visualization_path(\"gpu_memory_usage.png\")\n",
        "            plt.savefig(memory_chart_path)\n",
        "            plt.close()\n",
        "    \n",
        "    # 6. Create a summary dashboard HTML file\n",
        "    dashboard_html = f\"\"\"\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>Extraction Results Dashboard - {model_name}</title>\n",
        "        <style>\n",
        "            body {{\n",
        "                font-family: Arial, sans-serif;\n",
        "                margin: 0;\n",
        "                padding: 20px;\n",
        "                background-color: #f5f5f5;\n",
        "            }}\n",
        "            .container {{\n",
        "                max-width: 1200px;\n",
        "                margin: 0 auto;\n",
        "                background-color: white;\n",
        "                padding: 20px;\n",
        "                border-radius: 5px;\n",
        "                box-shadow: 0 0 10px rgba(0,0,0,0.1);\n",
        "            }}\n",
        "            h1, h2 {{\n",
        "                color: #333;\n",
        "            }}\n",
        "            .metrics {{\n",
        "                display: flex;\n",
        "                flex-wrap: wrap;\n",
        "                margin: 20px 0;\n",
        "            }}\n",
        "            .metric-card {{\n",
        "                background-color: white;\n",
        "                border-radius: 5px;\n",
        "                box-shadow: 0 0 5px rgba(0,0,0,0.1);\n",
        "                padding: 15px;\n",
        "                margin: 10px;\n",
        "                min-width: 200px;\n",
        "                flex: 1;\n",
        "            }}\n",
        "            .metric-title {{\n",
        "                font-size: 14px;\n",
        "                color: #666;\n",
        "                margin-bottom: 5px;\n",
        "            }}\n",
        "            .metric-value {{\n",
        "                font-size: 24px;\n",
        "                font-weight: bold;\n",
        "                color: #333;\n",
        "            }}\n",
        "            .chart-container {{\n",
        "                margin: 20px 0;\n",
        "                text-align: center;\n",
        "            }}\n",
        "            .chart {{\n",
        "                max-width: 100%;\n",
        "                height: auto;\n",
        "                border: 1px solid #ddd;\n",
        "                border-radius: 5px;\n",
        "            }}\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <div class=\"container\">\n",
        "            <h1>Extraction Results Dashboard</h1>\n",
        "            <p>\n",
        "                <strong>Model:</strong> {model_name}<br>\n",
        "                <strong>Field:</strong> {field_type}<br>\n",
        "                <strong>Date:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}<br>\n",
        "                <strong>Total Images:</strong> {total_images}\n",
        "            </p>\n",
        "            \n",
        "            <div class=\"metrics\">\n",
        "                <div class=\"metric-card\">\n",
        "                    <div class=\"metric-title\">Accuracy</div>\n",
        "                    <div class=\"metric-value\">{summary[\"accuracy\"][\"exact_match_accuracy\"]}%</div>\n",
        "                </div>\n",
        "                <div class=\"metric-card\">\n",
        "                    <div class=\"metric-title\">Character Error Rate</div>\n",
        "                    <div class=\"metric-value\">{summary[\"character_error_rate\"][\"average\"]}</div>\n",
        "                </div>\n",
        "                <div class=\"metric-card\">\n",
        "                    <div class=\"metric-title\">Avg Processing Time</div>\n",
        "                    <div class=\"metric-value\">{summary[\"timing\"][\"average_seconds\"]}s</div>\n",
        "                </div>\n",
        "                <div class=\"metric-card\">\n",
        "                    <div class=\"metric-title\">Total Processing Time</div>\n",
        "                    <div class=\"metric-value\">{summary[\"timing\"][\"total_seconds\"]}s</div>\n",
        "                </div>\n",
        "            </div>\n",
        "            \n",
        "            <h2>Visualizations</h2>\n",
        "            \n",
        "            <div class=\"chart-container\">\n",
        "                <h3>Accuracy</h3>\n",
        "                <img class=\"chart\" src=\"accuracy_chart.png\" alt=\"Accuracy Chart\">\n",
        "            </div>\n",
        "            \n",
        "            <div class=\"chart-container\">\n",
        "                <h3>Character Error Rate Distribution</h3>\n",
        "                <img class=\"chart\" src=\"character_error_rate_histogram.png\" alt=\"CER Distribution\">\n",
        "            </div>\n",
        "            \n",
        "            <div class=\"chart-container\">\n",
        "                <h3>Processing Time Distribution</h3>\n",
        "                <img class=\"chart\" src=\"processing_time_histogram.png\" alt=\"Processing Time Distribution\">\n",
        "            </div>\n",
        "        </div>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    \n",
        "    dashboard_path = paths.get_visualization_path(\"dashboard.html\")\n",
        "    with open(dashboard_path, \"w\") as f:\n",
        "        f.write(dashboard_html)\n",
        "    \n",
        "    # Display a summary of created visualizations\n",
        "    print(\"\\n‚úÖ Created the following visualizations:\")\n",
        "    print(f\"   1. Accuracy Bar Chart: {accuracy_chart_path}\")\n",
        "    if 'cer_chart_path' in locals():\n",
        "        print(f\"   2. Character Error Rate Histogram: {cer_chart_path}\")\n",
        "    print(f\"   3. Processing Time Histogram: {time_chart_path}\")\n",
        "    if 'failure_table_path' in locals():\n",
        "        print(f\"   4. Top Failures Table: {failure_table_path}\")\n",
        "    if 'memory_chart_path' in locals():\n",
        "        print(f\"   5. GPU Memory Usage Chart: {memory_chart_path}\")\n",
        "    print(f\"   6. Results Dashboard: {dashboard_path}\")\n",
        "    \n",
        "    # Store visualization paths for reference\n",
        "    visualization_paths = {\n",
        "        \"accuracy_chart\": str(accuracy_chart_path),\n",
        "        \"processing_time_histogram\": str(time_chart_path),\n",
        "        \"dashboard\": str(dashboard_path)\n",
        "    }\n",
        "    \n",
        "    if 'cer_chart_path' in locals():\n",
        "        visualization_paths[\"character_error_rate_histogram\"] = str(cer_chart_path)\n",
        "    \n",
        "    if 'failure_table_path' in locals():\n",
        "        visualization_paths[\"top_failures_table\"] = str(failure_table_path)\n",
        "    \n",
        "    if 'memory_chart_path' in locals():\n",
        "        visualization_paths[\"gpu_memory_usage\"] = str(memory_chart_path)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error creating visualizations: {e}\")\n",
        "    print(\"   Check that analysis results are available and valid\")\n",
        "    visualization_paths = {\"error\": str(e)}"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
