{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Model, Single Prompt Test\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates the extraction of work order numbers from invoice images using a single model (Pixtral-12B) and a single prompt approach. It serves as a replication of the successful RunPod experiment using our new structured framework.\n",
    "\n",
    "### Objectives\n",
    "- Load the Pixtral-12B model using our model management framework\n",
    "- Apply a specific prompt that was effective in previous tests\n",
    "- Process invoice images to extract work order numbers\n",
    "- Evaluate extraction accuracy against ground truth data\n",
    "- Establish a baseline for further prompt and model experimentation\n",
    "\n",
    "### Approach\n",
    "We'll use the basic prompt that worked well in our initial tests:\n",
    "```\n",
    "Extract the work order number from this invoice image.\n",
    "```\n",
    "\n",
    "This will be formatted appropriately for the Pixtral model using our prompt management system.\n",
    "\n",
    "### Expected Results\n",
    "- Extraction accuracy metrics (exact match rate, character error rate)\n",
    "- Processing time analysis\n",
    "- Comparison with previous RunPod experiment results\n",
    "- Foundation for systematic prompt and model comparisons\n",
    "\n",
    "This notebook represents Step 1 in our experimental workflow, focusing on reproducing known successful results before expanding to prompt comparison (Step 2) and model comparison (Step 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Environment Verification\n",
    "### Environment Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import torch\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure basic logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('invoice_extraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Environment Setup and Verification\n",
      "üêç Python version: 3.10.12\n",
      "üìä PyTorch version: 2.0.1+cu118\n",
      "ü§ñ Transformers version: 4.49.0\n",
      "üìÖ Date: 2025-03-18 00:21:36\n",
      "üñ•Ô∏è GPU: NVIDIA H200 (150.02 GB)\n",
      "üîç CUDA Version: 11.8\n",
      "üíæ GPU Memory: 0.00 GB allocated, 0.00 GB reserved\n",
      "‚úÖ GPU memory sufficient for model loading\n",
      "‚òÅÔ∏è RunPod environment detected\n",
      "üíª System: Linux 6.8.0-53-generic\n",
      "üìÇ Working directory: /workspace/notebooks\n"
     ]
    }
   ],
   "source": [
    "# Define function to check if we're in a RunPod environment\n",
    "def is_runpod():\n",
    "    \"\"\"Check if we're running in a RunPod environment\"\"\"\n",
    "    return (\n",
    "        os.environ.get(\"RUNPOD_POD_ID\") is not None or \n",
    "        \"A100\" in os.environ.get(\"GPU_NAME\", \"\") or\n",
    "        \"H100\" in os.environ.get(\"GPU_NAME\", \"\")\n",
    "    )\n",
    "\n",
    "# Display basic system information\n",
    "print(f\"üîç Environment Setup and Verification\")\n",
    "print(f\"üêç Python version: {platform.python_version()}\")\n",
    "print(f\"üìä PyTorch version: {torch.__version__}\")\n",
    "print(f\"ü§ñ Transformers version: {transformers.__version__}\")\n",
    "print(f\"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"üñ•Ô∏è GPU: {device_name} ({memory:.2f} GB)\")\n",
    "    print(f\"üîç CUDA Version: {torch.version.cuda}\")\n",
    "    \n",
    "    # Display GPU memory usage\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    print(f\"üíæ GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
    "    \n",
    "    # Basic GPU compatibility check\n",
    "    if memory < 24:\n",
    "        print(\"‚ö†Ô∏è Warning: GPU memory may be insufficient for Pixtral-12B without quantization\")\n",
    "    else:\n",
    "        print(\"‚úÖ GPU memory sufficient for model loading\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected - running in CPU mode\")\n",
    "    print(\"   Note: Processing will be extremely slow without GPU acceleration\")\n",
    "\n",
    "# Identify environment type\n",
    "if is_runpod():\n",
    "    print(\"‚òÅÔ∏è RunPod environment detected\")\n",
    "else:\n",
    "    print(\"üíª Local environment detected\")\n",
    "\n",
    "# Display platform details for debugging\n",
    "print(f\"üíª System: {platform.system()} {platform.release()}\")\n",
    "print(f\"üìÇ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found project root: /workspace\n",
      "üìå Added /workspace to Python path\n",
      "üîÑ Set PROJECT_ROOT environment variable to /workspace\n",
      "‚úÖ Directory exists: configs/environments\n",
      "‚úÖ Directory exists: configs/prompts\n",
      "‚úÖ Directory exists: src/config\n",
      "‚úÖ Directory exists: src/models\n",
      "‚úÖ Directory exists: src/prompts\n",
      "‚úÖ Directory exists: src/execution\n",
      "‚úÖ Directory exists: src/results\n",
      "‚úÖ Directory exists: src/analysis\n",
      "‚úÖ Directory exists: data/images\n",
      "‚úÖ Directory exists: models/cache\n",
      "‚úÖ Directory exists: results/raw\n",
      "‚úÖ Directory exists: results/processed\n",
      "‚úÖ Directory exists: results/visualizations\n",
      "‚úÖ Directory exists: logs\n",
      "‚úÖ Ground truth data file found\n",
      "‚úÖ Found 20 image files in /workspace/data/images\n",
      "üìä Created results directory for this run: /workspace/results/pixtral_extraction_20250318_002223\n",
      "\n",
      "üìã Path configuration complete!\n",
      "   Project root: /workspace\n",
      "   Data directory: /workspace/data\n",
      "   Results for this run: /workspace/results/pixtral_extraction_20250318_002223\n"
     ]
    }
   ],
   "source": [
    "# Project Path Configuration\n",
    "# Set up project paths - This cell finds the project root and initializes path configuration\n",
    "\n",
    "# First, try to find the project root directory\n",
    "notebook_path = Path().resolve()\n",
    "project_root = None\n",
    "\n",
    "# Look for project root markers (traversing upward from the notebook)\n",
    "for parent in [notebook_path] + list(notebook_path.parents):\n",
    "    if any((parent / marker).exists() for marker in ['.git', 'setup.py', 'requirements.txt']):\n",
    "        project_root = parent\n",
    "        break\n",
    "\n",
    "if project_root is None:\n",
    "    # If markers not found, assume we're in a subdirectory of the project\n",
    "    # and the parent directory is the project root\n",
    "    project_root = notebook_path.parent\n",
    "    print(\"‚ö†Ô∏è Could not definitively locate project root, using parent directory\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found project root: {project_root}\")\n",
    "\n",
    "# Add project root to Python path if not already there\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    print(f\"üìå Added {project_root} to Python path\")\n",
    "\n",
    "# Set environment variable for project root\n",
    "os.environ['PROJECT_ROOT'] = str(project_root)\n",
    "print(f\"üîÑ Set PROJECT_ROOT environment variable to {project_root}\")\n",
    "\n",
    "# Verify critical directories exist, create if needed\n",
    "critical_dirs = [\n",
    "    \"configs/environments\",\n",
    "    \"configs/prompts\",\n",
    "    \"src/config\",\n",
    "    \"src/models\",\n",
    "    \"src/prompts\",\n",
    "    \"src/execution\",\n",
    "    \"src/results\",\n",
    "    \"src/analysis\",\n",
    "    \"data/images\",\n",
    "    \"models/cache\",\n",
    "    \"results/raw\",\n",
    "    \"results/processed\",\n",
    "    \"results/visualizations\",\n",
    "    \"logs\"\n",
    "]\n",
    "\n",
    "for dir_path in critical_dirs:\n",
    "    full_path = project_root / dir_path\n",
    "    if not full_path.exists():\n",
    "        print(f\"üìÅ Creating directory: {dir_path}\")\n",
    "        full_path.mkdir(parents=True, exist_ok=True)\n",
    "    else:\n",
    "        print(f\"‚úÖ Directory exists: {dir_path}\")\n",
    "\n",
    "# Check for ground truth data file\n",
    "ground_truth_path = project_root / \"data\" / \"ground_truth.csv\"\n",
    "if os.path.exists(ground_truth_path):\n",
    "    print(f\"‚úÖ Ground truth data file found\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Ground truth data file not found at: {ground_truth_path}\")\n",
    "    print(\"   This will be needed for comparing extraction results.\")\n",
    "\n",
    "# Check for image files\n",
    "image_dir = project_root / \"data\" / \"images\"\n",
    "image_files = list(image_dir.glob(\"*.jpg\")) + list(image_dir.glob(\"*.png\"))\n",
    "if image_files:\n",
    "    print(f\"‚úÖ Found {len(image_files)} image files in {image_dir}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No image files found in {image_dir}\")\n",
    "    print(\"   This will be needed for extraction testing.\")\n",
    "\n",
    "# Set up results directory for this run with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_name = f\"prompt_comparison_{timestamp}\"\n",
    "results_dir = project_root / \"results\" / experiment_name\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"üìä Created results directory for this run: {results_dir}\")\n",
    "\n",
    "# Create subdirectories in results directory\n",
    "for subdir in [\"raw\", \"processed\", \"visualizations\"]:\n",
    "    (results_dir / subdir).mkdir(exist_ok=True)\n",
    "\n",
    "# Define path utility functions\n",
    "def get_results_path(filename):\n",
    "    \"\"\"Get full path for a results file\"\"\"\n",
    "    return results_dir / filename\n",
    "\n",
    "def get_raw_path(filename):\n",
    "    \"\"\"Get full path for a raw results file\"\"\"\n",
    "    return results_dir / \"raw\" / filename\n",
    "\n",
    "def get_processed_path(filename):\n",
    "    \"\"\"Get full path for a processed results file\"\"\"\n",
    "    return results_dir / \"processed\" / filename\n",
    "\n",
    "def get_visualization_path(filename):\n",
    "    \"\"\"Get full path for a visualization file\"\"\"\n",
    "    return results_dir / \"visualizations\" / filename\n",
    "\n",
    "def get_image_paths():\n",
    "    \"\"\"Get all image paths from the images directory\"\"\"\n",
    "    return list(image_dir.glob(\"*.jpg\")) + list(image_dir.glob(\"*.png\"))\n",
    "\n",
    "# Define paths dictionary for easy access\n",
    "paths = {\n",
    "    \"project_root\": project_root,\n",
    "    \"data_dir\": project_root / \"data\",\n",
    "    \"images_dir\": image_dir,\n",
    "    \"ground_truth_path\": ground_truth_path,\n",
    "    \"models_cache_dir\": project_root / \"models\" / \"cache\",\n",
    "    \"results_dir\": results_dir,\n",
    "    \"raw_results_dir\": results_dir / \"raw\",\n",
    "    \"processed_results_dir\": results_dir / \"processed\",\n",
    "    \"visualizations_dir\": results_dir / \"visualizations\",\n",
    "    \"get_results_path\": get_results_path,\n",
    "    \"get_raw_path\": get_raw_path,\n",
    "    \"get_processed_path\": get_processed_path,\n",
    "    \"get_visualization_path\": get_visualization_path,\n",
    "    \"get_image_paths\": get_image_paths\n",
    "}\n",
    "\n",
    "print(f\"\\nüìã Path configuration complete!\")\n",
    "print(f\"   Project root: {paths['project_root']}\")\n",
    "print(f\"   Data directory: {paths['data_dir']}\")\n",
    "print(f\"   Results for this run: {paths['results_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Core Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Any, Optional, Union\n",
    "\n",
    "# Data processing and visualization libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from Levenshtein import distance\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    LlavaForConditionalGeneration,\n",
    "    AutoModelForVision2Seq,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 00:23:54,944 - invoice_extraction - INFO - Starting invoice extraction experiment\n"
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "import os\n",
    "\n",
    "# Create logs directory if it doesn't exist\n",
    "log_dir = \"logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "log_file = f\"{log_dir}/extraction_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),  # Log to console\n",
    "        logging.FileHandler(log_file)  # Log to file\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger('invoice_extraction')\n",
    "logger.info(\"Starting invoice extraction experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Project Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 00:24:35,880 - invoice_extraction - ERROR - Error importing project modules: cannot import name 'optimize_model_memory' from 'src.models.optimization' (/workspace/src/models/optimization.py)\n",
      "2025-03-18 00:24:35,880 - invoice_extraction - WARNING - Some functionality may be limited. Check your project structure and path configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Warning: Failed to import some project modules: cannot import name 'optimize_model_memory' from 'src.models.optimization' (/workspace/src/models/optimization.py)\n",
      "   This may happen if you haven't implemented these modules yet.\n",
      "   The notebook will attempt to continue, but some cells may need modification.\n"
     ]
    }
   ],
   "source": [
    "# Import project-specific modules\n",
    "try:\n",
    "    # Attempt to import required modules from src directory\n",
    "    # These imports assume the project root has been added to sys.path in Cell 1\n",
    "    \n",
    "    # Configuration modules\n",
    "    from src.config.environment import get_environment_config\n",
    "    from src.config.paths import get_path_config, PathConfig\n",
    "    from src.config.experiment import get_experiment_config, ExperimentConfig\n",
    "    \n",
    "    # Model handling modules\n",
    "    from src.models.loader import load_model_and_processor, get_gpu_memory_info\n",
    "    from src.models.registry import get_model_config\n",
    "    from src.models.optimization import optimize_model_memory\n",
    "    \n",
    "    # Prompt management modules\n",
    "    from src.prompts.registry import get_prompt, list_available_prompts, format_prompt\n",
    "    from src.prompts.prompt_utils import analyze_prompt\n",
    "    \n",
    "    # Execution modules\n",
    "    from src.execution.inference import process_image\n",
    "    from src.execution.pipeline import ExtractionPipeline\n",
    "    from src.execution.batch import prepare_batch_items\n",
    "    \n",
    "    # Results and analysis modules\n",
    "    from src.results.collector import ResultsCollector\n",
    "    from src.results.metrics import calculate_metrics\n",
    "    from src.analysis.visualization import create_visualizations\n",
    "    \n",
    "    logger.info(\"Successfully imported project modules\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    logger.error(f\"Error importing project modules: {e}\")\n",
    "    logger.warning(\"Some functionality may be limited. Check your project structure and path configuration.\")\n",
    "    print(f\"‚ö†Ô∏è Warning: Failed to import some project modules: {e}\")\n",
    "    print(\"   This may happen if you haven't implemented these modules yet.\")\n",
    "    print(\"   The notebook will attempt to continue, but some cells may need modification.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Core Libraries Imported:\n",
      "üî¢ NumPy: 1.24.3\n",
      "üìä Pandas: 2.0.3\n",
      "üß† PyTorch: 2.0.1+cu118\n",
      "ü§ñ Transformers: 4.49.0\n"
     ]
    }
   ],
   "source": [
    "# Print summary of imported libraries\n",
    "print(\"üìö Core Libraries Imported:\")\n",
    "print(f\"üî¢ NumPy: {np.__version__}\")\n",
    "print(f\"üìä Pandas: {pd.__version__}\")\n",
    "print(f\"üß† PyTorch: {torch.__version__}\")\n",
    "print(f\"ü§ñ Transformers: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CUDA is available for GPU acceleration\n",
      "üîß Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU acceleration\n",
    "if torch.cuda.is_available():\n",
    "    print(\"‚úÖ CUDA is available for GPU acceleration\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CUDA is not available, using CPU (this will be slow)\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"üîß Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Configuration\n",
    "### Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 00:25:57,872 - src.config.environment - INFO - RunPod environment detected via environment variables\n",
      "2025-03-18 00:25:57,872 - src.config.environment - INFO - Detected environment: runpod\n",
      "2025-03-18 00:25:57,876 - src.config.environment - INFO - Setting up runpod environment\n",
      "2025-03-18 00:25:57,876 - src.config.environment - INFO - Project root: /workspace\n",
      "2025-03-18 00:25:57,877 - src.config.environment - INFO - CUDA version: 11.8\n",
      "2025-03-18 00:25:57,877 - src.config.environment - INFO - Enabled cuDNN benchmark mode for performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Detected environment type: runpod\n",
      "üìå Loaded runpod environment configuration\n",
      "üìÑ Loaded environment settings from /workspace/configs/environments/runpod.yaml\n",
      "  ‚Ü≥ Set environment = {'name': 'runpod', 'type': 'production'}\n",
      "üß™ Initializing experiment: pixtral_extraction_20250318_002223\n",
      "\n",
      "üìã Environment Configuration Summary:\n",
      "Environment: {'name': 'runpod', 'type': 'production'}\n",
      "Project root: /workspace\n",
      "\n",
      "System Information:\n",
      "  OS: Linux 6.8.0-53-generic\n",
      "  Python: 3.10.12\n",
      "\n",
      "GPU Information:\n",
      "  Device: NVIDIA H200\n",
      "  CUDA Version: 11.8\n",
      "  Memory: 150.02 GB\n",
      "\n",
      "Hardware Configuration:\n",
      "  gpu_required: True\n",
      "  gpu_memory_min: 24GB\n",
      "  cuda_version: 11.8\n",
      "\n",
      "Default Model Settings:\n",
      "  precision: bfloat16\n",
      "  device: cuda:0\n",
      "  quantization: 16-bit\n",
      "  batch_size: 1\n",
      "\n",
      "Path Configuration:\n",
      "  base_dir: /workspace\n",
      "  data_dir: /workspace/data\n",
      "  images_dir: /workspace/data/images\n",
      "  ground_truth_path: /workspace/data/ground_truth.csv\n",
      "  results_dir: /workspace/results/raw\n",
      "  model_cache_dir: /cache\n",
      "\n",
      "Dependency Requirements:\n",
      "  transformers: ==4.34.0\n",
      "  torch: ==2.0.1\n",
      "  accelerate: ==0.21.0\n",
      "  sentencepiece: ==0.1.99\n",
      "  tqdm: ==4.66.1\n",
      "  pillow: ==10.0.0\n",
      "  matplotlib: ==3.7.2\n",
      "  pandas: ==2.0.3\n",
      "  python-Levenshtein: ==0.21.1\n",
      "  bitsandbytes: ==0.41.1\n",
      "  numpy: ==1.24.3\n",
      "\n",
      "üß™ Experiment Configuration Summary:\n",
      "  name: pixtral_extraction_20250318_002223\n",
      "  timestamp: 20250318_002223\n",
      "  environment: {'name': 'runpod', 'type': 'production'}\n",
      "  model_name: pixtral-12b\n",
      "  model_repo_id: mistral-community/pixtral-12b\n",
      "  field_to_extract: work_order\n",
      "  use_4bit_quantization: False\n",
      "  use_half_precision: True\n",
      "  max_batch_size: 1\n",
      "  description: Extracting work order numbers from invoice images using Pixtral-12B\n",
      "\n",
      "üíæ Configuration saved to: /workspace/results/pixtral_extraction_20250318_002223/experiment_config.json\n",
      "‚úÖ Environment and experiment configuration complete!\n"
     ]
    }
   ],
   "source": [
    "# Environment Configuration\n",
    "\n",
    "# Import configuration modules if not already imported\n",
    "try:\n",
    "    from src.config.environment import get_environment_config, EnvironmentConfig\n",
    "except ImportError:\n",
    "    # Fallback implementation if modules aren't available yet\n",
    "    logger.warning(\"Using fallback environment configuration\")\n",
    "    \n",
    "    class EnvironmentConfig:\n",
    "        \"\"\"Fallback Environment Configuration class\"\"\"\n",
    "        def __init__(self, env_type=\"auto\"):\n",
    "            self.environment = \"runpod\" if is_runpod() else \"local\"\n",
    "            self.gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n",
    "            self.max_batch_size = 1 if self.environment == \"local\" else 4\n",
    "            self.optimize_memory = True\n",
    "            self.use_4bit_quantization = self.environment == \"runpod\" and self.gpu_memory_gb < 40\n",
    "            self.use_half_precision = True\n",
    "            \n",
    "        def print_summary(self):\n",
    "            \"\"\"Print a summary of the environment configuration\"\"\"\n",
    "            print(f\"Environment: {self.environment}\")\n",
    "            print(f\"GPU Memory: {self.gpu_memory_gb:.2f} GB\")\n",
    "            print(f\"Max Batch Size: {self.max_batch_size}\")\n",
    "            print(f\"Memory Optimization: {'Enabled' if self.optimize_memory else 'Disabled'}\")\n",
    "            print(f\"4-bit Quantization: {'Enabled' if self.use_4bit_quantization else 'Disabled'}\")\n",
    "            print(f\"Half Precision: {'Enabled' if self.use_half_precision else 'Disabled'}\")\n",
    "    \n",
    "    def get_environment_config(env_type=\"auto\"):\n",
    "        \"\"\"Fallback function to get environment configuration\"\"\"\n",
    "        return EnvironmentConfig(env_type)\n",
    "\n",
    "# Detect whether we're in a RunPod or local environment\n",
    "env_type = \"runpod\" if is_runpod() else \"local\"\n",
    "print(f\"üîç Detected environment type: {env_type}\")\n",
    "\n",
    "# Load appropriate configuration\n",
    "try:\n",
    "    # Get environment configuration\n",
    "    env_config = get_environment_config(env_type)\n",
    "    print(f\"üìå Loaded {env_config.environment} environment configuration\")\n",
    "    \n",
    "    # Load environment-specific settings from YAML if available\n",
    "    yaml_config_path = project_root / \"configs\" / \"environments\" / f\"{env_config.environment}.yaml\"\n",
    "    if os.path.exists(yaml_config_path):\n",
    "        import yaml\n",
    "        with open(yaml_config_path, 'r') as f:\n",
    "            yaml_config = yaml.safe_load(f)\n",
    "        print(f\"üìÑ Loaded environment settings from {yaml_config_path}\")\n",
    "        \n",
    "        # Override config settings with YAML values if present\n",
    "        for key, value in yaml_config.items():\n",
    "            if hasattr(env_config, key):\n",
    "                setattr(env_config, key, value)\n",
    "                print(f\"  ‚Ü≥ Set {key} = {value}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è No environment config file found at {yaml_config_path}, using defaults\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading environment configuration: {e}\")\n",
    "    print(\"‚ö†Ô∏è Using default environment settings\")\n",
    "    env_config = EnvironmentConfig(env_type)\n",
    "\n",
    "# Set up GPU memory optimization based on environment config\n",
    "if torch.cuda.is_available() and hasattr(env_config, 'optimize_memory') and env_config.optimize_memory:\n",
    "    # Set up memory optimization for better GPU usage\n",
    "    if hasattr(torch.cuda, 'empty_cache'):\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"üßπ Cleared CUDA cache\")\n",
    "    \n",
    "    # Set memory configuration based on environment\n",
    "    if env_config.environment == \"runpod\":\n",
    "        # More aggressive optimization for RunPod\n",
    "        torch.cuda.set_per_process_memory_fraction(0.95)  # Use up to 95% of GPU memory\n",
    "        print(\"‚öôÔ∏è Set GPU memory fraction to 95% for RunPod environment\")\n",
    "    else:\n",
    "        # More conservative for local environment\n",
    "        torch.cuda.set_per_process_memory_fraction(0.8)  # Use up to 80% of GPU memory\n",
    "        print(\"‚öôÔ∏è Set GPU memory fraction to 80% for local environment\")\n",
    "\n",
    "# Initialize experiment configuration\n",
    "experiment_name = f\"pixtral_extraction_{timestamp}\"\n",
    "print(f\"üß™ Initializing experiment: {experiment_name}\")\n",
    "\n",
    "# Create experiment configuration dictionary\n",
    "experiment_config = {\n",
    "    \"name\": experiment_name,\n",
    "    \"timestamp\": timestamp,\n",
    "    \"environment\": env_config.environment,\n",
    "    \"model_name\": \"pixtral-12b\",  # Default model\n",
    "    \"model_repo_id\": \"mistral-community/pixtral-12b\",  # Default repo\n",
    "    \"field_to_extract\": \"work_order\",  # What we're extracting from invoices\n",
    "    \"use_4bit_quantization\": env_config.use_4bit_quantization if hasattr(env_config, 'use_4bit_quantization') else False,\n",
    "    \"use_half_precision\": env_config.use_half_precision if hasattr(env_config, 'use_half_precision') else True,\n",
    "    \"max_batch_size\": env_config.max_batch_size if hasattr(env_config, 'max_batch_size') else 1,\n",
    "    \"description\": \"Extracting work order numbers from invoice images using Pixtral-12B\"\n",
    "}\n",
    "\n",
    "# Display environment and experiment configuration summary\n",
    "print(\"\\nüìã Environment Configuration Summary:\")\n",
    "env_config.print_summary()\n",
    "\n",
    "print(\"\\nüß™ Experiment Configuration Summary:\")\n",
    "for key, value in experiment_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Save configurations for reference\n",
    "config_path = get_results_path(\"experiment_config.json\")\n",
    "with open(config_path, \"w\") as f:\n",
    "    # Create serializable config by converting Path objects to strings\n",
    "    serializable_config = {\n",
    "        \"experiment\": experiment_config,\n",
    "        \"environment\": {k: str(v) if isinstance(v, Path) else v \n",
    "                        for k, v in vars(env_config).items() \n",
    "                        if not k.startswith('_') and not callable(v)},\n",
    "        \"paths\": {k: str(v) if isinstance(v, Path) else v \n",
    "                 for k, v in paths.items() \n",
    "                 if not callable(v)}\n",
    "    }\n",
    "    json.dump(serializable_config, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Configuration saved to: {config_path}\")\n",
    "print(\"‚úÖ Environment and experiment configuration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Prompt System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prompt system initialized with 43 prompts\n"
     ]
    }
   ],
   "source": [
    "# Initialize prompt system\n",
    "try:\n",
    "    from src.prompts import initialize_prompt_system\n",
    "    \n",
    "    # Initialize the prompt system\n",
    "    prompt_init_result = initialize_prompt_system()\n",
    "    if prompt_init_result[\"status\"] == \"success\":\n",
    "        print(f\"‚úÖ Prompt system initialized with {prompt_init_result['prompt_count']} prompts\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Prompt system initialization issue: {prompt_init_result.get('error', 'Unknown error')}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Prompt system initialization module not found: {e}\")\n",
    "    print(\"üìù Will use fallback prompt implementation\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error initializing prompt system: {e}\")\n",
    "    print(\"üìù Will use fallback prompt implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Found 43 available prompts\n",
      "   Available categories: basic, detailed, positioned, formatted, contextual, chain_of_thought, fallback\n",
      "‚úÖ Selected prompt: \"basic_work_order\"\n",
      "   Category: basic\n",
      "   Field: work_order\n",
      "   Prompt text: \"Extract the work order number from this invoice image.\"\n",
      "\n",
      "üìù Formatted for pixtral-12b:\n",
      "   \"<s>[INST]Extract the work order number from this invoice image.\n",
      "[IMG][/INST]\"\n",
      "\n",
      "üîç Prompt Analysis:\n",
      "   Word count: 9\n",
      "   Character count: 54\n",
      "   Complexity score: 0.36166666666666664\n"
     ]
    }
   ],
   "source": [
    "# Prompt Selection\n",
    "# This cell selects and formats the specific prompt for our experiment\n",
    "\n",
    "try:\n",
    "    # Import prompt utilities with the correct function names\n",
    "    from src.prompts.registry import get_prompt, list_all_prompts, list_prompt_categories\n",
    "    from src.prompts.prompt_utils import analyze_prompt\n",
    "    \n",
    "    # List available prompts\n",
    "    available_prompts = list_all_prompts()\n",
    "    print(f\"üìã Found {len(available_prompts)} available prompts\")\n",
    "    \n",
    "    # List categories for reference\n",
    "    categories = list_prompt_categories()\n",
    "    print(f\"   Available categories: {', '.join(categories)}\")\n",
    "    \n",
    "    # Select the specific prompt that worked well in previous tests\n",
    "    prompt_name = \"basic_work_order\"  # The prompt that worked in RunPod\n",
    "    selected_prompt = get_prompt(prompt_name)\n",
    "    \n",
    "    if selected_prompt:\n",
    "        print(f\"‚úÖ Selected prompt: \\\"{selected_prompt.name}\\\"\")\n",
    "        print(f\"   Category: {selected_prompt.category}\")\n",
    "        print(f\"   Field: {selected_prompt.field_to_extract}\")\n",
    "        print(f\"   Prompt text: \\\"{selected_prompt.text}\\\"\")\n",
    "        \n",
    "        # Format the prompt for the Pixtral model\n",
    "        model_name = \"pixtral-12b\"\n",
    "        formatted_prompt = selected_prompt.format_for_model(model_name)\n",
    "        \n",
    "        print(f\"\\nüìù Formatted for {model_name}:\")\n",
    "        print(f\"   \\\"{formatted_prompt}\\\"\")\n",
    "        \n",
    "        # Analyze the prompt if analyze_prompt function exists\n",
    "        try:\n",
    "            prompt_analysis = analyze_prompt(selected_prompt)\n",
    "            print(f\"\\nüîç Prompt Analysis:\")\n",
    "            print(f\"   Word count: {prompt_analysis['word_count']}\")\n",
    "            print(f\"   Character count: {prompt_analysis['character_count']}\")\n",
    "            print(f\"   Complexity score: {prompt_analysis.get('complexity_score', 'N/A')}\")\n",
    "        except (ImportError, NameError):\n",
    "            # Simple fallback analysis if function doesn't exist\n",
    "            word_count = len(selected_prompt.text.split())\n",
    "            char_count = len(selected_prompt.text)\n",
    "            prompt_analysis = {\n",
    "                \"word_count\": word_count,\n",
    "                \"character_count\": char_count,\n",
    "                \"complexity_score\": word_count / 10  # Simple metric\n",
    "            }\n",
    "            print(f\"\\nüîç Basic Prompt Analysis:\")\n",
    "            print(f\"   Word count: {prompt_analysis['word_count']}\")\n",
    "            print(f\"   Character count: {prompt_analysis['character_count']}\")\n",
    "        \n",
    "        # Store for later use\n",
    "        prompt_info = {\n",
    "            \"name\": selected_prompt.name,\n",
    "            \"text\": selected_prompt.text,\n",
    "            \"formatted_text\": formatted_prompt,\n",
    "            \"category\": selected_prompt.category,\n",
    "            \"field_to_extract\": selected_prompt.field_to_extract,\n",
    "            \"analysis\": prompt_analysis\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Error: Prompt '{prompt_name}' not found in registry\")\n",
    "        print(\"   Please check your prompt configuration and registry initialization\")\n",
    "        prompt_info = None\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing prompt modules: {e}\")\n",
    "    print(\"   Using direct implementation instead\")\n",
    "    \n",
    "    # Define a fallback prompt if imports fail\n",
    "    prompt_info = {\n",
    "        \"name\": \"basic_work_order\",\n",
    "        \"text\": \"Extract the work order number from this invoice image.\",\n",
    "        \"formatted_text\": \"<s>[INST]Extract the work order number from this invoice image.\\n[IMG][/INST]\",\n",
    "        \"category\": \"basic\",\n",
    "        \"field_to_extract\": \"work_order\",\n",
    "        \"analysis\": {\n",
    "            \"word_count\": 8,\n",
    "            \"character_count\": 50,\n",
    "            \"complexity_score\": 0.8\n",
    "        }\n",
    "    }\n",
    "    print(\"‚úÖ Using fallback prompt:\")\n",
    "    print(f\"   Name: {prompt_info['name']}\")\n",
    "    print(f\"   Text: \\\"{prompt_info['text']}\\\"\")\n",
    "    print(f\"   Formatted: \\\"{prompt_info['formatted_text']}\\\"\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during prompt selection: {e}\")\n",
    "    print(\"   Using fallback basic prompt instead\")\n",
    "    prompt_info = {\n",
    "        \"name\": \"basic_work_order\",\n",
    "        \"text\": \"Extract the work order number from this invoice image.\",\n",
    "        \"formatted_text\": \"<s>[INST]Extract the work order number from this invoice image.\\n[IMG][/INST]\",\n",
    "        \"category\": \"basic\",\n",
    "        \"field_to_extract\": \"work_order\",\n",
    "        \"analysis\": {\n",
    "            \"word_count\": 8,\n",
    "            \"character_count\": 50,\n",
    "            \"complexity_score\": 0.8\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Setting up experiment parameters\n",
      "\n",
      "üìã Experiment Details:\n",
      "   Name: pixtral_extraction_20250318_002223\n",
      "   Description: Extraction of work order numbers from invoice images using Pixtral-12B model\n",
      "   Timestamp: 20250318_002223\n",
      "\n",
      "ü§ñ Model Configuration:\n",
      "   Model: pixtral-12b\n",
      "   Repository: mistral-community/pixtral-12b\n",
      "   Data Type: bfloat16\n",
      "   Quantization: Disabled\n",
      "   Flash Attention: Enabled\n",
      "\n",
      "üîç Extraction Task:\n",
      "   Field to Extract: work_order\n",
      "   Ground Truth Column: Work Order Number/Numero de Orden\n",
      "   Prompt: \"Extract the work order number from this invoice image.\"\n",
      "   Prompt Category: basic\n",
      "   Formatted Prompt: \"<s>[INST]Extract the work order number from this invoice image.\n",
      "[IMG][/INST]\"\n",
      "\n",
      "üìä Evaluation Metrics:\n",
      "   Primary Metric: exact_match\n",
      "   Secondary Metrics: character_error_rate, levenshtein_distance, processing_time\n",
      "\n",
      "‚öôÔ∏è Execution Settings:\n",
      "   Max Batch Size: 1\n",
      "   Checkpointing: Enabled\n",
      "   Checkpoint Frequency: Every 5 images\n",
      "\n",
      "üíæ Full experiment configuration saved to: /workspace/results/pixtral_extraction_20250318_002223/full_experiment_config.json\n",
      "‚úÖ Experiment parameters configured successfully!\n"
     ]
    }
   ],
   "source": [
    "# Experiment Parameters\n",
    "\n",
    "# Import experiment configuration utilities if available\n",
    "try:\n",
    "    from src.config.experiment import get_experiment_config, ExperimentConfig\n",
    "except ImportError:\n",
    "    logger.warning(\"Using fallback experiment configuration\")\n",
    "    # Continue with manual configuration\n",
    "\n",
    "# Define experiment details\n",
    "print(\"üß™ Setting up experiment parameters\")\n",
    "\n",
    "# Define experiment name and basic metadata\n",
    "experiment_details = {\n",
    "    \"name\": experiment_name,\n",
    "    \"timestamp\": timestamp,\n",
    "    \"description\": \"Extraction of work order numbers from invoice images using Pixtral-12B model\",\n",
    "    \"version\": \"1.0.0\",\n",
    "    \"owner\": os.environ.get(\"USER\", \"unknown\"),\n",
    "}\n",
    "\n",
    "# Model configuration settings\n",
    "model_config = {\n",
    "    \"name\": \"pixtral-12b\",\n",
    "    \"repo_id\": \"mistral-community/pixtral-12b\",\n",
    "    \"revision\": None,  # Use latest version\n",
    "    \"dtype\": \"bfloat16\" if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else \"float16\",\n",
    "    \"device_map\": \"auto\",  # Let the library decide the optimal device mapping\n",
    "    \"use_quantization\": env_config.use_4bit_quantization if hasattr(env_config, \"use_4bit_quantization\") else False,\n",
    "    \"quantization_bits\": 4 if hasattr(env_config, \"use_4bit_quantization\") and env_config.use_4bit_quantization else None,\n",
    "    \"cache_dir\": str(paths[\"models_cache_dir\"]),\n",
    "    \"trust_remote_code\": True,  # Needed for some models\n",
    "    \"use_flash_attention\": torch.cuda.is_available() and torch.__version__ >= \"2.0.0\",\n",
    "}\n",
    "\n",
    "# Extraction task parameters - incorporate the selected prompt\n",
    "extraction_config = {\n",
    "    \"field_to_extract\": prompt_info[\"field_to_extract\"],  # Use field from selected prompt\n",
    "    \"field_column_name\": \"Work Order Number/Numero de Orden\",  # Column name in ground truth CSV\n",
    "    \"prompt_category\": prompt_info[\"category\"],  # Use category from selected prompt\n",
    "    \"prompt_name\": prompt_info[\"name\"],  # Use name from selected prompt\n",
    "    \"formatted_prompt\": prompt_info[\"formatted_text\"],  # Use formatted text from selected prompt\n",
    "}\n",
    "\n",
    "# Evaluation metrics configuration\n",
    "metrics_config = {\n",
    "    \"primary_metric\": \"exact_match\",\n",
    "    \"secondary_metrics\": [\"character_error_rate\", \"levenshtein_distance\", \"processing_time\"],\n",
    "    \"exact_match_case_sensitive\": False,  # Ignore case when comparing\n",
    "    \"exact_match_strip_whitespace\": True,  # Ignore whitespace when comparing\n",
    "    \"cer_normalize\": True,  # Normalize character error rate by length\n",
    "}\n",
    "\n",
    "# Pipeline execution settings\n",
    "execution_config = {\n",
    "    \"max_batch_size\": env_config.max_batch_size if hasattr(env_config, \"max_batch_size\") else 1,\n",
    "    \"save_checkpoints\": True,\n",
    "    \"checkpoint_frequency\": 5,  # Save after every 5 images\n",
    "    \"resume_from_checkpoint\": True,\n",
    "    \"timeout_per_image\": 60,  # Maximum seconds per image\n",
    "    \"max_retries\": 3,  # Retry failed images\n",
    "    \"log_gpu_memory\": True,\n",
    "}\n",
    "\n",
    "# Combine all configurations into master experiment config\n",
    "full_experiment_config = {\n",
    "    \"experiment\": experiment_details,\n",
    "    \"model\": model_config,\n",
    "    \"extraction\": extraction_config,\n",
    "    \"metrics\": metrics_config,\n",
    "    \"execution\": execution_config,\n",
    "    \"environment\": env_config.environment,\n",
    "    \"prompt\": prompt_info,  # Add complete prompt information\n",
    "}\n",
    "\n",
    "# Display summary of experiment configuration\n",
    "print(\"\\nüìã Experiment Details:\")\n",
    "print(f\"   Name: {experiment_details['name']}\")\n",
    "print(f\"   Description: {experiment_details['description']}\")\n",
    "print(f\"   Timestamp: {experiment_details['timestamp']}\")\n",
    "\n",
    "print(\"\\nü§ñ Model Configuration:\")\n",
    "print(f\"   Model: {model_config['name']}\")\n",
    "print(f\"   Repository: {model_config['repo_id']}\")\n",
    "print(f\"   Data Type: {model_config['dtype']}\")\n",
    "print(f\"   Quantization: {'Enabled (' + str(model_config['quantization_bits']) + '-bit)' if model_config['use_quantization'] else 'Disabled'}\")\n",
    "print(f\"   Flash Attention: {'Enabled' if model_config['use_flash_attention'] else 'Disabled'}\")\n",
    "\n",
    "print(\"\\nüîç Extraction Task:\")\n",
    "print(f\"   Field to Extract: {extraction_config['field_to_extract']}\")\n",
    "print(f\"   Ground Truth Column: {extraction_config['field_column_name']}\")\n",
    "print(f\"   Prompt: \\\"{prompt_info['text']}\\\"\")\n",
    "print(f\"   Prompt Category: {extraction_config['prompt_category']}\")\n",
    "print(f\"   Formatted Prompt: \\\"{extraction_config['formatted_prompt']}\\\"\")\n",
    "\n",
    "print(\"\\nüìä Evaluation Metrics:\")\n",
    "print(f\"   Primary Metric: {metrics_config['primary_metric']}\")\n",
    "print(f\"   Secondary Metrics: {', '.join(metrics_config['secondary_metrics'])}\")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Execution Settings:\")\n",
    "print(f\"   Max Batch Size: {execution_config['max_batch_size']}\")\n",
    "print(f\"   Checkpointing: {'Enabled' if execution_config['save_checkpoints'] else 'Disabled'}\")\n",
    "if execution_config['save_checkpoints']:\n",
    "    print(f\"   Checkpoint Frequency: Every {execution_config['checkpoint_frequency']} images\")\n",
    "\n",
    "# Save full experiment configuration to file\n",
    "exp_config_path = get_results_path(\"full_experiment_config.json\")\n",
    "with open(exp_config_path, \"w\") as f:\n",
    "    json.dump(full_experiment_config, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Full experiment configuration saved to: {exp_config_path}\")\n",
    "print(\"‚úÖ Experiment parameters configured successfully!\")\n",
    "\n",
    "# Make config available for other cells\n",
    "experiment_config = full_experiment_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imported data handling utilities\n",
      "\n",
      "üìä Loading ground truth data...\n",
      "‚úÖ Loaded ground truth data: 29 records\n",
      "\n",
      "First few rows of ground truth data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Invoice</th>\n",
       "      <th>Type</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Name</th>\n",
       "      <th>Work Order Number/Numero de Orden</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1017</td>\n",
       "      <td>Invoice</td>\n",
       "      <td>10/17/2024</td>\n",
       "      <td>Edgar</td>\n",
       "      <td>20502</td>\n",
       "      <td>950.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1018</td>\n",
       "      <td>Invoice</td>\n",
       "      <td>10/17/2024</td>\n",
       "      <td>Edgar</td>\n",
       "      <td>20558</td>\n",
       "      <td>550.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1019</td>\n",
       "      <td>Invoice</td>\n",
       "      <td>10/17/2024</td>\n",
       "      <td>Edgar</td>\n",
       "      <td>20509</td>\n",
       "      <td>150.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1020</td>\n",
       "      <td>Invoice</td>\n",
       "      <td>10/17/2024</td>\n",
       "      <td>Edgar</td>\n",
       "      <td>20479</td>\n",
       "      <td>140.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021</td>\n",
       "      <td>Invoice</td>\n",
       "      <td>10/17/2024</td>\n",
       "      <td>Edgar</td>\n",
       "      <td>20516</td>\n",
       "      <td>190.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Invoice     Type   Timestamp   Name Work Order Number/Numero de Orden  \\\n",
       "0     1017  Invoice  10/17/2024  Edgar                             20502   \n",
       "1     1018  Invoice  10/17/2024  Edgar                             20558   \n",
       "2     1019  Invoice  10/17/2024  Edgar                             20509   \n",
       "3     1020  Invoice  10/17/2024  Edgar                             20479   \n",
       "4     1021  Invoice  10/17/2024  Edgar                             20516   \n",
       "\n",
       "      Total  \n",
       "0   950.00   \n",
       "1   550.00   \n",
       "2   150.00   \n",
       "3   140.00   \n",
       "4   190.00   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns in ground truth data:\n",
      "  1. Invoice\n",
      "  2. Type\n",
      "  3. Timestamp\n",
      "  4. Name\n",
      "  5. Work Order Number/Numero de Orden\n",
      "  6. Total\n",
      "\n",
      "üîÑ Mapping images to ground truth values...\n",
      "üì∏ Found 20 image files\n",
      "‚úÖ Successfully mapped 20 images to ground truth data\n",
      "\n",
      "üîÑ Creating structured batch items for processing...\n",
      "‚úÖ Created 20 batch items\n",
      "\n",
      "üìä Ground Truth Statistics:\n",
      "   Total Images: 20\n",
      "   Unique Work Orders: 19\n",
      "   Average Work Order Length: 5.00 characters\n",
      "   Min Length: 5 characters\n",
      "   Max Length: 5 characters\n",
      "\n",
      "üìã Sample of Ground Truth Mapping:\n",
      "   1. Image: 1022\n",
      "      ‚Ü≥ Path: /workspace/data/images/1022.jpg\n",
      "      ‚Ü≥ Work Order: '20525'\n",
      "   2. Image: 1025\n",
      "      ‚Ü≥ Path: /workspace/data/images/1025.jpg\n",
      "      ‚Ü≥ Work Order: '20570'\n",
      "   3. Image: 1040\n",
      "      ‚Ü≥ Path: /workspace/data/images/1040.jpg\n",
      "      ‚Ü≥ Work Order: 'Aston'\n",
      "   4. Image: 1042\n",
      "      ‚Ü≥ Path: /workspace/data/images/1042.jpg\n",
      "      ‚Ü≥ Work Order: '20455'\n",
      "   5. Image: 1021\n",
      "      ‚Ü≥ Path: /workspace/data/images/1021.jpg\n",
      "      ‚Ü≥ Work Order: '20516'\n",
      "\n",
      "üíæ Ground truth mapping saved to: /workspace/results/pixtral_extraction_20250318_002223/ground_truth_mapping.json\n",
      "‚úÖ Ground truth data loading complete!\n"
     ]
    }
   ],
   "source": [
    "# Ground Truth Data Loading\n",
    "\n",
    "# Import data handling utilities\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Try to import project-specific data utilities\n",
    "try:\n",
    "    from src.data.loader import load_ground_truth, map_images_to_ground_truth\n",
    "    from src.execution.batch import prepare_batch_items\n",
    "    print(\"‚úÖ Imported data handling utilities\")\n",
    "    use_project_utils = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Project data utilities not available, using direct implementation\")\n",
    "    use_project_utils = False\n",
    "\n",
    "# Load ground truth CSV data\n",
    "print(\"\\nüìä Loading ground truth data...\")\n",
    "\n",
    "ground_truth_path = paths[\"ground_truth_path\"]\n",
    "if not os.path.exists(ground_truth_path):\n",
    "    print(f\"‚ùå Ground truth file not found at: {ground_truth_path}\")\n",
    "    raise FileNotFoundError(f\"Ground truth file not found: {ground_truth_path}\")\n",
    "\n",
    "try:\n",
    "    # Load the CSV file\n",
    "    ground_truth_df = pd.read_csv(ground_truth_path)\n",
    "    print(f\"‚úÖ Loaded ground truth data: {len(ground_truth_df)} records\")\n",
    "    \n",
    "    # Display basic information\n",
    "    print(f\"\\nFirst few rows of ground truth data:\")\n",
    "    display(ground_truth_df.head())\n",
    "    \n",
    "    # Display column information\n",
    "    print(f\"\\nColumns in ground truth data:\")\n",
    "    for i, col in enumerate(ground_truth_df.columns):\n",
    "        print(f\"  {i+1}. {col}\")\n",
    "    \n",
    "    # Check for any missing values in key fields\n",
    "    field_column = extraction_config[\"field_column_name\"]\n",
    "    if field_column not in ground_truth_df.columns:\n",
    "        print(f\"‚ùå Field column '{field_column}' not found in ground truth data\")\n",
    "        print(f\"   Available columns: {', '.join(ground_truth_df.columns)}\")\n",
    "        raise ValueError(f\"Field column '{field_column}' not found in ground truth data\")\n",
    "    \n",
    "    missing_values = ground_truth_df[field_column].isna().sum()\n",
    "    if missing_values > 0:\n",
    "        print(f\"‚ö†Ô∏è Found {missing_values} missing values in {field_column} field\")\n",
    "    \n",
    "    # Map images to ground truth values\n",
    "    print(\"\\nüîÑ Mapping images to ground truth values...\")\n",
    "    \n",
    "    # Get image files\n",
    "    image_dir = paths[\"images_dir\"]\n",
    "    image_files = list(image_dir.glob(\"*.jpg\")) + list(image_dir.glob(\"*.png\"))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"‚ùå No image files found in {image_dir}\")\n",
    "        raise FileNotFoundError(f\"No image files found in {image_dir}\")\n",
    "    \n",
    "    print(f\"üì∏ Found {len(image_files)} image files\")\n",
    "    \n",
    "    # Determine the image ID column (typically 'Invoice' or similar)\n",
    "    # This could be configurable in your extraction_config\n",
    "    image_id_column = \"Invoice\"  # Default, adjust if your CSV uses a different column\n",
    "    \n",
    "    if image_id_column not in ground_truth_df.columns:\n",
    "        print(f\"‚ö†Ô∏è Image ID column '{image_id_column}' not found, trying alternatives...\")\n",
    "        # Try to find a suitable column\n",
    "        for candidate in [\"invoice\", \"image\", \"filename\", \"file\", \"id\"]:\n",
    "            matches = [col for col in ground_truth_df.columns if candidate in col.lower()]\n",
    "            if matches:\n",
    "                image_id_column = matches[0]\n",
    "                print(f\"‚úÖ Using '{image_id_column}' as image ID column\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"‚ùå Could not find a suitable image ID column\")\n",
    "            print(f\"   Please specify the column that contains image identifiers\")\n",
    "            raise ValueError(\"Image ID column not found\")\n",
    "    \n",
    "    # Create mapping between images and ground truth\n",
    "    ground_truth_mapping = {}\n",
    "    unmatched_images = []\n",
    "    \n",
    "    for image_path in image_files:\n",
    "        image_id = image_path.stem  # Get filename without extension\n",
    "        \n",
    "        # Find matching row in ground truth\n",
    "        # Converting to string to ensure matching works with numeric IDs\n",
    "        matching_row = ground_truth_df[ground_truth_df[image_id_column].astype(str) == image_id]\n",
    "        \n",
    "        if len(matching_row) > 0:\n",
    "            row = matching_row.iloc[0]\n",
    "            ground_truth_mapping[image_id] = {\n",
    "                'image_path': str(image_path),\n",
    "                'work_order': str(row[field_column]).strip(),\n",
    "                'image_id': image_id\n",
    "            }\n",
    "        else:\n",
    "            unmatched_images.append(image_id)\n",
    "            print(f\"‚ö†Ô∏è No matching ground truth found for image {image_id}\")\n",
    "    \n",
    "    print(f\"‚úÖ Successfully mapped {len(ground_truth_mapping)} images to ground truth data\")\n",
    "    \n",
    "    if unmatched_images:\n",
    "        print(f\"‚ö†Ô∏è Found {len(unmatched_images)} images without ground truth data\")\n",
    "        if len(unmatched_images) < 10:\n",
    "            print(f\"   Unmatched images: {', '.join(unmatched_images)}\")\n",
    "        else:\n",
    "            print(f\"   First 10 unmatched images: {', '.join(unmatched_images[:10])}...\")\n",
    "    \n",
    "    # Create structured batch items for processing\n",
    "    print(\"\\nüîÑ Creating structured batch items for processing...\")\n",
    "    \n",
    "    batch_items = []\n",
    "    for image_id, data in ground_truth_mapping.items():\n",
    "        batch_items.append({\n",
    "            \"image_id\": image_id,\n",
    "            \"image_path\": data[\"image_path\"],\n",
    "            \"ground_truth\": data[\"work_order\"],\n",
    "            \"field_type\": extraction_config[\"field_to_extract\"]\n",
    "        })\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(batch_items)} batch items\")\n",
    "    \n",
    "    # Display statistics and sample mapping\n",
    "    print(\"\\nüìä Ground Truth Statistics:\")\n",
    "    \n",
    "    # Count unique work order values\n",
    "    unique_work_orders = len(set(data[\"work_order\"] for data in ground_truth_mapping.values()))\n",
    "    print(f\"   Total Images: {len(ground_truth_mapping)}\")\n",
    "    print(f\"   Unique Work Orders: {unique_work_orders}\")\n",
    "    \n",
    "    # Calculate average work order length\n",
    "    work_order_lengths = [len(data[\"work_order\"]) for data in ground_truth_mapping.values()]\n",
    "    avg_length = sum(work_order_lengths) / len(work_order_lengths) if work_order_lengths else 0\n",
    "    print(f\"   Average Work Order Length: {avg_length:.2f} characters\")\n",
    "    print(f\"   Min Length: {min(work_order_lengths) if work_order_lengths else 0} characters\")\n",
    "    print(f\"   Max Length: {max(work_order_lengths) if work_order_lengths else 0} characters\")\n",
    "    \n",
    "    # Display sample of the mapping\n",
    "    print(\"\\nüìã Sample of Ground Truth Mapping:\")\n",
    "    random_samples = random.sample(list(ground_truth_mapping.items()), min(5, len(ground_truth_mapping)))\n",
    "    for i, (img_id, data) in enumerate(random_samples, 1):\n",
    "        print(f\"   {i}. Image: {img_id}\")\n",
    "        print(f\"      ‚Ü≥ Path: {data['image_path']}\")\n",
    "        print(f\"      ‚Ü≥ Work Order: '{data['work_order']}'\")\n",
    "    \n",
    "    # Save the ground truth mapping for reference\n",
    "    mapping_path = get_results_path(\"ground_truth_mapping.json\")\n",
    "    with open(mapping_path, \"w\") as f:\n",
    "        # Convert to serializable format (paths need to be strings)\n",
    "        serializable_mapping = {\n",
    "            img_id: {k: v for k, v in data.items()}\n",
    "            for img_id, data in ground_truth_mapping.items()\n",
    "        }\n",
    "        json.dump(serializable_mapping, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Ground truth mapping saved to: {mapping_path}\")\n",
    "    print(\"‚úÖ Ground truth data loading complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading ground truth data: {e}\")\n",
    "    ground_truth_df = None\n",
    "    ground_truth_mapping = {}\n",
    "    batch_items = []\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "### GPU Verification & Memory Check / Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imported GPU verification utilities\n",
      "üîç Checking GPU compatibility for pixtral-12b...\n",
      "DEBUG - Compatibility object structure: {'compatible': True, 'model_requirements': {'gpu_required': True, 'gpu_memory_min': '24GB', 'recommended_gpu': 'A4000 or better'}, 'current_gpu': {'gpu_available': True, 'device_name': 'NVIDIA H200', 'total_memory_gb': 150.021603328, 'allocated_memory_gb': 0.0, 'reserved_memory_gb': 0.0, 'free_memory_gb': 150.021603328, 'utilization_percent': 0.0}}\n",
      "‚úÖ GPU is compatible with pixtral-12b\n",
      "   GPU: NVIDIA H200\n",
      "   Memory: 150.0 GB\n",
      "   Required: 24 GB\n",
      "\n",
      "üß† Pre-loading GPU memory state:\n",
      "   Total memory: 150.02 GB\n",
      "   Allocated memory: 0.00 GB\n",
      "   Free memory: 150.02 GB\n",
      "   Utilization: 0.0%\n",
      "\n",
      "üßπ Cleaning up GPU memory before model loading...\n",
      "   Memory after cleanup: 0.00 GB allocated\n",
      "   Free memory: 150.02 GB\n",
      "\n",
      "‚öôÔ∏è Configuring model loading parameters...\n",
      "   Using bfloat16 precision (optimal for Ampere+ GPUs)\n",
      "\n",
      "üíæ Pre-loading GPU state saved to: /workspace/results/pixtral_extraction_20250318_002223/pre_loading_gpu_state.json\n",
      "‚úÖ GPU verification complete and model loading configured!\n"
     ]
    }
   ],
   "source": [
    "# GPU Verification\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Try to import project-specific utilities\n",
    "try:\n",
    "    from src.models.loader import get_gpu_memory_info, verify_gpu_compatibility\n",
    "    print(\"‚úÖ Imported GPU verification utilities\")\n",
    "    use_project_utils = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Project GPU utilities not available, using direct implementation\")\n",
    "    use_project_utils = False\n",
    "    \n",
    "    # Implement fallback functions\n",
    "    def get_gpu_memory_info():\n",
    "        \"\"\"Get GPU memory information\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return {\n",
    "                \"total_memory_gb\": 0,\n",
    "                \"allocated_memory_gb\": 0,\n",
    "                \"free_memory_gb\": 0,\n",
    "                \"utilization\": 0\n",
    "            }\n",
    "        \n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "        allocated_memory = torch.cuda.memory_allocated(0)\n",
    "        reserved_memory = torch.cuda.memory_reserved(0)\n",
    "        free_memory = total_memory - allocated_memory\n",
    "        \n",
    "        # Convert to GB\n",
    "        total_memory_gb = total_memory / 1e9\n",
    "        allocated_memory_gb = allocated_memory / 1e9\n",
    "        free_memory_gb = free_memory / 1e9\n",
    "        utilization = (allocated_memory / total_memory) * 100\n",
    "        \n",
    "        return {\n",
    "            \"total_memory_gb\": total_memory_gb,\n",
    "            \"allocated_memory_gb\": allocated_memory_gb,\n",
    "            \"free_memory_gb\": free_memory_gb,\n",
    "            \"utilization\": utilization\n",
    "        }\n",
    "    \n",
    "    def verify_gpu_compatibility(model_name):\n",
    "        \"\"\"Check if GPU is compatible with model requirements\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return {\n",
    "                \"compatible\": False,\n",
    "                \"reason\": \"No GPU available\",\n",
    "                \"model_requirements\": {\"minimum_memory_gb\": 24},\n",
    "                \"current_gpu\": {\"name\": \"None\", \"memory_gb\": 0}\n",
    "            }\n",
    "        \n",
    "        # Get GPU info\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        total_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        \n",
    "        # Define model requirements\n",
    "        # Pixtral-12B typically needs around 24GB without quantization\n",
    "        model_requirements = {\n",
    "            \"pixtral-12b\": {\"minimum_memory_gb\": 24},\n",
    "            \"pixtral-7b\": {\"minimum_memory_gb\": 16},\n",
    "            \"default\": {\"minimum_memory_gb\": 12}\n",
    "        }\n",
    "        \n",
    "        # Get requirements for this model\n",
    "        req = model_requirements.get(model_name.lower(), model_requirements[\"default\"])\n",
    "        \n",
    "        # Check compatibility\n",
    "        compatible = total_memory_gb >= req[\"minimum_memory_gb\"]\n",
    "        \n",
    "        return {\n",
    "            \"compatible\": compatible,\n",
    "            \"reason\": None if compatible else f\"Insufficient GPU memory: {total_memory_gb:.1f}GB available, {req['minimum_memory_gb']}GB required\",\n",
    "            \"model_requirements\": req,\n",
    "            \"current_gpu\": {\n",
    "                \"name\": gpu_name,\n",
    "                \"memory_gb\": total_memory_gb\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Check for GPU availability\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"‚ùå No GPU detected!\")\n",
    "    print(\"   This model requires a GPU for reasonable processing times.\")\n",
    "    print(\"   If you're on RunPod, verify that the GPU is properly allocated.\")\n",
    "    raise RuntimeError(\"No GPU available for model loading\")\n",
    "\n",
    "# Get model name from experiment configuration\n",
    "model_name = model_config[\"name\"]\n",
    "print(f\"üîç Checking GPU compatibility for {model_name}...\")\n",
    "\n",
    "# Check GPU compatibility\n",
    "compatibility = verify_gpu_compatibility(model_name)\n",
    "\n",
    "# Debug the compatibility object structure\n",
    "print(f\"DEBUG - Compatibility object structure: {compatibility}\")\n",
    "\n",
    "# Use defensively coded approach to handle different structures\n",
    "if compatibility.get(\"compatible\", False):\n",
    "    print(f\"‚úÖ GPU is compatible with {model_name}\")\n",
    "    \n",
    "    # Get GPU info, using direct access if keys don't exist\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    # Try to get from compatibility if available\n",
    "    if isinstance(compatibility.get(\"current_gpu\"), dict):\n",
    "        gpu_name = compatibility[\"current_gpu\"].get(\"name\", gpu_name)\n",
    "        gpu_memory = compatibility[\"current_gpu\"].get(\"memory_gb\", gpu_memory)\n",
    "    \n",
    "    print(f\"   GPU: {gpu_name}\")\n",
    "    print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # Get requirements\n",
    "    req_memory = 24  # Default\n",
    "    if isinstance(compatibility.get(\"model_requirements\"), dict):\n",
    "        req_memory = compatibility[\"model_requirements\"].get(\"minimum_memory_gb\", req_memory)\n",
    "    \n",
    "    print(f\"   Required: {req_memory} GB\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Warning: {compatibility.get('reason', 'GPU may not be compatible')}\")\n",
    "    print(f\"   Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"   Proceeding with quantization enabled to reduce memory requirements\")\n",
    "    \n",
    "    # Enable quantization if not already enabled\n",
    "    if not model_config[\"use_quantization\"]:\n",
    "        print(\"üîÑ Automatically enabling 4-bit quantization to fit model in memory\")\n",
    "        model_config[\"use_quantization\"] = True\n",
    "        model_config[\"quantization_bits\"] = 4\n",
    "\n",
    "# Display pre-loading memory state\n",
    "print(\"\\nüß† Pre-loading GPU memory state:\")\n",
    "pre_memory = get_gpu_memory_info()\n",
    "print(f\"   Total memory: {pre_memory['total_memory_gb']:.2f} GB\")\n",
    "print(f\"   Allocated memory: {pre_memory['allocated_memory_gb']:.2f} GB\")\n",
    "print(f\"   Free memory: {pre_memory['free_memory_gb']:.2f} GB\")\n",
    "print(f\"   Utilization: {pre_memory.get('utilization', pre_memory.get('utilization_percent', 0)):.1f}%\")\n",
    "\n",
    "# Clear memory before loading\n",
    "print(\"\\nüßπ Cleaning up GPU memory before model loading...\")\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# Verify memory after cleanup\n",
    "post_cleanup_memory = get_gpu_memory_info()\n",
    "print(f\"   Memory after cleanup: {post_cleanup_memory['allocated_memory_gb']:.2f} GB allocated\")\n",
    "print(f\"   Free memory: {post_cleanup_memory['free_memory_gb']:.2f} GB\")\n",
    "\n",
    "# Configure model loading parameters based on compatibility check\n",
    "print(\"\\n‚öôÔ∏è Configuring model loading parameters...\")\n",
    "\n",
    "# Set up quantization configuration if enabled\n",
    "quantization_config = None\n",
    "if model_config[\"use_quantization\"]:\n",
    "    try:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        \n",
    "        bits = model_config[\"quantization_bits\"]\n",
    "        print(f\"   Using {bits}-bit quantization\")\n",
    "        \n",
    "        if bits == 4:\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "            print(\"   Configured 4-bit NormalFloat quantization\")\n",
    "        elif bits == 8:\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_8bit=True\n",
    "            )\n",
    "            print(\"   Configured 8-bit quantization\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è BitsAndBytesConfig not available, quantization will be limited\")\n",
    "        quantization_config = {\"load_in_4bit\": True}\n",
    "\n",
    "# Determine precision (dtype)\n",
    "if torch.cuda.is_available():\n",
    "    if torch.cuda.get_device_capability()[0] >= 8:  # Ampere or newer\n",
    "        dtype = torch.bfloat16\n",
    "        print(\"   Using bfloat16 precision (optimal for Ampere+ GPUs)\")\n",
    "    else:\n",
    "        dtype = torch.float16\n",
    "        print(\"   Using float16 precision (for older GPUs)\")\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "    print(\"   Using float32 precision (CPU mode)\")\n",
    "\n",
    "# Set up model loading configuration\n",
    "model_loading_config = {\n",
    "    \"model_name\": model_config[\"repo_id\"],\n",
    "    \"revision\": model_config[\"revision\"],\n",
    "    \"device_map\": model_config[\"device_map\"],\n",
    "    \"torch_dtype\": dtype,\n",
    "    \"quantization_config\": quantization_config,\n",
    "    \"cache_dir\": Path(model_config[\"cache_dir\"]),\n",
    "    \"trust_remote_code\": model_config[\"trust_remote_code\"]\n",
    "}\n",
    "\n",
    "# Save pre-loading state for comparison\n",
    "pre_loading_info = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model_name\": model_name,\n",
    "    \"gpu_info\": {\n",
    "        \"name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\",\n",
    "        \"total_memory_gb\": pre_memory[\"total_memory_gb\"],\n",
    "        \"allocated_memory_gb\": pre_memory[\"allocated_memory_gb\"],\n",
    "        \"free_memory_gb\": pre_memory[\"free_memory_gb\"]\n",
    "    },\n",
    "    \"quantization_enabled\": model_config[\"use_quantization\"],\n",
    "    \"quantization_bits\": model_config[\"quantization_bits\"],\n",
    "    \"precision\": str(dtype)\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "pre_loading_path = get_results_path(\"pre_loading_gpu_state.json\")\n",
    "with open(pre_loading_path, \"w\") as f:\n",
    "    # Convert non-serializable types\n",
    "    serializable_info = {**pre_loading_info}\n",
    "    serializable_info[\"precision\"] = str(serializable_info[\"precision\"])\n",
    "    json.dump(serializable_info, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Pre-loading GPU state saved to: {pre_loading_path}\")\n",
    "print(\"‚úÖ GPU verification complete and model loading configured!\")\n",
    "\n",
    "# Make model loading configuration available to next cell\n",
    "model_load_params = model_loading_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 01:04:49,567 - src.models.registry - INFO - Loaded configuration for model: mistral-community/pixtral-12b\n",
      "2025-03-18 01:04:49,568 - src.models.registry - INFO - Applied default quantization (bfloat16) for mistral-community/pixtral-12b\n",
      "2025-03-18 01:04:49,568 - src.models.registry - INFO - Removed 'default_strategy' from loading parameters (not for model constructor)\n",
      "2025-03-18 01:04:49,568 - src.models.loader - INFO - Loading model mistral-community/pixtral-12b from mistral-community/pixtral-12b\n",
      "2025-03-18 01:04:49,569 - src.models.loader - INFO - Loading parameters: {'torch_dtype': torch.bfloat16, 'device_map': 'auto', 'use_auth_token': False, 'use_flash_attention_2': False, 'attn_implementation': 'eager', 'cache_dir': PosixPath('/workspace/models/cache'), 'revision': None, 'quantization_config': None, 'trust_remote_code': True}\n",
      "2025-03-18 01:04:49,569 - src.models.loader - INFO - Initial GPU memory usage: 0.00 GB\n",
      "2025-03-18 01:04:49,569 - src.models.loader - INFO - GPU: NVIDIA H200\n",
      "2025-03-18 01:04:49,569 - src.models.loader - INFO - CUDA version: 11.8\n",
      "2025-03-18 01:04:49,570 - src.models.loader - INFO - Flash Attention not available, using eager implementation\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3513: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imported model loading utilities\n",
      "‚è≥ Loading pixtral-12b from mistral-community/pixtral-12b...\n",
      "   This may take several minutes depending on your internet connection and GPU speed.\n",
      "Using project model loading utility...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295cdb7dcbcd4cc490bbad7c4b3a5f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/997 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6599073527254bcf8bc2edce071d1667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/57.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ecabdb433c746b7bebbb2481f5061be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c609dec3c2c4de480a12bd22608433b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00006.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123de6f8993d417eabd38cd6e1955c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00006.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c886787e75034792badcc61bb04fa1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00006.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8c4a9781a3420c92da1a7f2e2c230a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00006.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88a6ebd38d245e28da68612a76f746b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00006.safetensors:   0%|          | 0.00/4.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc8a51e6f4d4ce78ee3991d8057d065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00006.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 01:07:17,658 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2025-03-18 01:07:17,691 - accelerate.utils.modeling - WARNING - The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6820ce706944638772e78bed55b8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afecb8997644c6a9fe26b0585fb360a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d43c1b790d42e78252f9dae273f674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/162 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25709a6bef0c4a2a80e66a79d9316a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92e3011143c4418aa009f30d45841c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97cb5dd3aee043f4944dfbca505e8261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/177k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e97c1a11ba44de090d83b7b4e5f8ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.26M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5aad02358a5402e9e0d988256848d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 01:07:24,116 - src.models.loader - INFO - Model loaded successfully using 25.37 GB of GPU memory\n",
      "2025-03-18 01:07:24,117 - src.models.loader - INFO - Total GPU memory usage: 25.37 GB\n",
      "2025-03-18 01:07:24,117 - src.models.loader - INFO - Model device: cuda:0\n",
      "2025-03-18 01:07:24,117 - src.models.loader - INFO - Model loading completed in 154.55 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully in 154.55 seconds!\n",
      "üìç Model loaded on device: cuda:0\n",
      "üî¢ Model data type: torch.bfloat16\n",
      "\n",
      "üß† Post-loading GPU memory state:\n",
      "   Total memory: 150.02 GB\n",
      "   Allocated memory: 25.37 GB\n",
      "   Free memory: 124.65 GB\n",
      "   Utilization: 16.9%\n",
      "üîç Model memory usage: 25.37 GB\n",
      "\n",
      "üíæ Model metadata saved to: /workspace/results/pixtral_extraction_20250318_002223/model_metadata.json\n",
      "\n",
      "üîç Verifying processor functionality...\n",
      "‚úÖ Processor verified: Can process text inputs\n",
      "‚úÖ Model and processor loaded successfully and ready for inference!\n"
     ]
    }
   ],
   "source": [
    "# Model Loading\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, AutoModelForVision2Seq\n",
    "from datetime import datetime\n",
    "\n",
    "# Try to import project-specific model loading utilities\n",
    "try:\n",
    "    from src.models.loader import load_model_and_processor, get_gpu_memory_info\n",
    "    print(\"‚úÖ Imported model loading utilities\")\n",
    "    use_project_utils = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Project model utilities not available, using direct implementation\")\n",
    "    use_project_utils = False\n",
    "\n",
    "# Record start time for loading\n",
    "start_time = time.time()\n",
    "\n",
    "# Display loading message\n",
    "model_name = model_config[\"name\"]\n",
    "repo_id = model_config[\"repo_id\"]\n",
    "print(f\"‚è≥ Loading {model_name} from {repo_id}...\")\n",
    "print(f\"   This may take several minutes depending on your internet connection and GPU speed.\")\n",
    "\n",
    "try:\n",
    "    # Method 1: Use project utility if available\n",
    "    if use_project_utils:\n",
    "        print(\"Using project model loading utility...\")\n",
    "        # Remove model_name from model_load_params to avoid duplicate\n",
    "        model_params = model_load_params.copy()\n",
    "        if 'model_name' in model_params:\n",
    "            del model_params['model_name']  # Remove to avoid duplication\n",
    "        \n",
    "        model, processor = load_model_and_processor(\n",
    "            model_name=model_name,  # Pass this directly\n",
    "            **model_params  # Pass remaining parameters\n",
    "        )\n",
    "    \n",
    "    # Method 2: Direct loading if project utility not available\n",
    "    else:\n",
    "        print(\"Using direct model loading...\")\n",
    "        \n",
    "        # Clear memory before loading\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Load the model with appropriate configuration\n",
    "        print(f\"Loading model: {repo_id}\")\n",
    "        model = LlavaForConditionalGeneration.from_pretrained(\n",
    "            repo_id,\n",
    "            torch_dtype=model_load_params[\"torch_dtype\"],\n",
    "            device_map=model_load_params[\"device_map\"],\n",
    "            quantization_config=model_load_params[\"quantization_config\"],\n",
    "            cache_dir=model_load_params[\"cache_dir\"],\n",
    "            trust_remote_code=model_load_params[\"trust_remote_code\"]\n",
    "        )\n",
    "        \n",
    "        # Load the processor\n",
    "        print(f\"Loading processor for: {repo_id}\")\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            repo_id,\n",
    "            cache_dir=model_load_params[\"cache_dir\"],\n",
    "            trust_remote_code=model_load_params[\"trust_remote_code\"]\n",
    "        )\n",
    "    \n",
    "    # Calculate loading time\n",
    "    loading_time = time.time() - start_time\n",
    "    print(f\"‚úÖ Model loaded successfully in {loading_time:.2f} seconds!\")\n",
    "    \n",
    "    # Get model device information\n",
    "    if hasattr(model, \"device\"):\n",
    "        model_device = model.device\n",
    "    elif hasattr(model, \"hf_device_map\"):\n",
    "        model_device = str(model.hf_device_map)\n",
    "    else:\n",
    "        model_device = \"Unknown\"\n",
    "    \n",
    "    print(f\"üìç Model loaded on device: {model_device}\")\n",
    "    \n",
    "    # Get model dtype information\n",
    "    if hasattr(model, \"dtype\"):\n",
    "        model_dtype = str(model.dtype)\n",
    "    else:\n",
    "        # Try to infer from parameters\n",
    "        for param in model.parameters():\n",
    "            model_dtype = str(param.dtype)\n",
    "            break\n",
    "    \n",
    "    print(f\"üî¢ Model data type: {model_dtype}\")\n",
    "    \n",
    "    # Display post-loading memory state\n",
    "    print(\"\\nüß† Post-loading GPU memory state:\")\n",
    "    post_memory = get_gpu_memory_info()\n",
    "    print(f\"   Total memory: {post_memory['total_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Allocated memory: {post_memory['allocated_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Free memory: {post_memory['free_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Utilization: {post_memory.get('utilization', post_memory.get('utilization_percent', 0)):.1f}%\")\n",
    "    \n",
    "    # Calculate memory usage for the model\n",
    "    memory_used = post_memory['allocated_memory_gb'] - pre_memory['allocated_memory_gb']\n",
    "    print(f\"üîç Model memory usage: {memory_used:.2f} GB\")\n",
    "    \n",
    "    # Store model metadata\n",
    "    model_metadata = {\n",
    "        \"name\": model_name,\n",
    "        \"repo_id\": repo_id,\n",
    "        \"loading_time_seconds\": loading_time,\n",
    "        \"device\": str(model_device),\n",
    "        \"dtype\": model_dtype,\n",
    "        \"loaded_at\": datetime.now().isoformat(),\n",
    "        \"memory_usage_gb\": memory_used,\n",
    "        \"quantization\": {\n",
    "            \"enabled\": model_config[\"use_quantization\"],\n",
    "            \"bits\": model_config[\"quantization_bits\"] if model_config[\"use_quantization\"] else None\n",
    "        },\n",
    "        \"gpu_info\": {\n",
    "            \"name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\",\n",
    "            \"pre_loading_memory_gb\": pre_memory[\"allocated_memory_gb\"],\n",
    "            \"post_loading_memory_gb\": post_memory[\"allocated_memory_gb\"],\n",
    "            \"total_memory_gb\": post_memory[\"total_memory_gb\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save model metadata\n",
    "    model_metadata_path = get_results_path(\"model_metadata.json\")\n",
    "    with open(model_metadata_path, \"w\") as f:\n",
    "        json.dump(model_metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Model metadata saved to: {model_metadata_path}\")\n",
    "    \n",
    "    # Verify processor functionality\n",
    "    print(\"\\nüîç Verifying processor functionality...\")\n",
    "    \n",
    "    # Simple test to verify processor\n",
    "    sample_text = \"Extract the work order number from this invoice image.\"\n",
    "    try:\n",
    "        # Just verify it can process a simple example without running inference\n",
    "        sample_input = processor(text=sample_text, return_tensors=\"pt\")\n",
    "        print(\"‚úÖ Processor verified: Can process text inputs\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Processor test failed for text processing: {e}\")\n",
    "    \n",
    "    print(\"‚úÖ Model and processor loaded successfully and ready for inference!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    # Attempt to free memory in case of loading failure\n",
    "    if 'model' in locals():\n",
    "        del model\n",
    "    if 'processor' in locals():\n",
    "        del processor\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    # Raise the exception to halt execution\n",
    "    raise RuntimeError(f\"Failed to load model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Pipeline\n",
    "### Pipeline Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 02:12:23,784 - src.config.paths - INFO - Path configuration initialized for experiment: pixtral_extraction_20250318_002223\n",
      "2025-03-18 02:12:23,785 - src.execution.pipeline - INFO - Extraction pipeline initialized for experiment: pixtral_extraction_20250318_002223\n",
      "2025-03-18 02:12:23,785 - src.execution.pipeline - INFO - Results will be stored in: /workspace/results/raw/pixtral_extraction_20250318_002223\n",
      "2025-03-18 02:12:23,785 - src.execution.pipeline - INFO - Raw results: /workspace/results/raw/pixtral_extraction_20250318_002223/raw\n",
      "2025-03-18 02:12:23,785 - src.execution.pipeline - INFO - Processed results: /workspace/results/raw/pixtral_extraction_20250318_002223/processed\n",
      "2025-03-18 02:12:23,786 - src.execution.pipeline - INFO - Visualizations: /workspace/results/raw/pixtral_extraction_20250318_002223/visualizations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imported extraction pipeline\n",
      "üöÄ Initializing extraction pipeline...\n",
      "\n",
      "‚öôÔ∏è Configuring pipeline settings...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ExtractionPipeline' object has no attribute 'configure'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 129\u001b[0m\n\u001b[1;32m    114\u001b[0m pipeline_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiment_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: experiment_name,\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m: timestamp\n\u001b[1;32m    126\u001b[0m }\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Apply configuration to pipeline\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigure\u001b[49m(pipeline_config)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Set up the model and processor in the pipeline\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müîÑ Setting up model and processor in pipeline...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ExtractionPipeline' object has no attribute 'configure'"
     ]
    }
   ],
   "source": [
    "# Pipeline Initialization\n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "# Try to import project-specific pipeline utilities\n",
    "try:\n",
    "    from src.execution.pipeline import ExtractionPipeline\n",
    "    print(\"‚úÖ Imported extraction pipeline\")\n",
    "    use_project_pipeline = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Project pipeline not available, using direct implementation\")\n",
    "    use_project_pipeline = False\n",
    "    \n",
    "    # Define a simple pipeline class if the project one isn't available\n",
    "    class ExtractionPipeline:\n",
    "        \"\"\"Basic extraction pipeline implementation\"\"\"\n",
    "        def __init__(self, experiment_name):\n",
    "            self.experiment_name = experiment_name\n",
    "            self.config = {}\n",
    "            self.model = None\n",
    "            self.processor = None\n",
    "            self.ground_truth_mapping = {}\n",
    "            self.results = []\n",
    "            self.batch_size = 1\n",
    "            self.checkpoints = {}\n",
    "            self.current_batch = 0\n",
    "            \n",
    "        def configure(self, config):\n",
    "            \"\"\"Configure the pipeline with settings\"\"\"\n",
    "            self.config.update(config)\n",
    "            return self\n",
    "            \n",
    "        def setup_model(self, model=None, processor=None):\n",
    "            \"\"\"Set up model and processor\"\"\"\n",
    "            if model is not None:\n",
    "                self.model = model\n",
    "            if processor is not None:\n",
    "                self.processor = processor\n",
    "            return self\n",
    "            \n",
    "        def load_ground_truth(self, ground_truth_mapping):\n",
    "            \"\"\"Load ground truth data\"\"\"\n",
    "            self.ground_truth_mapping = ground_truth_mapping\n",
    "            return self\n",
    "            \n",
    "        def determine_optimal_batch_size(self, start_size=1, max_size=8):\n",
    "            \"\"\"Determine the optimal batch size for the GPU\"\"\"\n",
    "            # In a real implementation, this would test different batch sizes\n",
    "            # For this fallback, we'll just use the config value or default to 1\n",
    "            self.batch_size = self.config.get(\"batch_processing\", {}).get(\"batch_size\", 1)\n",
    "            return self.batch_size\n",
    "\n",
    "# Initialize pipeline\n",
    "print(\"üöÄ Initializing extraction pipeline...\")\n",
    "experiment_name = experiment_config[\"experiment\"][\"name\"]\n",
    "\n",
    "# Create pipeline instance\n",
    "pipeline = ExtractionPipeline(experiment_name=experiment_name)\n",
    "\n",
    "# Set up batch processing configuration\n",
    "print(\"\\n‚öôÔ∏è Setting up pipeline configuration...\")\n",
    "batch_config = {\n",
    "    \"auto_batch_size\": True,  # Determine optimal batch size automatically\n",
    "    \"max_batch_size\": 8,      # Maximum batch size to consider\n",
    "    \"default_batch_size\": 1,  # Default if auto-estimation fails\n",
    "    \"optimize_memory\": True   # Clean up memory between batches\n",
    "}\n",
    "\n",
    "# Set up checkpointing configuration\n",
    "checkpoint_config = {\n",
    "    \"enable_checkpoints\": True,\n",
    "    \"checkpoint_frequency\": 5,  # Save checkpoint after every 5 batches\n",
    "    \"checkpoint_path\": str(get_results_path(\"extraction_checkpoint.json\")),\n",
    "    \"resume_from_checkpoint\": True  # Resume from checkpoint if available\n",
    "}\n",
    "\n",
    "# Set up metrics configuration\n",
    "metrics_config = {\n",
    "    \"metrics\": [\"exact_match\", \"character_error_rate\", \"levenshtein_distance\", \"processing_time\"],\n",
    "    \"case_sensitive\": False,\n",
    "    \"strip_whitespace\": True\n",
    "}\n",
    "\n",
    "# Create complete pipeline configuration dict\n",
    "pipeline_config = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"model_name\": model_config[\"name\"],\n",
    "    \"field_to_extract\": extraction_config[\"field_to_extract\"],\n",
    "    \"prompt_name\": prompt_info[\"name\"],\n",
    "    \"prompt_category\": prompt_info[\"category\"],\n",
    "    \"description\": experiment_config[\"experiment\"][\"description\"],\n",
    "    \"batch_processing\": batch_config,\n",
    "    \"checkpointing\": checkpoint_config,\n",
    "    \"metrics\": metrics_config,\n",
    "    \"environment\": env_config.environment,\n",
    "    \"timestamp\": timestamp\n",
    "}\n",
    "\n",
    "# Update pipeline's config dictionary directly\n",
    "for key, value in pipeline_config.items():\n",
    "    pipeline.config[key] = value\n",
    "\n",
    "print(f\"‚úÖ Updated pipeline configuration with {len(pipeline_config)} parameters\")\n",
    "\n",
    "# Set up the model and processor in the pipeline\n",
    "print(\"\\nüîÑ Setting up model and processor in pipeline...\")\n",
    "pipeline.setup_model(model=model, processor=processor)\n",
    "\n",
    "# Load ground truth data\n",
    "print(\"\\nüìä Loading ground truth data into pipeline...\")\n",
    "pipeline.load_ground_truth(field_column=extraction_config[\"field_to_extract\"])\n",
    "\n",
    "# Determine optimal batch size if auto-batch is enabled\n",
    "if batch_config[\"auto_batch_size\"]:\n",
    "    print(\"\\nüßÆ Determining optimal batch size...\")\n",
    "    try:\n",
    "        # Prepare a sample task to estimate batch size\n",
    "        sample_items = pipeline.prepare_extraction_task(limit=1)\n",
    "        \n",
    "        if sample_items and hasattr(pipeline, \"determine_optimal_batch_size\"):\n",
    "            optimal_batch_size = pipeline.determine_optimal_batch_size(\n",
    "                start_size=1,\n",
    "                max_size=batch_config[\"max_batch_size\"]\n",
    "            )\n",
    "            print(f\"‚úÖ Determined optimal batch size: {optimal_batch_size}\")\n",
    "        else:\n",
    "            # Fallback to default\n",
    "            optimal_batch_size = batch_config[\"default_batch_size\"]\n",
    "            print(f\"‚ÑπÔ∏è Using default batch size: {optimal_batch_size}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error determining batch size: {e}\")\n",
    "        print(f\"‚ÑπÔ∏è Falling back to default batch size: {batch_config['default_batch_size']}\")\n",
    "        optimal_batch_size = batch_config[\"default_batch_size\"]\n",
    "    \n",
    "    # Update config with determined batch size\n",
    "    pipeline.config[\"batch_processing\"][\"batch_size\"] = optimal_batch_size\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è Using fixed batch size: {batch_config.get('batch_size', batch_config['default_batch_size'])}\")\n",
    "\n",
    "# Check for existing checkpoint\n",
    "checkpoint_path = checkpoint_config[\"checkpoint_path\"]\n",
    "if os.path.exists(checkpoint_path) and checkpoint_config[\"resume_from_checkpoint\"]:\n",
    "    print(f\"\\nüîÑ Found existing checkpoint at: {checkpoint_path}\")\n",
    "    try:\n",
    "        with open(checkpoint_path, 'r') as f:\n",
    "            checkpoint = json.load(f)\n",
    "        if isinstance(checkpoint, dict) and \"batch_idx\" in checkpoint:\n",
    "            print(f\"‚ÑπÔ∏è Checkpoint from: {datetime.fromtimestamp(checkpoint.get('timestamp', 0)).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            print(f\"‚ÑπÔ∏è Already processed {len(checkpoint.get('processed_items', []))} items\")\n",
    "            print(f\"‚ÑπÔ∏è Will resume from batch {checkpoint.get('batch_idx', 0)}\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è Checkpoint format is different, will start from beginning\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading checkpoint: {e}\")\n",
    "        print(\"‚ÑπÔ∏è Will start processing from the beginning\")\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è No existing checkpoint found or resume disabled\")\n",
    "    print(\"‚ÑπÔ∏è Will start processing from the beginning\")\n",
    "\n",
    "# Prepare for extraction\n",
    "print(\"\\nüìã Pipeline initialization summary:\")\n",
    "print(f\"   Experiment: {pipeline.config['experiment_name']}\")\n",
    "print(f\"   Model: {pipeline.config['model_name']}\")\n",
    "print(f\"   Field to extract: {pipeline.config['field_to_extract']}\")\n",
    "print(f\"   Prompt: {pipeline.config['prompt_name']}\")\n",
    "print(f\"   Batch size: {pipeline.config['batch_processing'].get('batch_size', batch_config['default_batch_size'])}\")\n",
    "print(f\"   Checkpointing: {'Enabled' if checkpoint_config['enable_checkpoints'] else 'Disabled'}\")\n",
    "print(f\"   Images to process: {len(pipeline.ground_truth_mapping) if hasattr(pipeline, 'ground_truth_mapping') else 'Unknown'}\")\n",
    "\n",
    "# Save pipeline configuration\n",
    "pipeline_config_path = get_results_path(\"pipeline_config.json\")\n",
    "with open(pipeline_config_path, \"w\") as f:\n",
    "    # Convert any non-serializable objects to strings\n",
    "    serializable_config = json.loads(json.dumps(pipeline.config, default=str))\n",
    "    json.dump(serializable_config, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Pipeline configuration saved to: {pipeline_config_path}\")\n",
    "print(\"‚úÖ Pipeline initialization complete and ready for extraction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Image Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Image Test\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import time\n",
    "from PIL import Image\n",
    "from Levenshtein import distance\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Try to import project-specific inference utilities\n",
    "try:\n",
    "    from src.execution.inference import process_image_with_metrics\n",
    "    print(\"‚úÖ Imported inference utilities\")\n",
    "    use_project_utils = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Project inference utilities not available, using direct implementation\")\n",
    "    use_project_utils = False\n",
    "    \n",
    "    # Define a direct implementation for single image processing\n",
    "    def process_image_with_metrics(image_path, ground_truth, prompt, model_name, field_type, model, processor, metrics=None):\n",
    "        \"\"\"Process a single image and calculate metrics against ground truth\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Load the image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "            # Format the prompt with instruction tags\n",
    "            formatted_prompt = prompt[\"formatted_text\"]\n",
    "            \n",
    "            # Process using processor\n",
    "            inputs = processor(\n",
    "                text=formatted_prompt,\n",
    "                images=[image],  # Notice we're passing a list here\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Convert inputs to appropriate dtypes\n",
    "            for key in inputs:\n",
    "                if key == \"pixel_values\":\n",
    "                    inputs[key] = inputs[key].to(dtype=next(model.parameters()).dtype, device=model.device)\n",
    "                else:\n",
    "                    inputs[key] = inputs[key].to(device=model.device)\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_new_tokens=50, do_sample=False)\n",
    "            \n",
    "            # Decode the output\n",
    "            raw_extraction = processor.batch_decode(\n",
    "                outputs, \n",
    "                skip_special_tokens=True, \n",
    "                clean_up_tokenization_spaces=False\n",
    "            )[0]\n",
    "            \n",
    "            # Process the output - clean up whitespace and extract just numbers if needed\n",
    "            processed_extraction = raw_extraction.strip()\n",
    "            \n",
    "            # Calculate processing time\n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            # Calculate metrics\n",
    "            result = {\n",
    "                \"raw_extraction\": raw_extraction,\n",
    "                \"processed_extraction\": processed_extraction,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"processing_time\": processing_time\n",
    "            }\n",
    "            \n",
    "            # Calculate exact match\n",
    "            exact_match = processed_extraction.lower() == ground_truth.lower()\n",
    "            result[\"exact_match\"] = exact_match\n",
    "            \n",
    "            # Calculate character error rate (Levenshtein distance normalized by length)\n",
    "            if len(ground_truth) > 0:\n",
    "                cer = distance(processed_extraction, ground_truth) / len(ground_truth)\n",
    "            else:\n",
    "                cer = 1.0 if len(processed_extraction) > 0 else 0.0\n",
    "            result[\"character_error_rate\"] = cer\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            processing_time = time.time() - start_time\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"processing_time\": processing_time,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"raw_extraction\": \"ERROR\",\n",
    "                \"processed_extraction\": \"ERROR\",\n",
    "                \"exact_match\": False,\n",
    "                \"character_error_rate\": 1.0\n",
    "            }\n",
    "\n",
    "# Verify we have the pipeline, model, and ground truth data\n",
    "if 'pipeline' not in locals():\n",
    "    print(\"‚ùå Pipeline not initialized. Run the pipeline initialization cell first.\")\n",
    "    raise RuntimeError(\"Pipeline not initialized\")\n",
    "    \n",
    "if 'model' not in locals() or model is None:\n",
    "    print(\"‚ùå Model not loaded. Run the model loading cell first.\")\n",
    "    raise RuntimeError(\"Model not loaded\")\n",
    "    \n",
    "if len(ground_truth_mapping) == 0:\n",
    "    print(\"‚ùå No ground truth data available. Run the ground truth data loading cell first.\")\n",
    "    raise RuntimeError(\"No ground truth data available\")\n",
    "\n",
    "print(\"üß™ Running a test on a single image before full pipeline execution...\")\n",
    "\n",
    "# Select a sample image\n",
    "print(\"\\nüîç Selecting a test image...\")\n",
    "try:\n",
    "    # Get all image paths\n",
    "    image_paths = list(Path(paths[\"images_dir\"]).glob(\"*.jpg\")) + list(Path(paths[\"images_dir\"]).glob(\"*.png\"))\n",
    "    \n",
    "    if not image_paths:\n",
    "        raise FileNotFoundError(\"No image files found\")\n",
    "    \n",
    "    # Choose a random image that has ground truth data\n",
    "    valid_images = [img for img in image_paths if img.stem in ground_truth_mapping]\n",
    "    \n",
    "    if not valid_images:\n",
    "        print(\"‚ö†Ô∏è No images with ground truth found, using a random image\")\n",
    "        test_image = random.choice(image_paths)\n",
    "    else:\n",
    "        test_image = random.choice(valid_images)\n",
    "    \n",
    "    # Get ground truth for the test image\n",
    "    test_image_id = test_image.stem\n",
    "    ground_truth = ground_truth_mapping.get(test_image_id, {}).get('work_order', 'Unknown')\n",
    "    \n",
    "    print(f\"‚úÖ Selected test image: {test_image.name}\")\n",
    "    print(f\"üìã Ground truth value: '{ground_truth}'\")\n",
    "    \n",
    "    # Display the test image\n",
    "    print(\"\\nüì∏ Test Image:\")\n",
    "    img = Image.open(test_image)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Test Invoice: {test_image_id}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Run the test using the pipeline or direct implementation\n",
    "    print(\"\\n‚è≥ Processing test image...\")\n",
    "    \n",
    "    test_start_time = time.time()\n",
    "    \n",
    "    if hasattr(pipeline, \"process_single_image\") and callable(getattr(pipeline, \"process_single_image\")):\n",
    "        # Use pipeline's method if available\n",
    "        result = pipeline.process_single_image(\n",
    "            image_path=str(test_image),\n",
    "            ground_truth=ground_truth,\n",
    "            prompt=prompt_info,\n",
    "            metrics=[\"exact_match\", \"character_error_rate\"]\n",
    "        )\n",
    "    else:\n",
    "        # Use direct implementation\n",
    "        result = process_image_with_metrics(\n",
    "            image_path=str(test_image),\n",
    "            ground_truth=ground_truth,\n",
    "            prompt=prompt_info,\n",
    "            model_name=model_config[\"name\"],\n",
    "            field_type=extraction_config[\"field_to_extract\"],\n",
    "            model=model,\n",
    "            processor=processor,\n",
    "            metrics=[\"exact_match\", \"character_error_rate\"]\n",
    "        )\n",
    "    \n",
    "    test_total_time = time.time() - test_start_time\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nüìä Test Results:\")\n",
    "    print(f\"   Raw Extraction: '{result.get('raw_extraction', 'ERROR')}'\")\n",
    "    print(f\"   Processed Extraction: '{result.get('processed_extraction', 'ERROR')}'\")\n",
    "    print(f\"   Ground Truth: '{result.get('ground_truth', 'Unknown')}'\")\n",
    "    print(f\"   Exact Match: {result.get('exact_match', False)}\")\n",
    "    print(f\"   Character Error Rate: {result.get('character_error_rate', 1.0):.4f}\")\n",
    "    print(f\"   Processing Time: {result.get('processing_time', 0):.2f} seconds\")\n",
    "    print(f\"   Total Test Time: {test_total_time:.2f} seconds\")\n",
    "    \n",
    "    # Provide assessment of test quality\n",
    "    if \"error\" in result:\n",
    "        print(f\"\\n‚ùå Test failed with error: {result['error']}\")\n",
    "        print(\"   Fix the error before proceeding to full extraction\")\n",
    "    elif result.get(\"exact_match\", False):\n",
    "        print(\"\\n‚úÖ Test successful! Extraction matches ground truth.\")\n",
    "        print(\"   Ready to proceed with full extraction.\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Test extraction does not exactly match ground truth.\")\n",
    "        \n",
    "        # Analyze the difference\n",
    "        gt = result.get('ground_truth', '')\n",
    "        extracted = result.get('processed_extraction', '')\n",
    "        \n",
    "        if result.get('character_error_rate', 1.0) < 0.2:\n",
    "            print(\"   ‚ö†Ô∏è Minor differences detected - extraction is close to ground truth\")\n",
    "            print(f\"   Ground Truth: '{gt}'\")\n",
    "            print(f\"   Extraction:   '{extracted}'\")\n",
    "            # Show differing characters\n",
    "            diff_positions = [i for i in range(min(len(gt), len(extracted))) if gt[i] != extracted[i]]\n",
    "            if diff_positions:\n",
    "                print(f\"   Differences at positions: {diff_positions}\")\n",
    "                for pos in diff_positions:\n",
    "                    print(f\"   Position {pos}: '{gt[pos]}' vs '{extracted[pos]}'\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è Significant differences detected\")\n",
    "            print(f\"   Ground Truth: '{gt}'\")\n",
    "            print(f\"   Extraction:   '{extracted}'\")\n",
    "            print(f\"   Character Error Rate: {result.get('character_error_rate', 1.0):.4f}\")\n",
    "    \n",
    "    # Save test result\n",
    "    test_result_path = get_results_path(\"single_image_test_result.json\")\n",
    "    with open(test_result_path, \"w\") as f:\n",
    "        # Add image information to result\n",
    "        result[\"image_id\"] = test_image_id\n",
    "        result[\"image_path\"] = str(test_image)\n",
    "        result[\"test_timestamp\"] = datetime.now().isoformat()\n",
    "        result[\"total_test_time\"] = test_total_time\n",
    "        \n",
    "        # Convert to serializable format\n",
    "        import json\n",
    "        json.dump(result, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Test result saved to: {test_result_path}\")\n",
    "    \n",
    "    # Store the result for reference\n",
    "    test_result = result\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during single image test: {e}\")\n",
    "    test_result = {\"error\": str(e)}\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Extraction Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Extraction Run\n",
    "\n",
    "# Import required libraries\n",
    "import time\n",
    "import torch\n",
    "import gc\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Verify pipeline, model, and other requirements are available\n",
    "if 'pipeline' not in locals():\n",
    "    print(\"‚ùå Pipeline not initialized. Run the pipeline initialization cell first.\")\n",
    "    raise RuntimeError(\"Pipeline not initialized\")\n",
    "    \n",
    "if 'model' not in locals() or model is None:\n",
    "    print(\"‚ùå Model not loaded. Run the model loading cell first.\")\n",
    "    raise RuntimeError(\"Model not loaded\")\n",
    "    \n",
    "if 'batch_items' not in locals() or len(batch_items) == 0:\n",
    "    print(\"‚ùå No batch items available. Run the ground truth data loading cell first.\")\n",
    "    raise RuntimeError(\"No batch items available\")\n",
    "\n",
    "# Verify test was successful before proceeding\n",
    "if 'test_result' not in locals() or 'error' in test_result:\n",
    "    print(\"‚ö†Ô∏è Warning: Single image test did not complete successfully.\")\n",
    "    proceed = input(\"Do you want to proceed with full extraction anyway? (y/n): \").strip().lower()\n",
    "    if proceed != 'y':\n",
    "        raise RuntimeError(\"Full extraction aborted by user. Run the single image test first.\")\n",
    "\n",
    "# Define utility functions for the extraction run\n",
    "def format_eta(seconds):\n",
    "    \"\"\"Format seconds into a human-readable ETA string\"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.0f} seconds\"\n",
    "    elif seconds < 3600:\n",
    "        minutes = seconds / 60\n",
    "        return f\"{minutes:.1f} minutes\"\n",
    "    else:\n",
    "        hours = seconds / 3600\n",
    "        return f\"{hours:.2f} hours\"\n",
    "\n",
    "def estimate_completion_time(current, total, elapsed_time):\n",
    "    \"\"\"Estimate completion time based on progress\"\"\"\n",
    "    if current == 0 or elapsed_time <= 0:\n",
    "        return \"Calculating...\"\n",
    "    \n",
    "    items_per_second = current / elapsed_time\n",
    "    remaining_items = total - current\n",
    "    \n",
    "    if items_per_second > 0:\n",
    "        remaining_seconds = remaining_items / items_per_second\n",
    "        eta = format_eta(remaining_seconds)\n",
    "        completion_time = datetime.now() + timedelta(seconds=remaining_seconds)\n",
    "        formatted_time = completion_time.strftime(\"%H:%M:%S\")\n",
    "        return f\"{eta} (completion around {formatted_time})\"\n",
    "    else:\n",
    "        return \"Unable to estimate\"\n",
    "\n",
    "def log_memory_usage(step, info=None):\n",
    "    \"\"\"Log GPU memory usage for monitoring\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    \n",
    "    memory_info = {\n",
    "        \"step\": step,\n",
    "        \"timestamp\": time.time(),\n",
    "        \"total_memory_gb\": torch.cuda.get_device_properties(0).total_memory / 1e9,\n",
    "        \"allocated_memory_gb\": torch.cuda.memory_allocated(0) / 1e9,\n",
    "        \"reserved_memory_gb\": torch.cuda.memory_reserved(0) / 1e9,\n",
    "        \"info\": info\n",
    "    }\n",
    "    \n",
    "    # Calculate free memory (total - allocated)\n",
    "    memory_info[\"free_memory_gb\"] = memory_info[\"total_memory_gb\"] - memory_info[\"allocated_memory_gb\"]\n",
    "    \n",
    "    return memory_info\n",
    "\n",
    "# Set up checkpoint path for resuming\n",
    "checkpoint_path = pipeline_config[\"checkpointing\"][\"checkpoint_path\"]\n",
    "checkpoint_frequency = pipeline_config[\"checkpointing\"][\"checkpoint_frequency\"]\n",
    "enable_checkpoints = pipeline_config[\"checkpointing\"][\"enable_checkpoints\"]\n",
    "\n",
    "# Check for existing checkpoint if resuming is enabled\n",
    "processed_items = []\n",
    "start_batch = 0\n",
    "if os.path.exists(checkpoint_path) and pipeline_config[\"checkpointing\"][\"resume_from_checkpoint\"]:\n",
    "    try:\n",
    "        with open(checkpoint_path, \"r\") as f:\n",
    "            checkpoint = json.load(f)\n",
    "        \n",
    "        processed_items = checkpoint.get(\"processed_items\", [])\n",
    "        start_batch = checkpoint.get(\"batch_idx\", 0)\n",
    "        \n",
    "        print(f\"üìÇ Resuming from checkpoint: {len(processed_items)} items already processed\")\n",
    "        print(f\"   Starting from batch {start_batch}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading checkpoint: {e}\")\n",
    "        print(\"   Starting from the beginning\")\n",
    "        processed_items = []\n",
    "        start_batch = 0\n",
    "\n",
    "# Determine batch size to use\n",
    "batch_size = pipeline_config[\"batch_processing\"].get(\"batch_size\", 1)\n",
    "print(f\"üî¢ Using batch size: {batch_size}\")\n",
    "\n",
    "# Determine number of items to process\n",
    "total_items = len(batch_items)\n",
    "if 'limit' in locals() and limit is not None and limit < total_items:\n",
    "    print(f\"‚öôÔ∏è Limiting extraction to first {limit} items (out of {total_items})\")\n",
    "    total_items = limit\n",
    "    batch_items = batch_items[:limit]\n",
    "else:\n",
    "    print(f\"‚öôÔ∏è Processing all {total_items} items\")\n",
    "\n",
    "# Remove already processed items if resuming\n",
    "if processed_items:\n",
    "    # Get IDs of already processed items\n",
    "    processed_ids = [item[\"image_id\"] for item in processed_items]\n",
    "    # Filter out already processed items\n",
    "    batch_items = [item for item in batch_items if item[\"image_id\"] not in processed_ids]\n",
    "    print(f\"üîÑ Removed {len(processed_ids)} already processed items\")\n",
    "    print(f\"   Remaining items to process: {len(batch_items)}\")\n",
    "\n",
    "# Check if all items have already been processed\n",
    "if not batch_items:\n",
    "    print(\"‚úÖ All items have already been processed.\")\n",
    "    print(f\"üìä Total processed items: {len(processed_items)}\")\n",
    "    \n",
    "    # Calculate final statistics from processed items\n",
    "    exact_matches = sum(1 for r in processed_items if r.get(\"exact_match\", False))\n",
    "    overall_accuracy = (exact_matches / len(processed_items) * 100) if processed_items and len(processed_items) > 0 else 0\n",
    "    \n",
    "    print(f\"üìä Exact Match Accuracy: {exact_matches}/{len(processed_items)} ({overall_accuracy:.1f}%)\")\n",
    "    \n",
    "    # Save final results\n",
    "    results_path = get_results_path(\"extraction_results.json\")\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(processed_items, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Results saved to: {results_path}\")\n",
    "    \n",
    "    # Store results for analysis\n",
    "    extraction_results = processed_items\n",
    "    print(\"üíæ No need to run extraction again.\")\n",
    "    \n",
    "else:\n",
    "    # Set up memory monitoring\n",
    "    memory_log = []\n",
    "    memory_log.append(log_memory_usage(\"start\", \"Before extraction starts\"))\n",
    "\n",
    "    # Display confirmation and execution plan\n",
    "    print(\"\\nüöÄ Starting full extraction pipeline...\")\n",
    "    print(f\"üìä Processing {len(batch_items)} items with batch size {batch_size}\")\n",
    "    print(f\"üíæ Checkpointing: {'Enabled' if enable_checkpoints else 'Disabled'}\")\n",
    "    if enable_checkpoints:\n",
    "        print(f\"   Saving checkpoint every {checkpoint_frequency} batches to: {checkpoint_path}\")\n",
    "\n",
    "    # Ask for confirmation before proceeding\n",
    "    proceed = input(\"\\nüí¨ Proceed with full extraction? This may take a while. (y/n): \").strip().lower()\n",
    "    if proceed != 'y':\n",
    "        print(\"‚ùå Extraction canceled by user.\")\n",
    "        raise RuntimeError(\"Full extraction canceled by user.\")\n",
    "\n",
    "    # Record start time for the full extraction\n",
    "    full_start_time = time.time()\n",
    "\n",
    "    # Prepare for batch processing\n",
    "    all_results = processed_items.copy()  # Start with already processed items if any\n",
    "    num_batches = math.ceil(len(batch_items) / batch_size)\n",
    "\n",
    "    print(f\"\\n‚è≥ Processing {len(batch_items)} items in {num_batches} batches...\")\n",
    "\n",
    "    try:\n",
    "        # Process in batches\n",
    "        for batch_idx in range(start_batch, num_batches):\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            # Get current batch\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(batch_items))\n",
    "            current_batch = batch_items[start_idx:end_idx]\n",
    "            \n",
    "            # Skip if batch is empty (shouldn't happen, but just in case)\n",
    "            if not current_batch:\n",
    "                print(f\"‚ö†Ô∏è Batch {batch_idx+1}/{num_batches} is empty, skipping\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nüîÑ Processing Batch {batch_idx+1}/{num_batches} ({len(current_batch)} items)\")\n",
    "            \n",
    "            # Log memory before batch\n",
    "            memory_log.append(log_memory_usage(f\"batch_{batch_idx}_start\", f\"Before batch {batch_idx+1}\"))\n",
    "            \n",
    "            # Process each item in the batch\n",
    "            batch_results = []\n",
    "            for i, item in enumerate(current_batch):\n",
    "                item_start_time = time.time()\n",
    "                \n",
    "                # Display progress\n",
    "                overall_progress = len(all_results) + len(batch_results) + 1\n",
    "                elapsed_time = time.time() - full_start_time\n",
    "                if elapsed_time > 0 and overall_progress > 0:\n",
    "                    eta = estimate_completion_time(overall_progress, len(batch_items) + len(processed_items), elapsed_time)\n",
    "                else:\n",
    "                    eta = \"Calculating...\"\n",
    "                \n",
    "                print(f\"\\n‚è≥ Item {overall_progress}/{len(batch_items) + len(processed_items)} | \" \n",
    "                      f\"Batch {batch_idx+1}.{i+1}/{batch_idx+1}.{len(current_batch)} | ETA: {eta}\")\n",
    "                \n",
    "                # Process the item\n",
    "                try:\n",
    "                    if hasattr(pipeline, \"process_single_image\") and callable(getattr(pipeline, \"process_single_image\")):\n",
    "                        # Use pipeline's method if available\n",
    "                        result = pipeline.process_single_image(\n",
    "                            image_path=item[\"image_path\"],\n",
    "                            ground_truth=item[\"ground_truth\"],\n",
    "                            prompt=prompt_info,\n",
    "                            metrics=pipeline_config[\"metrics\"][\"metrics\"]\n",
    "                        )\n",
    "                    else:\n",
    "                        # Use direct implementation\n",
    "                        result = process_image_with_metrics(\n",
    "                            image_path=item[\"image_path\"],\n",
    "                            ground_truth=item[\"ground_truth\"],\n",
    "                            prompt=prompt_info,\n",
    "                            model_name=model_config[\"name\"],\n",
    "                            field_type=extraction_config[\"field_to_extract\"],\n",
    "                            model=model,\n",
    "                            processor=processor,\n",
    "                            metrics=pipeline_config[\"metrics\"][\"metrics\"]\n",
    "                        )\n",
    "                    \n",
    "                    # Add image ID to result\n",
    "                    result[\"image_id\"] = item[\"image_id\"]\n",
    "                    \n",
    "                    # Add timing information\n",
    "                    result[\"processing_timestamp\"] = datetime.now().isoformat()\n",
    "                    result[\"batch_idx\"] = batch_idx\n",
    "                    result[\"item_idx\"] = i\n",
    "                    \n",
    "                    # Display result\n",
    "                    exact_match = result.get(\"exact_match\", False)\n",
    "                    match_symbol = \"‚úì\" if exact_match else \"‚úó\"\n",
    "                    print(f\"   Result: {match_symbol} | Extracted: '{result.get('processed_extraction', 'ERROR')}' | \"\n",
    "                          f\"GT: '{result.get('ground_truth', 'Unknown')}' | \"\n",
    "                          f\"CER: {result.get('character_error_rate', 1.0):.4f} | \"\n",
    "                          f\"Time: {result.get('processing_time', 0):.2f}s\")\n",
    "                    \n",
    "                    # Add to batch results\n",
    "                    batch_results.append(result)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error processing item {item['image_id']}: {e}\")\n",
    "                    # Add error result\n",
    "                    error_result = {\n",
    "                        \"image_id\": item[\"image_id\"],\n",
    "                        \"error\": str(e),\n",
    "                        \"processing_timestamp\": datetime.now().isoformat(),\n",
    "                        \"batch_idx\": batch_idx,\n",
    "                        \"item_idx\": i,\n",
    "                        \"ground_truth\": item[\"ground_truth\"],\n",
    "                        \"exact_match\": False,\n",
    "                        \"character_error_rate\": 1.0\n",
    "                    }\n",
    "                    batch_results.append(error_result)\n",
    "            \n",
    "            # Add batch results to all results\n",
    "            all_results.extend(batch_results)\n",
    "            \n",
    "            # Log memory after batch\n",
    "            memory_log.append(log_memory_usage(f\"batch_{batch_idx}_end\", f\"After batch {batch_idx+1}\"))\n",
    "            \n",
    "            # Calculate batch statistics\n",
    "            batch_time = time.time() - batch_start_time\n",
    "            # Ensure we don't divide by zero\n",
    "            if current_batch and len(current_batch) > 0:\n",
    "                avg_item_time = batch_time / len(current_batch)\n",
    "                exact_matches = sum(1 for r in batch_results if r.get(\"exact_match\", False))\n",
    "                accuracy = (exact_matches / len(current_batch) * 100)\n",
    "            else:\n",
    "                avg_item_time = 0\n",
    "                exact_matches = 0\n",
    "                accuracy = 0\n",
    "            \n",
    "            print(f\"\\nüìä Batch {batch_idx+1} Summary:\")\n",
    "            print(f\"   Processed {len(current_batch)} items in {batch_time:.2f}s ({avg_item_time:.2f}s per item)\")\n",
    "            print(f\"   Exact Match: {exact_matches}/{len(current_batch)} ({accuracy:.1f}%)\")\n",
    "            \n",
    "            # Clean up memory between batches if configured\n",
    "            if pipeline_config[\"batch_processing\"].get(\"optimize_memory\", True):\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    print(\"   üßπ Cleaned up memory\")\n",
    "            \n",
    "            # Save checkpoint if enabled\n",
    "            if enable_checkpoints and (batch_idx + 1) % checkpoint_frequency == 0:\n",
    "                if hasattr(pipeline, \"save_checkpoint\") and callable(getattr(pipeline, \"save_checkpoint\")):\n",
    "                    # Use pipeline's method if available\n",
    "                    checkpoint = pipeline.save_checkpoint(checkpoint_path, batch_idx + 1, all_results)\n",
    "                else:\n",
    "                    # Direct implementation\n",
    "                    checkpoint = {\n",
    "                        \"batch_idx\": batch_idx + 1,\n",
    "                        \"processed_items\": all_results,\n",
    "                        \"timestamp\": time.time()\n",
    "                    }\n",
    "                    with open(checkpoint_path, \"w\") as f:\n",
    "                        json.dump(checkpoint, f)\n",
    "                \n",
    "                print(f\"   üíæ Saved checkpoint after batch {batch_idx+1}\")\n",
    "        \n",
    "        # Log final memory usage\n",
    "        memory_log.append(log_memory_usage(\"end\", \"After extraction completes\"))\n",
    "        \n",
    "        # Save final results\n",
    "        results_path = get_results_path(\"extraction_results.json\")\n",
    "        with open(results_path, \"w\") as f:\n",
    "            json.dump(all_results, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nüíæ Final results saved to: {results_path}\")\n",
    "        \n",
    "        # Save memory log\n",
    "        memory_log_path = get_results_path(\"memory_usage_log.json\")\n",
    "        with open(memory_log_path, \"w\") as f:\n",
    "            json.dump(memory_log, f, indent=2)\n",
    "        \n",
    "        print(f\"üíæ Memory usage log saved to: {memory_log_path}\")\n",
    "        \n",
    "        # Calculate final statistics\n",
    "        total_time = time.time() - full_start_time\n",
    "        # Ensure we don't divide by zero\n",
    "        if all_results and len(all_results) > 0:\n",
    "            avg_time = total_time / len(all_results)\n",
    "            exact_matches = sum(1 for r in all_results if r.get(\"exact_match\", False))\n",
    "            overall_accuracy = (exact_matches / len(all_results) * 100)\n",
    "        else:\n",
    "            avg_time = 0\n",
    "            exact_matches = 0\n",
    "            overall_accuracy = 0\n",
    "        \n",
    "        print(\"\\nüìä Extraction Complete!\")\n",
    "        print(f\"   Processed {len(all_results)} items in {total_time:.2f}s ({avg_time:.2f}s per item)\")\n",
    "        print(f\"   Exact Match Accuracy: {exact_matches}/{len(all_results)} ({overall_accuracy:.1f}%)\")\n",
    "        \n",
    "        # Store results for analysis\n",
    "        extraction_results = all_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error during extraction: {e}\")\n",
    "        \n",
    "        # Try to save partial results if available\n",
    "        if len(all_results) > 0:\n",
    "            partial_results_path = get_results_path(\"partial_extraction_results.json\")\n",
    "            with open(partial_results_path, \"w\") as f:\n",
    "                json.dump(all_results, f, indent=2)\n",
    "            \n",
    "            print(f\"üíæ Partial results saved to: {partial_results_path}\")\n",
    "        \n",
    "        # Try to save memory log if available\n",
    "        if len(memory_log) > 0:\n",
    "            memory_log.append(log_memory_usage(\"error\", f\"Error: {str(e)}\"))\n",
    "            memory_log_path = get_results_path(\"error_memory_usage_log.json\")\n",
    "            with open(memory_log_path, \"w\") as f:\n",
    "                json.dump(memory_log, f, indent=2)\n",
    "            \n",
    "            print(f\"üíæ Memory usage log saved to: {memory_log_path}\")\n",
    "        \n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        # Clean up memory regardless of success or failure\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results / Analysis\n",
    "### Basic Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Metrics Calculation\n",
    "\n",
    "# Import required libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Verify we have extraction results\n",
    "if 'extraction_results' not in locals() or not extraction_results:\n",
    "    print(\"‚ùå No extraction results found. Run the extraction cell first.\")\n",
    "    raise RuntimeError(\"No extraction results available for analysis\")\n",
    "\n",
    "print(\"üìä Calculating metrics from extraction results...\")\n",
    "print(f\"   Total results: {len(extraction_results)}\")\n",
    "\n",
    "# Convert results to pandas DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(extraction_results)\n",
    "print(f\"‚úÖ Converted {len(results_df)} results to DataFrame\")\n",
    "\n",
    "# Display dataframe info\n",
    "print(\"\\nüìã Results DataFrame Information:\")\n",
    "print(f\"   Shape: {results_df.shape}\")\n",
    "print(f\"   Columns: {', '.join(results_df.columns)}\")\n",
    "\n",
    "# Basic cleaning and type conversion if needed\n",
    "if 'exact_match' in results_df.columns:\n",
    "    results_df['exact_match'] = results_df['exact_match'].astype(bool)\n",
    "\n",
    "if 'processing_time' in results_df.columns:\n",
    "    # Ensure processing_time is numeric\n",
    "    results_df['processing_time'] = pd.to_numeric(results_df['processing_time'], errors='coerce')\n",
    "\n",
    "# Check for error cases\n",
    "error_cases = results_df[results_df['error'].notna()] if 'error' in results_df.columns else pd.DataFrame()\n",
    "valid_results = results_df[~results_df.index.isin(error_cases.index)] if not error_cases.empty else results_df\n",
    "\n",
    "print(f\"\\nüîç Found {len(error_cases)} error cases and {len(valid_results)} valid extraction results\")\n",
    "\n",
    "# Calculate basic accuracy metrics\n",
    "accuracy_metrics = {}\n",
    "\n",
    "if 'exact_match' in valid_results.columns:\n",
    "    exact_matches = valid_results['exact_match'].sum()\n",
    "    total_valid = len(valid_results)\n",
    "    exact_match_rate = exact_matches / total_valid if total_valid > 0 else 0\n",
    "    \n",
    "    accuracy_metrics['exact_match_count'] = int(exact_matches)\n",
    "    accuracy_metrics['total_valid_count'] = total_valid\n",
    "    accuracy_metrics['exact_match_rate'] = exact_match_rate\n",
    "    \n",
    "    print(f\"\\nüìà Exact Match Accuracy: {exact_matches}/{total_valid} ({exact_match_rate:.2%})\")\n",
    "\n",
    "# Calculate character error rate statistics\n",
    "if 'character_error_rate' in valid_results.columns:\n",
    "    cer_values = valid_results['character_error_rate'].dropna()\n",
    "    \n",
    "    if len(cer_values) > 0:\n",
    "        min_cer = cer_values.min()\n",
    "        max_cer = cer_values.max()\n",
    "        mean_cer = cer_values.mean()\n",
    "        median_cer = cer_values.median()\n",
    "        \n",
    "        accuracy_metrics['character_error_rate'] = {\n",
    "            'min': float(min_cer),\n",
    "            'max': float(max_cer),\n",
    "            'mean': float(mean_cer),\n",
    "            'median': float(median_cer)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìâ Character Error Rate Statistics:\")\n",
    "        print(f\"   Min: {min_cer:.4f}\")\n",
    "        print(f\"   Max: {max_cer:.4f}\")\n",
    "        print(f\"   Mean: {mean_cer:.4f}\")\n",
    "        print(f\"   Median: {median_cer:.4f}\")\n",
    "\n",
    "# Calculate timing statistics\n",
    "if 'processing_time' in valid_results.columns:\n",
    "    time_values = valid_results['processing_time'].dropna()\n",
    "    \n",
    "    if len(time_values) > 0:\n",
    "        min_time = time_values.min()\n",
    "        max_time = time_values.max()\n",
    "        mean_time = time_values.mean()\n",
    "        median_time = time_values.median()\n",
    "        total_time = time_values.sum()\n",
    "        \n",
    "        timing_metrics = {\n",
    "            'min_seconds': float(min_time),\n",
    "            'max_seconds': float(max_time),\n",
    "            'mean_seconds': float(mean_time),\n",
    "            'median_seconds': float(median_time),\n",
    "            'total_seconds': float(total_time)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚è±Ô∏è Processing Time Statistics:\")\n",
    "        print(f\"   Min: {min_time:.2f} seconds\")\n",
    "        print(f\"   Max: {max_time:.2f} seconds\")\n",
    "        print(f\"   Mean: {mean_time:.2f} seconds\")\n",
    "        print(f\"   Median: {median_time:.2f} seconds\")\n",
    "        print(f\"   Total: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "    else:\n",
    "        timing_metrics = {}\n",
    "else:\n",
    "    timing_metrics = {}\n",
    "\n",
    "# Analyze error types\n",
    "if not error_cases.empty and 'error' in error_cases.columns:\n",
    "    # Extract error message patterns\n",
    "    error_types = {}\n",
    "    \n",
    "    for error_msg in error_cases['error']:\n",
    "        if not isinstance(error_msg, str):\n",
    "            continue\n",
    "            \n",
    "        # Extract the first part of the error message (typically the error type)\n",
    "        error_type = error_msg.split(':')[0] if ':' in error_msg else error_msg\n",
    "        error_types[error_type] = error_types.get(error_type, 0) + 1\n",
    "    \n",
    "    print(f\"\\n‚ùå Error Type Analysis:\")\n",
    "    for error_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   {error_type}: {count} occurrences\")\n",
    "    \n",
    "    error_analysis = {\n",
    "        'total_errors': len(error_cases),\n",
    "        'error_types': error_types\n",
    "    }\n",
    "else:\n",
    "    error_analysis = {\n",
    "        'total_errors': 0,\n",
    "        'error_types': {}\n",
    "    }\n",
    "\n",
    "# Categorize results by success/failure\n",
    "if 'exact_match' in valid_results.columns:\n",
    "    successes = valid_results[valid_results['exact_match'] == True]\n",
    "    failures = valid_results[valid_results['exact_match'] == False]\n",
    "    \n",
    "    print(f\"\\nüéØ Result Categories:\")\n",
    "    print(f\"   Successful extractions: {len(successes)}\")\n",
    "    print(f\"   Failed extractions: {len(failures)}\")\n",
    "    print(f\"   Error cases: {len(error_cases)}\")\n",
    "    \n",
    "    result_categories = {\n",
    "        'successful_count': len(successes),\n",
    "        'failed_count': len(failures),\n",
    "        'error_count': len(error_cases)\n",
    "    }\n",
    "else:\n",
    "    result_categories = {}\n",
    "\n",
    "# Combine all metrics\n",
    "analysis_results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'total_results': len(extraction_results),\n",
    "    'accuracy_metrics': accuracy_metrics,\n",
    "    'timing_metrics': timing_metrics,\n",
    "    'error_analysis': error_analysis,\n",
    "    'result_categories': result_categories\n",
    "}\n",
    "\n",
    "# Save metrics to file\n",
    "metrics_path = get_results_path(\"extraction_metrics.json\")\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    json.dump(analysis_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Analysis metrics saved to: {metrics_path}\")\n",
    "\n",
    "# Store the metrics and categorized results for next cells\n",
    "metrics_results = analysis_results\n",
    "\n",
    "# Store categorized DataFrames for error analysis\n",
    "if 'exact_match' in valid_results.columns:\n",
    "    categorized_results = {\n",
    "        'successes': successes,\n",
    "        'failures': failures,\n",
    "        'errors': error_cases,\n",
    "        'all_results': results_df\n",
    "    }\n",
    "else:\n",
    "    categorized_results = {\n",
    "        'all_results': results_df\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Basic metrics calculation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Analysis\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from Levenshtein import distance\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Verify we have categorized results\n",
    "if 'categorized_results' not in locals() or not categorized_results:\n",
    "    print(\"‚ùå No categorized results found. Run the metrics calculation cell first.\")\n",
    "    raise RuntimeError(\"No categorized results available for analysis\")\n",
    "\n",
    "print(\"üîç Analyzing extraction errors and failure patterns...\")\n",
    "\n",
    "# Check if we have failure cases to analyze\n",
    "if 'failures' not in categorized_results or categorized_results['failures'].empty:\n",
    "    print(\"‚ÑπÔ∏è No failed extractions to analyze - all extractions were successful or had errors!\")\n",
    "    # Create empty dataframes to avoid errors in later cells\n",
    "    error_patterns = pd.DataFrame(columns=['pattern', 'count', 'percentage'])\n",
    "    failure_examples = pd.DataFrame(columns=['image_id', 'ground_truth', 'extracted_text', 'character_error_rate'])\n",
    "else:\n",
    "    # Get the failures dataframe\n",
    "    failures_df = categorized_results['failures'].copy()  # Make a copy to avoid SettingWithCopyWarning\n",
    "    print(f\"üìã Analyzing {len(failures_df)} failed extractions\")\n",
    "    \n",
    "    # Create a detailed analysis function for extraction failures\n",
    "    def analyze_extraction_difference(ground_truth, extraction):\n",
    "        \"\"\"Analyze the difference between ground truth and extraction\"\"\"\n",
    "        if not isinstance(ground_truth, str) or not isinstance(extraction, str):\n",
    "            return \"Invalid input types\"\n",
    "        \n",
    "        # Clean strings\n",
    "        gt = ground_truth.strip()\n",
    "        ext = extraction.strip()\n",
    "        \n",
    "        # Handle empty cases\n",
    "        if not gt and not ext:\n",
    "            return \"Both empty\"\n",
    "        if not gt:\n",
    "            return \"Ground truth empty\"\n",
    "        if not ext:\n",
    "            return \"Extraction empty\"\n",
    "        \n",
    "        # Calculate length difference\n",
    "        len_diff = len(ext) - len(gt)\n",
    "        \n",
    "        # Check for case differences\n",
    "        if gt.lower() == ext.lower():\n",
    "            return \"Case difference only\"\n",
    "        \n",
    "        # Check for whitespace-only differences\n",
    "        if gt.replace(\" \", \"\") == ext.replace(\" \", \"\"):\n",
    "            return \"Whitespace difference only\"\n",
    "            \n",
    "        # Check for character type mismatches (e.g., letter vs. number)\n",
    "        gt_alphas = sum(c.isalpha() for c in gt)\n",
    "        ext_alphas = sum(c.isalpha() for c in ext)\n",
    "        gt_digits = sum(c.isdigit() for c in gt)\n",
    "        ext_digits = sum(c.isdigit() for c in ext)\n",
    "        \n",
    "        if abs(gt_alphas - ext_alphas) > 0.5 * max(gt_alphas, ext_alphas) or \\\n",
    "           abs(gt_digits - ext_digits) > 0.5 * max(gt_digits, ext_digits):\n",
    "            return \"Character type mismatch\"\n",
    "            \n",
    "        # Check for substring relationship\n",
    "        if gt in ext:\n",
    "            return \"Ground truth is substring of extraction\"\n",
    "        if ext in gt:\n",
    "            return \"Extraction is substring of ground truth\"\n",
    "            \n",
    "        # Check for digits-only case\n",
    "        if gt.isdigit() and ext.isdigit():\n",
    "            if len(gt) == len(ext):\n",
    "                return \"Digit transposition or substitution\"\n",
    "            elif len(gt) > len(ext):\n",
    "                return \"Missing digits\"\n",
    "            else:\n",
    "                return \"Extra digits\"\n",
    "                \n",
    "        # For more complex cases, calculate edit distance and characterize\n",
    "        edit_dist = distance(gt, ext)\n",
    "        if edit_dist <= 2:\n",
    "            return \"Minor edit distance (1-2 chars)\"\n",
    "        elif edit_dist <= len(gt) * 0.3:\n",
    "            return \"Moderate edit distance (‚â§30%)\"\n",
    "        else:\n",
    "            return \"Major difference (>30% edit distance)\"\n",
    "    \n",
    "    # Add error pattern analysis to failures dataframe\n",
    "    pattern_column = []\n",
    "    details_column = []\n",
    "    \n",
    "    for _, row in failures_df.iterrows():\n",
    "        gt = row.get('ground_truth', '')\n",
    "        extracted = row.get('processed_extraction', '')\n",
    "        \n",
    "        if not isinstance(gt, str):\n",
    "            gt = str(gt) if gt is not None else ''\n",
    "        if not isinstance(extracted, str):\n",
    "            extracted = str(extracted) if extracted is not None else ''\n",
    "        \n",
    "        # Get error pattern\n",
    "        pattern = analyze_extraction_difference(gt, extracted)\n",
    "        pattern_column.append(pattern)\n",
    "        \n",
    "        # Create detailed comparison\n",
    "        if len(gt) > 0 and len(extracted) > 0:\n",
    "            # Highlight differences character by character\n",
    "            max_len = max(len(gt), len(extracted))\n",
    "            detail_text = f\"GT: '{gt}'\\nEX: '{extracted}'\\nDF: '\"\n",
    "            \n",
    "            for i in range(max_len):\n",
    "                if i < len(gt) and i < len(extracted):\n",
    "                    if gt[i] == extracted[i]:\n",
    "                        detail_text += \" \"\n",
    "                    else:\n",
    "                        detail_text += \"^\"\n",
    "                elif i < len(gt):\n",
    "                    detail_text += \"-\"  # Missing in extraction\n",
    "                else:\n",
    "                    detail_text += \"+\"  # Extra in extraction\n",
    "            \n",
    "            detail_text += \"'\"\n",
    "        else:\n",
    "            if len(gt) == 0:\n",
    "                detail_text = \"Ground truth is empty\"\n",
    "            elif len(extracted) == 0:\n",
    "                detail_text = \"Extraction is empty\"\n",
    "            else:\n",
    "                detail_text = \"Unknown comparison\"\n",
    "        \n",
    "        details_column.append(detail_text)\n",
    "    \n",
    "    # Add columns to dataframe - using .loc to avoid SettingWithCopyWarning\n",
    "    failures_df.loc[:, 'error_pattern'] = pattern_column\n",
    "    failures_df.loc[:, 'difference_details'] = details_column\n",
    "    \n",
    "    # Count and analyze error patterns\n",
    "    pattern_counts = Counter(pattern_column)\n",
    "    total_failures = len(failures_df)\n",
    "    \n",
    "    # Create a dataframe of error patterns\n",
    "    error_patterns = pd.DataFrame({\n",
    "        'pattern': list(pattern_counts.keys()),\n",
    "        'count': list(pattern_counts.values())\n",
    "    })\n",
    "    \n",
    "    # Add percentage column\n",
    "    error_patterns['percentage'] = error_patterns['count'] / total_failures * 100\n",
    "    \n",
    "    # Sort by count descending\n",
    "    error_patterns = error_patterns.sort_values('count', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Display error patterns\n",
    "    print(\"\\nüìä Error Pattern Analysis:\")\n",
    "    for i, row in error_patterns.iterrows():\n",
    "        print(f\"   {row['pattern']}: {row['count']} occurrences ({row['percentage']:.1f}%)\")\n",
    "    \n",
    "    # Select representative examples of each error pattern\n",
    "    print(\"\\nüìã Representative Examples of Error Patterns:\")\n",
    "    \n",
    "    # For each error pattern, find an example\n",
    "    examples = []\n",
    "    for pattern in error_patterns['pattern']:\n",
    "        # Get rows with this pattern\n",
    "        pattern_rows = failures_df[failures_df['error_pattern'] == pattern]\n",
    "        \n",
    "        if not pattern_rows.empty:\n",
    "            # Select a representative example (one with the median CER)\n",
    "            if 'character_error_rate' in pattern_rows.columns:\n",
    "                pattern_rows = pattern_rows.sort_values('character_error_rate')\n",
    "                example_row = pattern_rows.iloc[len(pattern_rows) // 2]  # Median\n",
    "            else:\n",
    "                # Just take the first one if no CER\n",
    "                example_row = pattern_rows.iloc[0]\n",
    "            \n",
    "            examples.append({\n",
    "                'pattern': pattern,\n",
    "                'image_id': example_row.get('image_id', 'Unknown'),\n",
    "                'ground_truth': example_row.get('ground_truth', ''),\n",
    "                'extracted_text': example_row.get('processed_extraction', ''),\n",
    "                'character_error_rate': example_row.get('character_error_rate', None),\n",
    "                'difference_details': example_row.get('difference_details', '')\n",
    "            })\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    failure_examples = pd.DataFrame(examples)\n",
    "    \n",
    "    # Display examples\n",
    "    for i, row in failure_examples.iterrows():\n",
    "        print(f\"\\n   Example of '{row['pattern']}':\")\n",
    "        print(f\"      Image: {row['image_id']}\")\n",
    "        print(f\"      Ground Truth: '{row['ground_truth']}'\")\n",
    "        print(f\"      Extracted: '{row['extracted_text']}'\")\n",
    "        if row['character_error_rate'] is not None:\n",
    "            print(f\"      Character Error Rate: {row['character_error_rate']:.4f}\")\n",
    "        print(f\"      {row['difference_details']}\")\n",
    "    \n",
    "    # Analyze length characteristics - using .loc to avoid SettingWithCopyWarning\n",
    "    if 'ground_truth' in failures_df.columns and 'processed_extraction' in failures_df.columns:\n",
    "        failures_df.loc[:, 'gt_length'] = failures_df['ground_truth'].astype(str).apply(len)\n",
    "        failures_df.loc[:, 'extraction_length'] = failures_df['processed_extraction'].astype(str).apply(len)\n",
    "        failures_df.loc[:, 'length_diff'] = failures_df['extraction_length'] - failures_df['gt_length']\n",
    "        \n",
    "        # Create length analysis\n",
    "        len_stats = {\n",
    "            'avg_gt_length': failures_df['gt_length'].mean(),\n",
    "            'avg_extraction_length': failures_df['extraction_length'].mean(),\n",
    "            'avg_abs_length_diff': failures_df['length_diff'].abs().mean(),\n",
    "            'shorter_extractions': (failures_df['length_diff'] < 0).sum(),\n",
    "            'longer_extractions': (failures_df['length_diff'] > 0).sum(),\n",
    "            'same_length': (failures_df['length_diff'] == 0).sum()\n",
    "        }\n",
    "        \n",
    "        print(\"\\nüìè Length Characteristics in Failed Extractions:\")\n",
    "        print(f\"   Average ground truth length: {len_stats['avg_gt_length']:.1f} characters\")\n",
    "        print(f\"   Average extraction length: {len_stats['avg_extraction_length']:.1f} characters\")\n",
    "        print(f\"   Average absolute length difference: {len_stats['avg_abs_length_diff']:.1f} characters\")\n",
    "        print(f\"   Extractions shorter than ground truth: {len_stats['shorter_extractions']} ({len_stats['shorter_extractions']/total_failures*100:.1f}%)\")\n",
    "        print(f\"   Extractions longer than ground truth: {len_stats['longer_extractions']} ({len_stats['longer_extractions']/total_failures*100:.1f}%)\")\n",
    "        print(f\"   Extractions same length as ground truth: {len_stats['same_length']} ({len_stats['same_length']/total_failures*100:.1f}%)\")\n",
    "\n",
    "# Function to convert NumPy types to native Python types for JSON serialization\n",
    "def convert_to_serializable(obj):\n",
    "    \"\"\"Convert NumPy types to native Python types for JSON serialization\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(item) for item in obj]\n",
    "    elif isinstance(obj, (np.integer, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return convert_to_serializable(obj.tolist())\n",
    "    elif isinstance(obj, pd.DataFrame):\n",
    "        return convert_to_serializable(obj.to_dict('records'))\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Prepare the error analysis data for the report\n",
    "error_analysis_data = {\n",
    "    'total_failures': len(categorized_results.get('failures', pd.DataFrame())),\n",
    "    'error_patterns': error_patterns.to_dict('records') if 'error_patterns' in locals() else [],\n",
    "    'examples': failure_examples.to_dict('records') if 'failure_examples' in locals() else [],\n",
    "    'length_stats': len_stats if 'len_stats' in locals() else {}\n",
    "}\n",
    "\n",
    "# Convert to serializable format\n",
    "error_analysis_data = convert_to_serializable(error_analysis_data)\n",
    "\n",
    "# Save error analysis to file\n",
    "error_analysis_path = get_results_path(\"error_analysis.json\")\n",
    "with open(error_analysis_path, \"w\") as f:\n",
    "    json.dump(error_analysis_data, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Error analysis saved to: {error_analysis_path}\")\n",
    "print(\"‚úÖ Error analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "\n",
    "# Import required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Verify we have results to visualize\n",
    "if 'categorized_results' not in locals() or not categorized_results:\n",
    "    print(\"‚ùå No results to visualize. Run the metrics calculation cell first.\")\n",
    "    raise RuntimeError(\"No results available for visualization\")\n",
    "\n",
    "print(\"üìä Creating visualizations from extraction results...\")\n",
    "\n",
    "# Create directory for visualizations if needed\n",
    "vis_dir = get_results_path(\"visualizations\")\n",
    "os.makedirs(vis_dir, exist_ok=True)\n",
    "print(f\"üìÅ Saving visualizations to: {vis_dir}\")\n",
    "\n",
    "# Function to save figures\n",
    "def save_figure(fig, filename):\n",
    "    \"\"\"Save a figure to the visualizations directory\"\"\"\n",
    "    filepath = os.path.join(vis_dir, filename)\n",
    "    fig.savefig(filepath, bbox_inches='tight', dpi=150)\n",
    "    return filepath\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 1. Create accuracy bar chart\n",
    "# -------------------------------------------------------------------------------\n",
    "print(\"üìä Creating accuracy bar chart...\")\n",
    "\n",
    "# Get the counts from metrics\n",
    "all_results = categorized_results['all_results']\n",
    "success_count = len(categorized_results.get('successes', pd.DataFrame()))\n",
    "failure_count = len(categorized_results.get('failures', pd.DataFrame()))\n",
    "error_count = len(categorized_results.get('errors', pd.DataFrame()))\n",
    "total_count = success_count + failure_count + error_count\n",
    "\n",
    "# Create the figure\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Define categories and counts\n",
    "categories = ['Exact Match', 'Non-Match', 'Error']\n",
    "counts = [success_count, failure_count, error_count]\n",
    "colors = ['#2ecc71', '#e74c3c', '#f39c12']  # Green, Red, Orange\n",
    "\n",
    "# Create bar chart\n",
    "bars = ax.bar(categories, counts, color=colors)\n",
    "\n",
    "# Add title and labels\n",
    "ax.set_title('Extraction Results Breakdown', fontsize=16)\n",
    "ax.set_ylabel('Number of Images', fontsize=14)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    percentage = height / total_count * 100\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "            f'{int(height)}\\n({percentage:.1f}%)', \n",
    "            ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "# Save the figure\n",
    "accuracy_chart_path = save_figure(fig, \"accuracy_chart.png\")\n",
    "plt.close(fig)\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 2. Create character error rate histogram\n",
    "# -------------------------------------------------------------------------------\n",
    "print(\"üìä Creating character error rate histogram...\")\n",
    "\n",
    "# Filter out error cases and get valid CER values\n",
    "valid_results = all_results[~all_results.index.isin(categorized_results.get('errors', pd.DataFrame()).index)]\n",
    "if 'character_error_rate' in valid_results.columns:\n",
    "    cer_values = valid_results['character_error_rate'].dropna()\n",
    "    \n",
    "    if len(cer_values) > 0:\n",
    "        # Create the figure\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        # Create histogram with custom bins\n",
    "        bins = np.linspace(0, min(1.0, cer_values.max() * 1.1), 20)\n",
    "        n, bins, patches = ax.hist(cer_values, bins=bins, color='#3498db', alpha=0.7, edgecolor='black')\n",
    "        \n",
    "        # Add title and labels\n",
    "        ax.set_title('Character Error Rate Distribution', fontsize=16)\n",
    "        ax.set_xlabel('Character Error Rate', fontsize=14)\n",
    "        ax.set_ylabel('Number of Images', fontsize=14)\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Add vertical line for mean and median\n",
    "        mean_cer = cer_values.mean()\n",
    "        median_cer = cer_values.median()\n",
    "        ax.axvline(x=mean_cer, color='#e74c3c', linestyle='--', \n",
    "                   label=f'Mean: {mean_cer:.4f}')\n",
    "        ax.axvline(x=median_cer, color='#2ecc71', linestyle='-', \n",
    "                   label=f'Median: {median_cer:.4f}')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Save the figure\n",
    "        cer_chart_path = save_figure(fig, \"character_error_rate_histogram.png\")\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No valid character error rate values to plot\")\n",
    "        cer_chart_path = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No character error rate data found in results\")\n",
    "    cer_chart_path = None\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 3. Create processing time histogram\n",
    "# -------------------------------------------------------------------------------\n",
    "print(\"üìä Creating processing time histogram...\")\n",
    "\n",
    "# Get processing time values\n",
    "if 'processing_time' in all_results.columns:\n",
    "    time_values = all_results['processing_time'].dropna()\n",
    "    \n",
    "    if len(time_values) > 0:\n",
    "        # Create the figure\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        # Create histogram\n",
    "        bins = 20\n",
    "        n, bins, patches = ax.hist(time_values, bins=bins, color='#9b59b6', alpha=0.7, edgecolor='black')\n",
    "        \n",
    "        # Add title and labels\n",
    "        ax.set_title('Processing Time Distribution', fontsize=16)\n",
    "        ax.set_xlabel('Processing Time (seconds)', fontsize=14)\n",
    "        ax.set_ylabel('Number of Images', fontsize=14)\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Add vertical line for mean and median\n",
    "        mean_time = time_values.mean()\n",
    "        median_time = time_values.median()\n",
    "        ax.axvline(x=mean_time, color='#e74c3c', linestyle='--', \n",
    "                  label=f'Mean: {mean_time:.2f}s')\n",
    "        ax.axvline(x=median_time, color='#2ecc71', linestyle='-', \n",
    "                  label=f'Median: {median_time:.2f}s')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Save the figure\n",
    "        time_chart_path = save_figure(fig, \"processing_time_histogram.png\")\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No valid processing time values to plot\")\n",
    "        time_chart_path = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No processing time data found in results\")\n",
    "    time_chart_path = None\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 4. Create error patterns bar chart (if available)\n",
    "# -------------------------------------------------------------------------------\n",
    "print(\"üìä Creating error patterns chart...\")\n",
    "\n",
    "if 'error_patterns' in locals() and not error_patterns.empty:\n",
    "    # Create the figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Get top N patterns (to avoid overcrowding)\n",
    "    top_n = min(8, len(error_patterns))\n",
    "    top_patterns = error_patterns.head(top_n)\n",
    "    \n",
    "    # Create horizontal bar chart\n",
    "    bars = ax.barh(top_patterns['pattern'], top_patterns['count'], color='#e74c3c', alpha=0.7)\n",
    "    \n",
    "    # Add title and labels\n",
    "    ax.set_title('Top Error Patterns', fontsize=16)\n",
    "    ax.set_xlabel('Number of Occurrences', fontsize=14)\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        percentage = top_patterns.iloc[i]['percentage']\n",
    "        ax.text(width + 0.5, bar.get_y() + bar.get_height()/2.,\n",
    "                f'{int(width)} ({percentage:.1f}%)', \n",
    "                ha='left', va='center', fontsize=12)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    error_patterns_chart_path = save_figure(fig, \"error_patterns_chart.png\")\n",
    "    plt.close(fig)\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No error patterns data available to plot\")\n",
    "    error_patterns_chart_path = None\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 5. Generate HTML summary dashboard\n",
    "# -------------------------------------------------------------------------------\n",
    "print(\"üìÑ Generating HTML summary dashboard...\")\n",
    "\n",
    "# Get metadata for the dashboard\n",
    "experiment_name = extraction_config[\"experiment_name\"] if \"extraction_config\" in locals() else \"Extraction Experiment\"\n",
    "model_name = model_config[\"name\"] if \"model_config\" in locals() else \"Unknown Model\"\n",
    "prompt_text = prompt_info[\"text\"] if \"prompt_info\" in locals() else \"Unknown Prompt\"\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Calculate summary metrics\n",
    "total_images = len(all_results)\n",
    "success_rate = success_count / total_images * 100 if total_images > 0 else 0\n",
    "avg_time = all_results['processing_time'].mean() if 'processing_time' in all_results.columns else 0\n",
    "avg_cer = valid_results['character_error_rate'].mean() if 'character_error_rate' in valid_results.columns else 0\n",
    "\n",
    "# Create HTML content\n",
    "html_content = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Extraction Results Dashboard</title>\n",
    "    <style>\n",
    "        body {{\n",
    "            font-family: Arial, sans-serif;\n",
    "            margin: 0;\n",
    "            padding: 20px;\n",
    "            background-color: #f5f5f5;\n",
    "        }}\n",
    "        .container {{\n",
    "            max-width: 1200px;\n",
    "            margin: 0 auto;\n",
    "            background-color: white;\n",
    "            padding: 20px;\n",
    "            border-radius: 5px;\n",
    "            box-shadow: 0 0 10px rgba(0,0,0,0.1);\n",
    "        }}\n",
    "        h1, h2, h3 {{\n",
    "            color: #333;\n",
    "        }}\n",
    "        .metrics {{\n",
    "            display: flex;\n",
    "            flex-wrap: wrap;\n",
    "            margin: 20px 0;\n",
    "        }}\n",
    "        .metric-card {{\n",
    "            background-color: white;\n",
    "            border-radius: 5px;\n",
    "            box-shadow: 0 0 5px rgba(0,0,0,0.1);\n",
    "            padding: 15px;\n",
    "            margin: 10px;\n",
    "            min-width: 200px;\n",
    "            flex: 1;\n",
    "        }}\n",
    "        .metric-title {{\n",
    "            font-size: 14px;\n",
    "            color: #666;\n",
    "            margin-bottom: 5px;\n",
    "        }}\n",
    "        .metric-value {{\n",
    "            font-size: 24px;\n",
    "            font-weight: bold;\n",
    "            color: #333;\n",
    "        }}\n",
    "        .chart-container {{\n",
    "            margin: 20px 0;\n",
    "            text-align: center;\n",
    "        }}\n",
    "        .chart {{\n",
    "            max-width: 100%;\n",
    "            height: auto;\n",
    "            margin: 10px 0;\n",
    "            border: 1px solid #eee;\n",
    "        }}\n",
    "        .experiment-info {{\n",
    "            background-color: #f9f9f9;\n",
    "            padding: 15px;\n",
    "            border-radius: 5px;\n",
    "            margin-bottom: 20px;\n",
    "        }}\n",
    "        .prompt-box {{\n",
    "            background-color: #f5f5f5;\n",
    "            padding: 15px;\n",
    "            border-radius: 5px;\n",
    "            border-left: 4px solid #3498db;\n",
    "            font-family: monospace;\n",
    "            white-space: pre-wrap;\n",
    "            margin: 10px 0;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>Extraction Results Dashboard</h1>\n",
    "        \n",
    "        <div class=\"experiment-info\">\n",
    "            <h2>Experiment Information</h2>\n",
    "            <p><strong>Experiment:</strong> {experiment_name}</p>\n",
    "            <p><strong>Model:</strong> {model_name}</p>\n",
    "            <p><strong>Date:</strong> {timestamp}</p>\n",
    "            <p><strong>Total Images:</strong> {total_images}</p>\n",
    "            \n",
    "            <h3>Prompt Used:</h3>\n",
    "            <div class=\"prompt-box\">{prompt_text}</div>\n",
    "        </div>\n",
    "        \n",
    "        <h2>Summary Metrics</h2>\n",
    "        <div class=\"metrics\">\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-title\">Success Rate</div>\n",
    "                <div class=\"metric-value\">{success_rate:.1f}%</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-title\">Average CER</div>\n",
    "                <div class=\"metric-value\">{avg_cer:.4f}</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-title\">Average Processing Time</div>\n",
    "                <div class=\"metric-value\">{avg_time:.2f}s</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-title\">Success Count</div>\n",
    "                <div class=\"metric-value\">{success_count}</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-title\">Failure Count</div>\n",
    "                <div class=\"metric-value\">{failure_count}</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-title\">Error Count</div>\n",
    "                <div class=\"metric-value\">{error_count}</div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <h2>Charts</h2>\n",
    "        \n",
    "        <div class=\"chart-container\">\n",
    "            <h3>Accuracy Breakdown</h3>\n",
    "            <img class=\"chart\" src=\"accuracy_chart.png\" alt=\"Accuracy Chart\">\n",
    "        </div>\n",
    "        \n",
    "        {f'<div class=\"chart-container\"><h3>Character Error Rate Distribution</h3><img class=\"chart\" src=\"character_error_rate_histogram.png\" alt=\"CER Histogram\"></div>' if cer_chart_path else ''}\n",
    "        \n",
    "        {f'<div class=\"chart-container\"><h3>Processing Time Distribution</h3><img class=\"chart\" src=\"processing_time_histogram.png\" alt=\"Processing Time Histogram\"></div>' if time_chart_path else ''}\n",
    "        \n",
    "        {f'<div class=\"chart-container\"><h3>Top Error Patterns</h3><img class=\"chart\" src=\"error_patterns_chart.png\" alt=\"Error Patterns Chart\"></div>' if 'error_patterns_chart_path' in locals() and error_patterns_chart_path else ''}\n",
    "        \n",
    "        <h2>Conclusion</h2>\n",
    "        <p>This dashboard presents the results of the extraction experiment performed on {total_images} invoice images.</p>\n",
    "        <p>The model achieved a success rate of {success_rate:.1f}% with an average character error rate of {avg_cer:.4f}.</p>\n",
    "        \n",
    "        <footer style=\"margin-top: 50px; text-align: center; color: #777; font-size: 12px;\">\n",
    "            Generated on {timestamp} ‚Ä¢ Invoice Processing Analysis System\n",
    "        </footer>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Save the HTML dashboard\n",
    "dashboard_path = get_results_path(\"results_dashboard.html\")\n",
    "with open(dashboard_path, \"w\") as f:\n",
    "    f.write(html_content)\n",
    "\n",
    "print(f\"\\nüìÑ Dashboard saved to: {dashboard_path}\")\n",
    "print(\"‚úÖ Visualizations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "### Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Results Summary\n",
    "\n",
    "# Import required libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "# Verify we have analysis results\n",
    "if 'metrics_results' not in locals() or not metrics_results:\n",
    "    print(\"‚ùå No metrics results found. Run the metrics calculation cell first.\")\n",
    "    raise RuntimeError(\"No metrics results available for summary\")\n",
    "\n",
    "print(\"üìã Generating comprehensive experiment summary...\")\n",
    "\n",
    "# Gather all experiment details\n",
    "experiment_details = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"experiment_name\": extraction_config[\"experiment_name\"] if \"extraction_config\" in locals() else \"Extraction Experiment\",\n",
    "    \"model\": {\n",
    "        \"name\": model_config[\"name\"] if \"model_config\" in locals() else \"Unknown Model\",\n",
    "        \"repo_id\": model_config[\"repo_id\"] if \"model_config\" in locals() else \"Unknown Repo\",\n",
    "        \"quantization\": model_config.get(\"use_quantization\", False) if \"model_config\" in locals() else False,\n",
    "        \"quantization_bits\": model_config.get(\"quantization_bits\", None) if \"model_config\" in locals() else None,\n",
    "        \"dtype\": model_metadata.get(\"dtype\", \"Unknown\") if \"model_metadata\" in locals() else \"Unknown\",\n",
    "        \"device\": model_metadata.get(\"device\", \"Unknown\") if \"model_metadata\" in locals() else \"Unknown\"\n",
    "    },\n",
    "    \"prompt\": {\n",
    "        \"text\": prompt_info[\"text\"] if \"prompt_info\" in locals() else \"Unknown Prompt\",\n",
    "        \"name\": prompt_info[\"name\"] if \"prompt_info\" in locals() else \"Unknown\",\n",
    "        \"category\": prompt_info[\"category\"] if \"prompt_info\" in locals() else \"Unknown\"\n",
    "    },\n",
    "    \"execution\": {\n",
    "        \"batch_size\": pipeline_config[\"batch_processing\"].get(\"batch_size\", 1) if \"pipeline_config\" in locals() else 1,\n",
    "        \"checkpointing\": pipeline_config[\"checkpointing\"].get(\"enable_checkpoints\", False) if \"pipeline_config\" in locals() else False,\n",
    "        \"environment\": env_config.environment if \"env_config\" in locals() else \"Unknown\"\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"total_images\": len(categorized_results['all_results']) if 'categorized_results' in locals() else 0,\n",
    "        \"field_to_extract\": extraction_config[\"field_to_extract\"] if \"extraction_config\" in locals() else \"work_order\",\n",
    "        \"ground_truth_column\": extraction_config.get(\"field_column_name\", \"Unknown\") if \"extraction_config\" in locals() else \"Unknown\"\n",
    "    },\n",
    "    \"results\": metrics_results\n",
    "}\n",
    "\n",
    "# Save the comprehensive summary\n",
    "summary_path = get_results_path(\"experiment_summary.json\")\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump(experiment_details, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Comprehensive summary saved to: {summary_path}\")\n",
    "\n",
    "# Display summary table\n",
    "print(\"\\nüìä Experiment Results Summary\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Experiment: {experiment_details['experiment_name']}\")\n",
    "print(f\"Model: {experiment_details['model']['name']}\")\n",
    "print(f\"Conducted: {datetime.fromisoformat(experiment_details['timestamp']).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Environment: {experiment_details['execution']['environment']}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display dataset information\n",
    "print(\"\\nüìä Dataset Information:\")\n",
    "print(f\"Total Images: {experiment_details['dataset']['total_images']}\")\n",
    "print(f\"Field to Extract: {experiment_details['dataset']['field_to_extract']}\")\n",
    "\n",
    "# Display model information\n",
    "print(\"\\nü§ñ Model Information:\")\n",
    "print(f\"Model: {experiment_details['model']['name']}\")\n",
    "print(f\"Repository: {experiment_details['model']['repo_id']}\")\n",
    "print(f\"Dtype: {experiment_details['model']['dtype']}\")\n",
    "print(f\"Device: {experiment_details['model']['device']}\")\n",
    "if experiment_details['model']['quantization']:\n",
    "    print(f\"Quantization: {experiment_details['model']['quantization_bits']}-bit\")\n",
    "else:\n",
    "    print(\"Quantization: Disabled\")\n",
    "\n",
    "# Display prompt information\n",
    "print(\"\\nüìù Prompt Information:\")\n",
    "print(f\"Name: {experiment_details['prompt']['name']}\")\n",
    "print(f\"Category: {experiment_details['prompt']['category']}\")\n",
    "print(f\"Text: \\\"{experiment_details['prompt']['text']}\\\"\")\n",
    "\n",
    "# Display execution information\n",
    "print(\"\\n‚öôÔ∏è Execution Information:\")\n",
    "print(f\"Batch Size: {experiment_details['execution']['batch_size']}\")\n",
    "print(f\"Checkpointing: {'Enabled' if experiment_details['execution']['checkpointing'] else 'Disabled'}\")\n",
    "\n",
    "# Display performance metrics\n",
    "print(\"\\nüìà Performance Metrics:\")\n",
    "accuracy = experiment_details['results']['accuracy_metrics'].get('exact_match_rate', 0) * 100\n",
    "exact_matches = experiment_details['results']['accuracy_metrics'].get('exact_match_count', 0)\n",
    "total_valid = experiment_details['results']['accuracy_metrics'].get('total_valid_count', 0)\n",
    "avg_cer = experiment_details['results']['accuracy_metrics'].get('character_error_rate', {}).get('mean', 0)\n",
    "avg_time = experiment_details['results']['timing_metrics'].get('mean_seconds', 0)\n",
    "total_time = experiment_details['results']['timing_metrics'].get('total_seconds', 0)\n",
    "\n",
    "print(f\"Exact Match: {exact_matches}/{total_valid} ({accuracy:.2f}%)\")\n",
    "print(f\"Average Character Error Rate: {avg_cer:.4f}\")\n",
    "print(f\"Average Processing Time: {avg_time:.2f} seconds per image\")\n",
    "print(f\"Total Processing Time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "\n",
    "# Display error statistics\n",
    "error_count = experiment_details['results']['error_analysis'].get('total_errors', 0)\n",
    "error_pct = error_count / experiment_details['dataset']['total_images'] * 100 if experiment_details['dataset']['total_images'] > 0 else 0\n",
    "print(f\"Error Cases: {error_count} ({error_pct:.2f}%)\")\n",
    "\n",
    "# Check if there are previous runs to compare with\n",
    "results_dir = os.path.dirname(get_results_path(\"\"))\n",
    "parent_dir = os.path.dirname(results_dir)\n",
    "all_result_dirs = glob.glob(os.path.join(parent_dir, \"*\"))\n",
    "previous_summaries = []\n",
    "\n",
    "for result_dir in all_result_dirs:\n",
    "    # Skip current results directory\n",
    "    if result_dir == results_dir:\n",
    "        continue\n",
    "    \n",
    "    # Look for summary file\n",
    "    summary_file = os.path.join(result_dir, \"experiment_summary.json\")\n",
    "    if os.path.exists(summary_file):\n",
    "        try:\n",
    "            with open(summary_file, \"r\") as f:\n",
    "                previous_summary = json.load(f)\n",
    "            previous_summaries.append({\n",
    "                \"dir\": os.path.basename(result_dir),\n",
    "                \"summary\": previous_summary\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading summary from {result_dir}: {e}\")\n",
    "\n",
    "# Compare with previous runs if available\n",
    "if previous_summaries:\n",
    "    print(\"\\nüîÑ Comparison with Previous Runs:\")\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_data = []\n",
    "    for prev in previous_summaries:\n",
    "        # Get key metrics\n",
    "        prev_accuracy = prev[\"summary\"].get(\"results\", {}).get(\"accuracy_metrics\", {}).get(\"exact_match_rate\", 0) * 100\n",
    "        prev_avg_cer = prev[\"summary\"].get(\"results\", {}).get(\"accuracy_metrics\", {}).get(\"character_error_rate\", {}).get(\"mean\", 0)\n",
    "        prev_avg_time = prev[\"summary\"].get(\"results\", {}).get(\"timing_metrics\", {}).get(\"mean_seconds\", 0)\n",
    "        prev_model = prev[\"summary\"].get(\"model\", {}).get(\"name\", \"Unknown\")\n",
    "        prev_prompt_category = prev[\"summary\"].get(\"prompt\", {}).get(\"category\", \"Unknown\")\n",
    "        prev_timestamp = datetime.fromisoformat(prev[\"summary\"].get(\"timestamp\", datetime.now().isoformat())).strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        comparison_data.append({\n",
    "            \"run\": prev[\"dir\"],\n",
    "            \"date\": prev_timestamp,\n",
    "            \"model\": prev_model,\n",
    "            \"prompt_category\": prev_prompt_category,\n",
    "            \"accuracy\": prev_accuracy,\n",
    "            \"avg_cer\": prev_avg_cer,\n",
    "            \"avg_time\": prev_avg_time\n",
    "        })\n",
    "    \n",
    "    # Add current run\n",
    "    comparison_data.append({\n",
    "        \"run\": os.path.basename(results_dir),\n",
    "        \"date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"model\": experiment_details[\"model\"][\"name\"],\n",
    "        \"prompt_category\": experiment_details[\"prompt\"][\"category\"],\n",
    "        \"accuracy\": accuracy,\n",
    "        \"avg_cer\": avg_cer,\n",
    "        \"avg_time\": avg_time,\n",
    "        \"current\": True\n",
    "    })\n",
    "    \n",
    "    # Convert to dataframe and sort by date\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.sort_values(\"date\", ascending=False)\n",
    "    \n",
    "    # Display comparison table\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(\"\\nComparison Table:\")\n",
    "    display(comparison_df)\n",
    "    \n",
    "    # Calculate improvements\n",
    "    if len(comparison_df) > 1:\n",
    "        # Find the most recent previous run\n",
    "        previous_runs = comparison_df[comparison_df[\"current\"] != True]\n",
    "        if not previous_runs.empty:\n",
    "            most_recent = previous_runs.iloc[0]\n",
    "            \n",
    "            # Calculate changes\n",
    "            accuracy_change = accuracy - most_recent[\"accuracy\"]\n",
    "            cer_change = avg_cer - most_recent[\"avg_cer\"]\n",
    "            time_change = avg_time - most_recent[\"avg_time\"]\n",
    "            \n",
    "            print(\"\\nComparison with Most Recent Run:\")\n",
    "            print(f\"Accuracy Change: {accuracy_change:+.2f}% ({'+' if accuracy_change >= 0 else ''}{accuracy_change/most_recent['accuracy']*100:.2f}% relative)\")\n",
    "            print(f\"CER Change: {cer_change:+.4f} ({'+' if cer_change >= 0 else ''}{cer_change/most_recent['avg_cer']*100:.2f}% relative)\")\n",
    "            print(f\"Processing Time Change: {time_change:+.2f}s ({'+' if time_change >= 0 else ''}{time_change/most_recent['avg_time']*100:.2f}% relative)\")\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è No previous runs available for comparison\")\n",
    "\n",
    "# Generate conclusions\n",
    "print(\"\\nüìù Experiment Conclusions:\")\n",
    "\n",
    "# Accuracy assessment\n",
    "if accuracy >= 95:\n",
    "    accuracy_assessment = \"Excellent accuracy achieved, suitable for production use.\"\n",
    "elif accuracy >= 85:\n",
    "    accuracy_assessment = \"Good accuracy achieved, may need review for critical cases.\"\n",
    "elif accuracy >= 70:\n",
    "    accuracy_assessment = \"Moderate accuracy, needs improvement for production use.\"\n",
    "else:\n",
    "    accuracy_assessment = \"Low accuracy, significant improvements needed.\"\n",
    "\n",
    "print(f\"1. {accuracy_assessment}\")\n",
    "\n",
    "# CER assessment\n",
    "if avg_cer <= 0.05:\n",
    "    cer_assessment = \"Very low character error rate, indicating high extraction quality.\"\n",
    "elif avg_cer <= 0.15:\n",
    "    cer_assessment = \"Acceptable character error rate, minor extraction errors present.\"\n",
    "elif avg_cer <= 0.3:\n",
    "    cer_assessment = \"Moderate character error rate, review needed for many extractions.\"\n",
    "else:\n",
    "    cer_assessment = \"High character error rate, indicating significant extraction issues.\"\n",
    "\n",
    "print(f\"2. {cer_assessment}\")\n",
    "\n",
    "# Performance assessment\n",
    "if avg_time <= 0.5:\n",
    "    perf_assessment = \"Excellent processing speed, suitable for high volume processing.\"\n",
    "elif avg_time <= 1.5:\n",
    "    perf_assessment = \"Good processing speed, acceptable for most use cases.\"\n",
    "elif avg_time <= 3.0:\n",
    "    perf_assessment = \"Moderate processing speed, may need optimization for large volumes.\"\n",
    "else:\n",
    "    perf_assessment = \"Slow processing speed, optimization needed for production use.\"\n",
    "\n",
    "print(f\"3. {perf_assessment}\")\n",
    "\n",
    "# Error assessment\n",
    "if error_pct <= 1:\n",
    "    error_assessment = \"Very few errors encountered, indicating stable extraction pipeline.\"\n",
    "elif error_pct <= 5:\n",
    "    error_assessment = \"Low error rate, acceptable for most use cases.\"\n",
    "elif error_pct <= 10:\n",
    "    error_assessment = \"Moderate error rate, needs investigation for production use.\"\n",
    "else:\n",
    "    error_assessment = \"High error rate, indicating significant pipeline stability issues.\"\n",
    "\n",
    "print(f\"4. {error_assessment}\")\n",
    "\n",
    "# Overall conclusion\n",
    "print(\"\\nOverall Conclusion:\")\n",
    "if accuracy >= 85 and avg_cer <= 0.15 and error_pct <= 5:\n",
    "    overall = \"This extraction pipeline demonstrates strong performance and is suitable for production use with minimal oversight.\"\n",
    "elif accuracy >= 70 and avg_cer <= 0.3 and error_pct <= 10:\n",
    "    overall = \"This extraction pipeline shows promise but needs further refinement before full production deployment.\"\n",
    "else:\n",
    "    overall = \"This extraction pipeline requires significant improvements in accuracy and stability before production use.\"\n",
    "\n",
    "print(overall)\n",
    "\n",
    "# Save conclusions\n",
    "conclusions = {\n",
    "    \"accuracy_assessment\": accuracy_assessment,\n",
    "    \"cer_assessment\": cer_assessment,\n",
    "    \"performance_assessment\": perf_assessment,\n",
    "    \"error_assessment\": error_assessment,\n",
    "    \"overall_conclusion\": overall\n",
    "}\n",
    "\n",
    "# Add conclusions to the experiment summary\n",
    "experiment_details[\"conclusions\"] = conclusions\n",
    "\n",
    "# Save updated summary\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump(experiment_details, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Updated summary with conclusions saved to: {summary_path}\")\n",
    "print(\"\\n‚úÖ Experiment summary complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource Cleanup\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üßπ Cleaning up resources and recording final state...\")\n",
    "\n",
    "# Record initial memory state\n",
    "initial_memory = None\n",
    "if torch.cuda.is_available():\n",
    "    initial_memory = {\n",
    "        \"total_memory_gb\": torch.cuda.get_device_properties(0).total_memory / 1e9,\n",
    "        \"allocated_memory_gb\": torch.cuda.memory_allocated(0) / 1e9,\n",
    "        \"reserved_memory_gb\": torch.cuda.memory_reserved(0) / 1e9,\n",
    "        \"free_memory_gb\": (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1e9\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüß† Initial GPU Memory State:\")\n",
    "    print(f\"   Total Memory: {initial_memory['total_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Allocated Memory: {initial_memory['allocated_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Reserved Memory: {initial_memory['reserved_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Free Memory: {initial_memory['free_memory_gb']:.2f} GB\")\n",
    "\n",
    "# Release model from memory\n",
    "print(\"\\nüîÑ Releasing model from memory...\")\n",
    "if 'model' in locals():\n",
    "    try:\n",
    "        # Delete the model\n",
    "        del model\n",
    "        print(\"   ‚úì Model deleted\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error deleting model: {e}\")\n",
    "else:\n",
    "    print(\"   ‚ÑπÔ∏è No model in memory to delete\")\n",
    "\n",
    "# Release processor from memory\n",
    "if 'processor' in locals():\n",
    "    try:\n",
    "        # Delete the processor\n",
    "        del processor\n",
    "        print(\"   ‚úì Processor deleted\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error deleting processor: {e}\")\n",
    "else:\n",
    "    print(\"   ‚ÑπÔ∏è No processor in memory to delete\")\n",
    "\n",
    "# Release pipeline from memory\n",
    "if 'pipeline' in locals():\n",
    "    try:\n",
    "        # Delete the pipeline\n",
    "        del pipeline\n",
    "        print(\"   ‚úì Pipeline deleted\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error deleting pipeline: {e}\")\n",
    "else:\n",
    "    print(\"   ‚ÑπÔ∏è No pipeline in memory to delete\")\n",
    "\n",
    "# Release other large variables\n",
    "large_variables = ['extraction_results', 'categorized_results', 'all_results', 'metrics_results']\n",
    "for var_name in large_variables:\n",
    "    if var_name in locals():\n",
    "        try:\n",
    "            # Use exec to delete the variable\n",
    "            exec(f\"del {var_name}\")\n",
    "            print(f\"   ‚úì Deleted {var_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error deleting {var_name}: {e}\")\n",
    "\n",
    "# Run garbage collection\n",
    "print(\"\\nüßπ Running garbage collection...\")\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"   ‚úì CUDA cache emptied and synchronized\")\n",
    "else:\n",
    "    print(\"   ‚ÑπÔ∏è No CUDA available for cache emptying\")\n",
    "\n",
    "# Clean up temporary files\n",
    "print(\"\\nüóëÔ∏è Cleaning up temporary files...\")\n",
    "\n",
    "# Define temporary directories to clean\n",
    "temp_dirs = []\n",
    "\n",
    "# Clean up any temporary files in the results directory\n",
    "temp_files = []  # Add any specific temp files pattern here if needed\n",
    "\n",
    "# Remove temporary files if any were specified\n",
    "for file_pattern in temp_files:\n",
    "    matching_files = []  # Add glob or similar to find matching files\n",
    "    for file_path in matching_files:\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"   ‚úì Removed temporary file: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error removing file {file_path}: {e}\")\n",
    "\n",
    "# Record final memory state\n",
    "final_memory = None\n",
    "if torch.cuda.is_available():\n",
    "    final_memory = {\n",
    "        \"total_memory_gb\": torch.cuda.get_device_properties(0).total_memory / 1e9,\n",
    "        \"allocated_memory_gb\": torch.cuda.memory_allocated(0) / 1e9,\n",
    "        \"reserved_memory_gb\": torch.cuda.memory_reserved(0) / 1e9,\n",
    "        \"free_memory_gb\": (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1e9\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüß† Final GPU Memory State:\")\n",
    "    print(f\"   Total Memory: {final_memory['total_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Allocated Memory: {final_memory['allocated_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Reserved Memory: {final_memory['reserved_memory_gb']:.2f} GB\")\n",
    "    print(f\"   Free Memory: {final_memory['free_memory_gb']:.2f} GB\")\n",
    "    \n",
    "    if initial_memory:\n",
    "        memory_freed = initial_memory['allocated_memory_gb'] - final_memory['allocated_memory_gb']\n",
    "        print(f\"   Memory Freed: {memory_freed:.2f} GB\")\n",
    "\n",
    "# Record final environment state\n",
    "final_state = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"torch_version\": torch.__version__,\n",
    "    \"cuda_available\": torch.cuda.is_available(),\n",
    "    \"initial_memory\": initial_memory,\n",
    "    \"final_memory\": final_memory,\n",
    "    \"cleanup_success\": True\n",
    "}\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    final_state[\"cuda_version\"] = torch.version.cuda\n",
    "    final_state[\"device_name\"] = torch.cuda.get_device_name(0)\n",
    "\n",
    "# Save final state\n",
    "try:\n",
    "    final_state_path = get_results_path(\"final_environment_state.json\")\n",
    "    with open(final_state_path, \"w\") as f:\n",
    "        json.dump(final_state, f, indent=2)\n",
    "    print(f\"\\nüíæ Final environment state saved to: {final_state_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Error saving final state: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Resource cleanup complete!\")\n",
    "print(\"\\nüìù Experiment execution finished. Results and visualizations are available in the results directory.\")\n",
    "print(f\"   Results directory: {get_results_path('')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
